<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimization Algorithms</title>

    <link rel="alternate" href="https://campusempresa.com/mod/fundamentos_ia/03-03-algoritmos-optimizacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/fundamentos_ia/03-03-algoritmos-optimizacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/fundamentos_ia/03-03-optimization-algorithms" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/fundamentos_ia/03-03-algoritmos-optimizacion" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/fundamentos_ia/03-03-algoritmos-optimizacion" class="px-2">CA</a>
<br>
			<cite>Building today's and tomorrow's society</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
									<a href="./">Course Content</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Technical Skills</a>
				<a href="/all/conocimientos">Knowledge</a>
				<a href="/all/soft_skills">Social Skills</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='03-02-search-algorithms' title="Search Algorithms">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Optimization Algorithms</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-01-basic-concepts-ml' title="Basic Concepts of Machine Learning">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Optimization algorithms are a crucial part of artificial intelligence, particularly in machine learning and operations research. These algorithms aim to find the best solution from a set of possible solutions, often by maximizing or minimizing a particular function. In this section, we will cover the fundamental concepts, types, and practical examples of optimization algorithms.</p>
</div><h1><p>Key Concepts</p>
</h1>
<div class='content'><ol>
<li><strong>Objective Function</strong>: The function that needs to be optimized (maximized or minimized).</li>
<li><strong>Constraints</strong>: Conditions that the solution must satisfy.</li>
<li><strong>Feasible Region</strong>: The set of all possible solutions that satisfy the constraints.</li>
<li><strong>Global Optimum</strong>: The best possible solution across the entire feasible region.</li>
<li><strong>Local Optimum</strong>: The best solution within a neighboring set of solutions.</li>
</ol>
</div><h1><p>Types of Optimization Algorithms</p>
</h1>
<div class='content'></div><h2><ol>
<li>Gradient Descent</li>
</ol>
</h2>
<div class='content'><p>Gradient Descent is an iterative optimization algorithm used for finding the minimum of a function. It is widely used in machine learning for optimizing the cost function.</p>
<p><strong>Algorithm Steps</strong>:</p>
<ol>
<li>Initialize the parameters.</li>
<li>Compute the gradient of the objective function.</li>
<li>Update the parameters in the direction opposite to the gradient.</li>
<li>Repeat steps 2 and 3 until convergence.</li>
</ol>
<p><strong>Mathematical Representation</strong>:
\[ \theta = \theta - \alpha \nabla J(\theta) \]
where:</p>
<ul>
<li>\( \theta \) represents the parameters.</li>
<li>\( \alpha \) is the learning rate.</li>
<li>\( \nabla J(\theta) \) is the gradient of the objective function.</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIE9iamVjdGl2ZSBmdW5jdGlvbjogZih4KSA9IHheMgpkZWYgb2JqZWN0aXZlX2Z1bmN0aW9uKHgpOgogICAgcmV0dXJuIHgqKjIKCiMgR3JhZGllbnQgb2YgdGhlIG9iamVjdGl2ZSBmdW5jdGlvbjogZicoeCkgPSAyeApkZWYgZ3JhZGllbnQoeCk6CiAgICByZXR1cm4gMip4CgojIEdyYWRpZW50IERlc2NlbnQgQWxnb3JpdGhtCmRlZiBncmFkaWVudF9kZXNjZW50KHN0YXJ0aW5nX3BvaW50LCBsZWFybmluZ19yYXRlLCBpdGVyYXRpb25zKToKICAgIHggPSBzdGFydGluZ19wb2ludAogICAgZm9yIF8gaW4gcmFuZ2UoaXRlcmF0aW9ucyk6CiAgICAgICAgZ3JhZCA9IGdyYWRpZW50KHgpCiAgICAgICAgeCA9IHggLSBsZWFybmluZ19yYXRlICogZ3JhZAogICAgcmV0dXJuIHgKCiMgUGFyYW1ldGVycwpzdGFydGluZ19wb2ludCA9IDEwCmxlYXJuaW5nX3JhdGUgPSAwLjEKaXRlcmF0aW9ucyA9IDEwMAoKIyBSdW5uaW5nIHRoZSBhbGdvcml0aG0Kb3B0aW1hbF94ID0gZ3JhZGllbnRfZGVzY2VudChzdGFydGluZ19wb2ludCwgbGVhcm5pbmdfcmF0ZSwgaXRlcmF0aW9ucykKcHJpbnQoZiJUaGUgb3B0aW1hbCB2YWx1ZSBvZiB4IGlzOiB7b3B0aW1hbF94fSIp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Objective function: f(x) = x^2
def objective_function(x):
    return x**2

# Gradient of the objective function: f'(x) = 2x
def gradient(x):
    return 2*x

# Gradient Descent Algorithm
def gradient_descent(starting_point, learning_rate, iterations):
    x = starting_point
    for _ in range(iterations):
        grad = gradient(x)
        x = x - learning_rate * grad
    return x

# Parameters
starting_point = 10
learning_rate = 0.1
iterations = 100

# Running the algorithm
optimal_x = gradient_descent(starting_point, learning_rate, iterations)
print(f&quot;The optimal value of x is: {optimal_x}&quot;)</pre></div><div class='content'></div><h2><ol start="2">
<li>Genetic Algorithms</li>
</ol>
</h2>
<div class='content'><p>Genetic Algorithms (GAs) are inspired by the process of natural selection. They are used to find approximate solutions to optimization and search problems.</p>
<p><strong>Algorithm Steps</strong>:</p>
<ol>
<li>Initialize a population of solutions.</li>
<li>Evaluate the fitness of each solution.</li>
<li>Select the best solutions for reproduction.</li>
<li>Apply crossover and mutation to create new solutions.</li>
<li>Repeat steps 2-4 until convergence.</li>
</ol>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import random

# Objective function: f(x) = x^2
def objective_function(x):
    return x**2

# Generate initial population
def generate_population(size, x_min, x_max):
    return [random.uniform(x_min, x_max) for _ in range(size)]

# Evaluate fitness
def evaluate_population(population):
    return [objective_function(individual) for individual in population]

# Select parents
def select_parents(population, fitness, num_parents):
    parents = sorted(zip(population, fitness), key=lambda x: x[1])
    return [parent[0] for parent in parents[:num_parents]]

# Crossover
def crossover(parents, offspring_size):
    offspring = []
    for _ in range(offspring_size):
        parent1 = random.choice(parents)
        parent2 = random.choice(parents)
        child = (parent1 + parent2) / 2
        offspring.append(child)
    return offspring

# Mutation
def mutate(offspring, mutation_rate, x_min, x_max):
    for i in range(len(offspring)):
        if random.random() &lt; mutation_rate:
            offspring[i] = random.uniform(x_min, x_max)
    return offspring

# Genetic Algorithm
def genetic_algorithm(population_size, x_min, x_max, num_generations, num_parents, mutation_rate):
    population = generate_population(population_size, x_min, x_max)
    for _ in range(num_generations):
        fitness = evaluate_population(population)
        parents = select_parents(population, fitness, num_parents)
        offspring = crossover(parents, population_size - num_parents)
        population = parents + mutate(offspring, mutation_rate, x_min, x_max)
    best_solution = min(population, key=objective_function)
    return best_solution

# Parameters
population_size = 20
x_min = -10
x_max = 10
num_generations = 100
num_parents = 10
mutation_rate = 0.1

# Running the algorithm
optimal_x = genetic_algorithm(population_size, x_min, x_max, num_generations, num_parents, mutation_rate)
print(f&quot;The optimal value of x is: {optimal_x}&quot;)</pre></div><div class='content'></div><h2><ol start="3">
<li>Simulated Annealing</li>
</ol>
</h2>
<div class='content'><p>Simulated Annealing is a probabilistic technique for approximating the global optimum of a given function. It is inspired by the annealing process in metallurgy.</p>
<p><strong>Algorithm Steps</strong>:</p>
<ol>
<li>Start with an initial solution.</li>
<li>Perturb the solution to create a new candidate solution.</li>
<li>Evaluate the new solution.</li>
<li>Accept the new solution with a probability that decreases over time.</li>
<li>Repeat steps 2-4 until convergence.</li>
</ol>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG1hdGgKaW1wb3J0IHJhbmRvbQoKIyBPYmplY3RpdmUgZnVuY3Rpb246IGYoeCkgPSB4XjIKZGVmIG9iamVjdGl2ZV9mdW5jdGlvbih4KToKICAgIHJldHVybiB4KioyCgojIFNpbXVsYXRlZCBBbm5lYWxpbmcgQWxnb3JpdGhtCmRlZiBzaW11bGF0ZWRfYW5uZWFsaW5nKHN0YXJ0aW5nX3BvaW50LCB0ZW1wZXJhdHVyZSwgY29vbGluZ19yYXRlLCBpdGVyYXRpb25zKToKICAgIGN1cnJlbnRfc29sdXRpb24gPSBzdGFydGluZ19wb2ludAogICAgY3VycmVudF92YWx1ZSA9IG9iamVjdGl2ZV9mdW5jdGlvbihjdXJyZW50X3NvbHV0aW9uKQogICAgYmVzdF9zb2x1dGlvbiA9IGN1cnJlbnRfc29sdXRpb24KICAgIGJlc3RfdmFsdWUgPSBjdXJyZW50X3ZhbHVlCgogICAgZm9yIF8gaW4gcmFuZ2UoaXRlcmF0aW9ucyk6CiAgICAgICAgbmV3X3NvbHV0aW9uID0gY3VycmVudF9zb2x1dGlvbiArIHJhbmRvbS51bmlmb3JtKC0xLCAxKQogICAgICAgIG5ld192YWx1ZSA9IG9iamVjdGl2ZV9mdW5jdGlvbihuZXdfc29sdXRpb24pCiAgICAgICAgYWNjZXB0YW5jZV9wcm9iYWJpbGl0eSA9IG1hdGguZXhwKChjdXJyZW50X3ZhbHVlIC0gbmV3X3ZhbHVlKSAvIHRlbXBlcmF0dXJlKQoKICAgICAgICBpZiBuZXdfdmFsdWUgPCBjdXJyZW50X3ZhbHVlIG9yIHJhbmRvbS5yYW5kb20oKSA8IGFjY2VwdGFuY2VfcHJvYmFiaWxpdHk6CiAgICAgICAgICAgIGN1cnJlbnRfc29sdXRpb24gPSBuZXdfc29sdXRpb24KICAgICAgICAgICAgY3VycmVudF92YWx1ZSA9IG5ld192YWx1ZQoKICAgICAgICBpZiBuZXdfdmFsdWUgPCBiZXN0X3ZhbHVlOgogICAgICAgICAgICBiZXN0X3NvbHV0aW9uID0gbmV3X3NvbHV0aW9uCiAgICAgICAgICAgIGJlc3RfdmFsdWUgPSBuZXdfdmFsdWUKCiAgICAgICAgdGVtcGVyYXR1cmUgKj0gY29vbGluZ19yYXRlCgogICAgcmV0dXJuIGJlc3Rfc29sdXRpb24KCiMgUGFyYW1ldGVycwpzdGFydGluZ19wb2ludCA9IDEwCnRlbXBlcmF0dXJlID0gMTAwCmNvb2xpbmdfcmF0ZSA9IDAuOTkKaXRlcmF0aW9ucyA9IDEwMDAKCiMgUnVubmluZyB0aGUgYWxnb3JpdGhtCm9wdGltYWxfeCA9IHNpbXVsYXRlZF9hbm5lYWxpbmcoc3RhcnRpbmdfcG9pbnQsIHRlbXBlcmF0dXJlLCBjb29saW5nX3JhdGUsIGl0ZXJhdGlvbnMpCnByaW50KGYiVGhlIG9wdGltYWwgdmFsdWUgb2YgeCBpczoge29wdGltYWxfeH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import math
import random

# Objective function: f(x) = x^2
def objective_function(x):
    return x**2

# Simulated Annealing Algorithm
def simulated_annealing(starting_point, temperature, cooling_rate, iterations):
    current_solution = starting_point
    current_value = objective_function(current_solution)
    best_solution = current_solution
    best_value = current_value

    for _ in range(iterations):
        new_solution = current_solution + random.uniform(-1, 1)
        new_value = objective_function(new_solution)
        acceptance_probability = math.exp((current_value - new_value) / temperature)

        if new_value &lt; current_value or random.random() &lt; acceptance_probability:
            current_solution = new_solution
            current_value = new_value

        if new_value &lt; best_value:
            best_solution = new_solution
            best_value = new_value

        temperature *= cooling_rate

    return best_solution

# Parameters
starting_point = 10
temperature = 100
cooling_rate = 0.99
iterations = 1000

# Running the algorithm
optimal_x = simulated_annealing(starting_point, temperature, cooling_rate, iterations)
print(f&quot;The optimal value of x is: {optimal_x}&quot;)</pre></div><div class='content'></div><h1><p>Practical Exercises</p>
</h1>
<div class='content'></div><h2><p>Exercise 1: Implement Gradient Descent</p>
</h2>
<div class='content'><p>Implement the gradient descent algorithm to minimize the function \( f(x) = (x-3)^2 \).</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIG9iamVjdGl2ZV9mdW5jdGlvbih4KToKICAgIHJldHVybiAoeCAtIDMpKioyCgpkZWYgZ3JhZGllbnQoeCk6CiAgICByZXR1cm4gMiAqICh4IC0gMykKCmRlZiBncmFkaWVudF9kZXNjZW50KHN0YXJ0aW5nX3BvaW50LCBsZWFybmluZ19yYXRlLCBpdGVyYXRpb25zKToKICAgIHggPSBzdGFydGluZ19wb2ludAogICAgZm9yIF8gaW4gcmFuZ2UoaXRlcmF0aW9ucyk6CiAgICAgICAgZ3JhZCA9IGdyYWRpZW50KHgpCiAgICAgICAgeCA9IHggLSBsZWFybmluZ19yYXRlICogZ3JhZAogICAgcmV0dXJuIHgKCnN0YXJ0aW5nX3BvaW50ID0gMTAKbGVhcm5pbmdfcmF0ZSA9IDAuMQppdGVyYXRpb25zID0gMTAwCgpvcHRpbWFsX3ggPSBncmFkaWVudF9kZXNjZW50KHN0YXJ0aW5nX3BvaW50LCBsZWFybmluZ19yYXRlLCBpdGVyYXRpb25zKQpwcmludChmIlRoZSBvcHRpbWFsIHZhbHVlIG9mIHggaXM6IHtvcHRpbWFsX3h9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def objective_function(x):
    return (x - 3)**2

def gradient(x):
    return 2 * (x - 3)

def gradient_descent(starting_point, learning_rate, iterations):
    x = starting_point
    for _ in range(iterations):
        grad = gradient(x)
        x = x - learning_rate * grad
    return x

starting_point = 10
learning_rate = 0.1
iterations = 100

optimal_x = gradient_descent(starting_point, learning_rate, iterations)
print(f&quot;The optimal value of x is: {optimal_x}&quot;)</pre></div><div class='content'></div><h2><p>Exercise 2: Genetic Algorithm for Function Optimization</p>
</h2>
<div class='content'><p>Use a genetic algorithm to find the minimum of the function \( f(x) = (x-5)^2 \).</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIG9iamVjdGl2ZV9mdW5jdGlvbih4KToKICAgIHJldHVybiAoeCAtIDUpKioyCgpwb3B1bGF0aW9uX3NpemUgPSAyMAp4X21pbiA9IC0xMAp4X21heCA9IDEwCm51bV9nZW5lcmF0aW9ucyA9IDEwMApudW1fcGFyZW50cyA9IDEwCm11dGF0aW9uX3JhdGUgPSAwLjEKCm9wdGltYWxfeCA9IGdlbmV0aWNfYWxnb3JpdGhtKHBvcHVsYXRpb25fc2l6ZSwgeF9taW4sIHhfbWF4LCBudW1fZ2VuZXJhdGlvbnMsIG51bV9wYXJlbnRzLCBtdXRhdGlvbl9yYXRlKQpwcmludChmIlRoZSBvcHRpbWFsIHZhbHVlIG9mIHggaXM6IHtvcHRpbWFsX3h9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def objective_function(x):
    return (x - 5)**2

population_size = 20
x_min = -10
x_max = 10
num_generations = 100
num_parents = 10
mutation_rate = 0.1

optimal_x = genetic_algorithm(population_size, x_min, x_max, num_generations, num_parents, mutation_rate)
print(f&quot;The optimal value of x is: {optimal_x}&quot;)</pre></div><div class='content'></div><h2><p>Exercise 3: Simulated Annealing for Function Optimization</p>
</h2>
<div class='content'><p>Apply simulated annealing to minimize the function \( f(x) = (x+2)^2 \).</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIG9iamVjdGl2ZV9mdW5jdGlvbih4KToKICAgIHJldHVybiAoeCArIDIpKioyCgpzdGFydGluZ19wb2ludCA9IDEwCnRlbXBlcmF0dXJlID0gMTAwCmNvb2xpbmdfcmF0ZSA9IDAuOTkKaXRlcmF0aW9ucyA9IDEwMDAKCm9wdGltYWxfeCA9IHNpbXVsYXRlZF9hbm5lYWxpbmcoc3RhcnRpbmdfcG9pbnQsIHRlbXBlcmF0dXJlLCBjb29saW5nX3JhdGUsIGl0ZXJhdGlvbnMpCnByaW50KGYiVGhlIG9wdGltYWwgdmFsdWUgb2YgeCBpczoge29wdGltYWxfeH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def objective_function(x):
    return (x + 2)**2

starting_point = 10
temperature = 100
cooling_rate = 0.99
iterations = 1000

optimal_x = simulated_annealing(starting_point, temperature, cooling_rate, iterations)
print(f&quot;The optimal value of x is: {optimal_x}&quot;)</pre></div><div class='content'></div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we explored various optimization algorithms, including Gradient Descent, Genetic Algorithms, and Simulated Annealing. Each algorithm has its unique approach and is suitable for different types of optimization problems. By understanding these algorithms and practicing with the provided exercises, you will gain a solid foundation in optimization techniques used in artificial intelligence.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='03-02-search-algorithms' title="Search Algorithms">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-01-basic-concepts-ml' title="Basic Concepts of Machine Learning">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
