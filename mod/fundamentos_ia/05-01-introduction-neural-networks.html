<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Introduction to Neural Networks</title>

    <link rel="alternate" href="https://campusempresa.com/mod/fundamentos_ia/05-01-introduccion-redes-neuronales" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/fundamentos_ia/05-01-introduccion-redes-neuronales" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/fundamentos_ia/05-01-introduction-neural-networks" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/fundamentos_ia/05-01-introduccion-redes-neuronales" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/fundamentos_ia/05-01-introduccion-redes-neuronales" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-04-evaluation-validation' title="Model Evaluation and Validation">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Introduction to Neural Networks</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='05-02-neural-network-architecture' title="Neural Network Architecture">Next &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1><p>Overview</p>
</h1>
<div class='content'><p>Neural networks are a subset of machine learning and are at the heart of deep learning algorithms. They are inspired by the structure and function of the human brain, consisting of interconnected nodes (neurons) that work together to process information. This section will cover the basic concepts, structure, and functioning of neural networks.</p>
</div><h1><p>Key Concepts</p>
</h1>
<div class='content'></div><h2><ol>
<li>Neurons and Layers</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Neurons</strong>: The basic units of a neural network, analogous to the neurons in the human brain. Each neuron receives input, processes it, and passes the output to the next layer.</li>
<li><strong>Layers</strong>: Neural networks are composed of layers of neurons:
<ul>
<li><strong>Input Layer</strong>: The first layer that receives the input data.</li>
<li><strong>Hidden Layers</strong>: Intermediate layers where the actual processing is done through weighted connections.</li>
<li><strong>Output Layer</strong>: The final layer that produces the output.</li>
</ul>
</li>
</ul>
</div><h2><ol start="2">
<li>Activation Functions</li>
</ol>
</h2>
<div class='content'><p>Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. Common activation functions include:</p>
<ul>
<li><strong>Sigmoid</strong>: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>ReLU (Rectified Linear Unit)</strong>: \( \text{ReLU}(x) = \max(0, x) \)</li>
<li><strong>Tanh</strong>: \( \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)</li>
</ul>
</div><h2><ol start="3">
<li>Weights and Biases</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Weights</strong>: Parameters that transform input data within the network. They are adjusted during training to minimize the error.</li>
<li><strong>Biases</strong>: Additional parameters that allow the activation functions to be shifted to the left or right, improving the model's flexibility.</li>
</ul>
</div><h2><ol start="4">
<li>Forward Propagation</li>
</ol>
</h2>
<div class='content'><p>The process of passing input data through the network to get the output. It involves:</p>
<ul>
<li>Multiplying inputs by weights.</li>
<li>Adding biases.</li>
<li>Applying activation functions.</li>
</ul>
</div><h2><ol start="5">
<li>Backpropagation</li>
</ol>
</h2>
<div class='content'><p>A method used to train neural networks by updating weights and biases. It involves:</p>
<ul>
<li>Calculating the error at the output.</li>
<li>Propagating the error backward through the network.</li>
<li>Adjusting weights and biases to minimize the error.</li>
</ul>
</div><h1><p>Example: Simple Neural Network</p>
</h1>
<div class='content'><p>Let's consider a simple neural network with one input layer, one hidden layer, and one output layer.</p>
</div><h2><p>Structure</p>
</h2>
<div class='content'><ul>
<li><strong>Input Layer</strong>: 2 neurons (for two input features).</li>
<li><strong>Hidden Layer</strong>: 3 neurons.</li>
<li><strong>Output Layer</strong>: 1 neuron (for binary classification).</li>
</ul>
</div><h2><p>Code Example</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIFNpZ21vaWQgYWN0aXZhdGlvbiBmdW5jdGlvbgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKIyBEZXJpdmF0aXZlIG9mIHRoZSBzaWdtb2lkIGZ1bmN0aW9uCmRlZiBzaWdtb2lkX2Rlcml2YXRpdmUoeCk6CiAgICByZXR1cm4geCAqICgxIC0geCkKCiMgSW5wdXQgZGF0YXNldAppbnB1dHMgPSBucC5hcnJheShbWzAsIDBdLCBbMCwgMV0sIFsxLCAwXSwgWzEsIDFdXSkKCiMgT3V0cHV0IGRhdGFzZXQKb3V0cHV0cyA9IG5wLmFycmF5KFtbMF0sIFsxXSwgWzFdLCBbMF1dKQoKIyBTZWVkIGZvciByZXByb2R1Y2liaWxpdHkKbnAucmFuZG9tLnNlZWQoNDIpCgojIEluaXRpYWxpemUgd2VpZ2h0cyBhbmQgYmlhc2VzCmlucHV0X2xheWVyX25ldXJvbnMgPSBpbnB1dHMuc2hhcGVbMV0KaGlkZGVuX2xheWVyX25ldXJvbnMgPSAzCm91dHB1dF9uZXVyb25zID0gMQoKIyBXZWlnaHRzIGFuZCBiaWFzZXMKaGlkZGVuX3dlaWdodHMgPSBucC5yYW5kb20udW5pZm9ybShzaXplPShpbnB1dF9sYXllcl9uZXVyb25zLCBoaWRkZW5fbGF5ZXJfbmV1cm9ucykpCmhpZGRlbl9iaWFzID0gbnAucmFuZG9tLnVuaWZvcm0oc2l6ZT0oMSwgaGlkZGVuX2xheWVyX25ldXJvbnMpKQpvdXRwdXRfd2VpZ2h0cyA9IG5wLnJhbmRvbS51bmlmb3JtKHNpemU9KGhpZGRlbl9sYXllcl9uZXVyb25zLCBvdXRwdXRfbmV1cm9ucykpCm91dHB1dF9iaWFzID0gbnAucmFuZG9tLnVuaWZvcm0oc2l6ZT0oMSwgb3V0cHV0X25ldXJvbnMpKQoKIyBUcmFpbmluZyBwYXJhbWV0ZXJzCmxlYXJuaW5nX3JhdGUgPSAwLjEKZXBvY2hzID0gMTAwMDAKCiMgVHJhaW5pbmcgdGhlIG5ldXJhbCBuZXR3b3JrCmZvciBlcG9jaCBpbiByYW5nZShlcG9jaHMpOgogICAgIyBGb3J3YXJkIFByb3BhZ2F0aW9uCiAgICBoaWRkZW5fbGF5ZXJfYWN0aXZhdGlvbiA9IG5wLmRvdChpbnB1dHMsIGhpZGRlbl93ZWlnaHRzKQogICAgaGlkZGVuX2xheWVyX2FjdGl2YXRpb24gKz0gaGlkZGVuX2JpYXMKICAgIGhpZGRlbl9sYXllcl9vdXRwdXQgPSBzaWdtb2lkKGhpZGRlbl9sYXllcl9hY3RpdmF0aW9uKQoKICAgIG91dHB1dF9sYXllcl9hY3RpdmF0aW9uID0gbnAuZG90KGhpZGRlbl9sYXllcl9vdXRwdXQsIG91dHB1dF93ZWlnaHRzKQogICAgb3V0cHV0X2xheWVyX2FjdGl2YXRpb24gKz0gb3V0cHV0X2JpYXMKICAgIHByZWRpY3RlZF9vdXRwdXQgPSBzaWdtb2lkKG91dHB1dF9sYXllcl9hY3RpdmF0aW9uKQoKICAgICMgQmFja3Byb3BhZ2F0aW9uCiAgICBlcnJvciA9IG91dHB1dHMgLSBwcmVkaWN0ZWRfb3V0cHV0CiAgICBkX3ByZWRpY3RlZF9vdXRwdXQgPSBlcnJvciAqIHNpZ21vaWRfZGVyaXZhdGl2ZShwcmVkaWN0ZWRfb3V0cHV0KQoKICAgIGVycm9yX2hpZGRlbl9sYXllciA9IGRfcHJlZGljdGVkX291dHB1dC5kb3Qob3V0cHV0X3dlaWdodHMuVCkKICAgIGRfaGlkZGVuX2xheWVyID0gZXJyb3JfaGlkZGVuX2xheWVyICogc2lnbW9pZF9kZXJpdmF0aXZlKGhpZGRlbl9sYXllcl9vdXRwdXQpCgogICAgIyBVcGRhdGluZyBXZWlnaHRzIGFuZCBCaWFzZXMKICAgIG91dHB1dF93ZWlnaHRzICs9IGhpZGRlbl9sYXllcl9vdXRwdXQuVC5kb3QoZF9wcmVkaWN0ZWRfb3V0cHV0KSAqIGxlYXJuaW5nX3JhdGUKICAgIG91dHB1dF9iaWFzICs9IG5wLnN1bShkX3ByZWRpY3RlZF9vdXRwdXQsIGF4aXM9MCwga2VlcGRpbXM9VHJ1ZSkgKiBsZWFybmluZ19yYXRlCiAgICBoaWRkZW5fd2VpZ2h0cyArPSBpbnB1dHMuVC5kb3QoZF9oaWRkZW5fbGF5ZXIpICogbGVhcm5pbmdfcmF0ZQogICAgaGlkZGVuX2JpYXMgKz0gbnAuc3VtKGRfaGlkZGVuX2xheWVyLCBheGlzPTAsIGtlZXBkaW1zPVRydWUpICogbGVhcm5pbmdfcmF0ZQoKIyBPdXRwdXQgYWZ0ZXIgdHJhaW5pbmcKcHJpbnQoIk91dHB1dCBhZnRlciB0cmFpbmluZzoiKQpwcmludChwcmVkaWN0ZWRfb3V0cHV0KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of the sigmoid function
def sigmoid_derivative(x):
    return x * (1 - x)

# Input dataset
inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# Output dataset
outputs = np.array([[0], [1], [1], [0]])

# Seed for reproducibility
np.random.seed(42)

# Initialize weights and biases
input_layer_neurons = inputs.shape[1]
hidden_layer_neurons = 3
output_neurons = 1

# Weights and biases
hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))
output_weights = np.random.uniform(size=(hidden_layer_neurons, output_neurons))
output_bias = np.random.uniform(size=(1, output_neurons))

# Training parameters
learning_rate = 0.1
epochs = 10000

# Training the neural network
for epoch in range(epochs):
    # Forward Propagation
    hidden_layer_activation = np.dot(inputs, hidden_weights)
    hidden_layer_activation += hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_activation)

    output_layer_activation = np.dot(hidden_layer_output, output_weights)
    output_layer_activation += output_bias
    predicted_output = sigmoid(output_layer_activation)

    # Backpropagation
    error = outputs - predicted_output
    d_predicted_output = error * sigmoid_derivative(predicted_output)

    error_hidden_layer = d_predicted_output.dot(output_weights.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

    # Updating Weights and Biases
    output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    hidden_weights += inputs.T.dot(d_hidden_layer) * learning_rate
    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

# Output after training
print(&quot;Output after training:&quot;)
print(predicted_output)</pre></div><div class='content'></div><h2><p>Explanation</p>
</h2>
<div class='content'><ol>
<li><strong>Initialization</strong>: We initialize the weights and biases randomly.</li>
<li><strong>Forward Propagation</strong>: We calculate the activations for the hidden and output layers using the sigmoid function.</li>
<li><strong>Backpropagation</strong>: We compute the error and update the weights and biases to minimize this error.</li>
<li><strong>Training Loop</strong>: We repeat the forward and backward propagation steps for a specified number of epochs.</li>
</ol>
</div><h1><p>Practical Exercise</p>
</h1>
<div class='content'></div><h2><p>Task</p>
</h2>
<div class='content'><p>Implement a neural network to solve the XOR problem using the provided code template. Modify the number of neurons in the hidden layer and observe the changes in the output.</p>
</div><h2><p>Solution</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBNb2RpZnkgdGhlIG51bWJlciBvZiBuZXVyb25zIGluIHRoZSBoaWRkZW4gbGF5ZXIKaGlkZGVuX2xheWVyX25ldXJvbnMgPSA0ICAjIENoYW5nZSB0aGlzIHZhbHVlIGFuZCBvYnNlcnZlIHRoZSByZXN1bHRzCgojIFJlc3Qgb2YgdGhlIGNvZGUgcmVtYWlucyB0aGUgc2FtZQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Modify the number of neurons in the hidden layer
hidden_layer_neurons = 4  # Change this value and observe the results

# Rest of the code remains the same</pre></div><div class='content'></div><h2><p>Common Mistakes</p>
</h2>
<div class='content'><ul>
<li><strong>Incorrect Weight Initialization</strong>: Ensure weights are initialized properly to avoid vanishing or exploding gradients.</li>
<li><strong>Learning Rate</strong>: A very high or very low learning rate can hinder the training process.</li>
<li><strong>Overfitting</strong>: Using too many neurons or layers can lead to overfitting. Regularization techniques can help mitigate this.</li>
</ul>
</div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we introduced the basic concepts of neural networks, including neurons, layers, activation functions, weights, biases, forward propagation, and backpropagation. We also provided a simple example of a neural network and a practical exercise to reinforce the concepts. Understanding these fundamentals is crucial for delving deeper into more complex neural network architectures and applications.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-04-evaluation-validation' title="Model Evaluation and Validation">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='05-02-neural-network-architecture' title="Neural Network Architecture">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
