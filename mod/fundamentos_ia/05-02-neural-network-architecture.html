<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Network Architecture</title>

    <link rel="alternate" href="https://campusempresa.com/mod/fundamentos_ia/05-02-arquitectura-redes-neuronales" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/fundamentos_ia/05-02-arquitectura-redes-neuronales" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/fundamentos_ia/05-02-neural-network-architecture" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/fundamentos_ia/05-02-arquitectura-redes-neuronales" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/fundamentos_ia/05-02-arquitectura-redes-neuronales" class="px-2">CA</a>
					</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<!-- <a href="/">Home</a>  -->
									<a href="./">Course Content</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Technical Skills</a>
				<a href="/all/conocimientos">Knowledge</a>
				<a href="/all/soft_skills">Social Skills</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-01-introduction-neural-networks' title="Introduction to Neural Networks">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Neural Network Architecture</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='05-03-deep-learning-applications' title="Deep Learning and its Applications">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>In this section, we will delve into the architecture of neural networks, which are the backbone of many modern AI applications. Understanding the structure and components of neural networks is crucial for designing and implementing effective AI models.</p>
</div><h1><p>Key Concepts</p>
</h1>
<div class='content'><ol>
<li>
<p><strong>Neurons and Layers</strong></p>
<ul>
<li><strong>Neurons</strong>: The basic units of a neural network, analogous to biological neurons. Each neuron receives input, processes it, and passes the output to the next layer.</li>
<li><strong>Layers</strong>: Neural networks are composed of multiple layers of neurons. The three main types of layers are:
<ul>
<li><strong>Input Layer</strong>: The first layer that receives the initial data.</li>
<li><strong>Hidden Layers</strong>: Intermediate layers that process inputs from the input layer.</li>
<li><strong>Output Layer</strong>: The final layer that produces the network's output.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Activation Functions</strong></p>
<ul>
<li>Functions that determine the output of a neuron given an input or set of inputs. Common activation functions include:
<ul>
<li><strong>Sigmoid</strong>: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>ReLU (Rectified Linear Unit)</strong>: \( \text{ReLU}(x) = \max(0, x) \)</li>
<li><strong>Tanh</strong>: \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Weights and Biases</strong></p>
<ul>
<li><strong>Weights</strong>: Parameters that transform input data within the network. Each connection between neurons has an associated weight.</li>
<li><strong>Biases</strong>: Additional parameters that allow the activation function to be shifted to the left or right, improving the model's flexibility.</li>
</ul>
</li>
<li>
<p><strong>Forward Propagation</strong></p>
<ul>
<li>The process by which input data is passed through the network, layer by layer, to generate an output.</li>
</ul>
</li>
<li>
<p><strong>Backpropagation</strong></p>
<ul>
<li>A method used to train neural networks by adjusting weights and biases based on the error of the output. It involves:
<ul>
<li>Calculating the error at the output.</li>
<li>Propagating this error backward through the network.</li>
<li>Updating the weights and biases to minimize the error.</li>
</ul>
</li>
</ul>
</li>
</ol>
</div><h1><p>Example: Simple Neural Network</p>
</h1>
<div class='content'><p>Let's consider a simple neural network with one input layer, one hidden layer, and one output layer.</p>
</div><h2><p>Structure</p>
</h2>
<div class='content'><ul>
<li><strong>Input Layer</strong>: 3 neurons (features)</li>
<li><strong>Hidden Layer</strong>: 4 neurons</li>
<li><strong>Output Layer</strong>: 1 neuron (binary classification)</li>
</ul>
</div><h2><p>Diagram</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("SW5wdXQgTGF5ZXIgICAgICAgSGlkZGVuIExheWVyICAgICAgIE91dHB1dCBMYXllcgogICgzIG5ldXJvbnMpICAgICAgICg0IG5ldXJvbnMpICAgICAgICAoMSBuZXVyb24pCiAgICAgbyAgICAgICAgICAgICAgICAgIG8gICAgICAgICAgICAgICAgICBvCiAgICAgbyAgICAgICAgICAgICAgICAgIG8KICAgICBvICAgICAgICAgICAgICAgICAgbwogICAgICAgICAgICAgICAgICAgICAgICBv"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>Input Layer       Hidden Layer       Output Layer
  (3 neurons)       (4 neurons)        (1 neuron)
     o                  o                  o
     o                  o
     o                  o
                        o</pre></div><div class='content'></div><h2><p>Code Example</p>
</h2>
<div class='content'><p>Here's a basic implementation of a neural network using Python and NumPy:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEFjdGl2YXRpb24gZnVuY3Rpb24gYW5kIGl0cyBkZXJpdmF0aXZlCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgpkZWYgc2lnbW9pZF9kZXJpdmF0aXZlKHgpOgogICAgcmV0dXJuIHggKiAoMSAtIHgpCgojIElucHV0IGRhdGEgKDMgZmVhdHVyZXMpCmlucHV0cyA9IG5wLmFycmF5KFtbMCwgMCwgMV0sCiAgICAgICAgICAgICAgICAgICBbMSwgMSwgMV0sCiAgICAgICAgICAgICAgICAgICBbMSwgMCwgMV0sCiAgICAgICAgICAgICAgICAgICBbMCwgMSwgMV1dKQoKIyBFeHBlY3RlZCBvdXRwdXQgKGJpbmFyeSBjbGFzc2lmaWNhdGlvbikKb3V0cHV0cyA9IG5wLmFycmF5KFtbMF0sIFsxXSwgWzFdLCBbMF1dKQoKIyBTZWVkIGZvciByZXByb2R1Y2liaWxpdHkKbnAucmFuZG9tLnNlZWQoMSkKCiMgSW5pdGlhbGl6ZSB3ZWlnaHRzIHJhbmRvbWx5IHdpdGggbWVhbiAwCndlaWdodHNfaW5wdXRfaGlkZGVuID0gMiAqIG5wLnJhbmRvbS5yYW5kb20oKDMsIDQpKSAtIDEKd2VpZ2h0c19oaWRkZW5fb3V0cHV0ID0gMiAqIG5wLnJhbmRvbS5yYW5kb20oKDQsIDEpKSAtIDEKCiMgVHJhaW5pbmcgcHJvY2Vzcwpmb3IgZXBvY2ggaW4gcmFuZ2UoMTAwMDApOgogICAgIyBGb3J3YXJkIHByb3BhZ2F0aW9uCiAgICBpbnB1dF9sYXllciA9IGlucHV0cwogICAgaGlkZGVuX2xheWVyID0gc2lnbW9pZChucC5kb3QoaW5wdXRfbGF5ZXIsIHdlaWdodHNfaW5wdXRfaGlkZGVuKSkKICAgIG91dHB1dF9sYXllciA9IHNpZ21vaWQobnAuZG90KGhpZGRlbl9sYXllciwgd2VpZ2h0c19oaWRkZW5fb3V0cHV0KSkKICAgIAogICAgIyBDYWxjdWxhdGUgZXJyb3IKICAgIG91dHB1dF9lcnJvciA9IG91dHB1dHMgLSBvdXRwdXRfbGF5ZXIKICAgIAogICAgIyBCYWNrcHJvcGFnYXRpb24KICAgIG91dHB1dF9kZWx0YSA9IG91dHB1dF9lcnJvciAqIHNpZ21vaWRfZGVyaXZhdGl2ZShvdXRwdXRfbGF5ZXIpCiAgICBoaWRkZW5fZXJyb3IgPSBvdXRwdXRfZGVsdGEuZG90KHdlaWdodHNfaGlkZGVuX291dHB1dC5UKQogICAgaGlkZGVuX2RlbHRhID0gaGlkZGVuX2Vycm9yICogc2lnbW9pZF9kZXJpdmF0aXZlKGhpZGRlbl9sYXllcikKICAgIAogICAgIyBVcGRhdGUgd2VpZ2h0cwogICAgd2VpZ2h0c19oaWRkZW5fb3V0cHV0ICs9IGhpZGRlbl9sYXllci5ULmRvdChvdXRwdXRfZGVsdGEpCiAgICB3ZWlnaHRzX2lucHV0X2hpZGRlbiArPSBpbnB1dF9sYXllci5ULmRvdChoaWRkZW5fZGVsdGEpCgojIE91dHB1dCBhZnRlciB0cmFpbmluZwpwcmludCgiT3V0cHV0IGFmdGVyIHRyYWluaW5nOiIpCnByaW50KG91dHB1dF9sYXllcik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Input data (3 features)
inputs = np.array([[0, 0, 1],
                   [1, 1, 1],
                   [1, 0, 1],
                   [0, 1, 1]])

# Expected output (binary classification)
outputs = np.array([[0], [1], [1], [0]])

# Seed for reproducibility
np.random.seed(1)

# Initialize weights randomly with mean 0
weights_input_hidden = 2 * np.random.random((3, 4)) - 1
weights_hidden_output = 2 * np.random.random((4, 1)) - 1

# Training process
for epoch in range(10000):
    # Forward propagation
    input_layer = inputs
    hidden_layer = sigmoid(np.dot(input_layer, weights_input_hidden))
    output_layer = sigmoid(np.dot(hidden_layer, weights_hidden_output))
    
    # Calculate error
    output_error = outputs - output_layer
    
    # Backpropagation
    output_delta = output_error * sigmoid_derivative(output_layer)
    hidden_error = output_delta.dot(weights_hidden_output.T)
    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer)
    
    # Update weights
    weights_hidden_output += hidden_layer.T.dot(output_delta)
    weights_input_hidden += input_layer.T.dot(hidden_delta)

# Output after training
print(&quot;Output after training:&quot;)
print(output_layer)</pre></div><div class='content'></div><h2><p>Explanation</p>
</h2>
<div class='content'><ul>
<li><strong>Initialization</strong>: We initialize the weights for the input-to-hidden and hidden-to-output layers randomly.</li>
<li><strong>Forward Propagation</strong>: We calculate the activations for the hidden and output layers using the sigmoid function.</li>
<li><strong>Error Calculation</strong>: We compute the error at the output layer.</li>
<li><strong>Backpropagation</strong>: We propagate the error backward, calculating the deltas for the hidden and output layers.</li>
<li><strong>Weight Update</strong>: We update the weights to minimize the error.</li>
</ul>
</div><h1><p>Practical Exercise</p>
</h1>
<div class='content'></div><h2><p>Exercise</p>
</h2>
<div class='content'><p>Implement a neural network with the following specifications:</p>
<ul>
<li>Input Layer: 2 neurons</li>
<li>Hidden Layer: 3 neurons</li>
<li>Output Layer: 1 neuron</li>
</ul>
<p>Use the following input data and expected output:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW5wdXRzID0gbnAuYXJyYXkoW1swLCAwXSwKICAgICAgICAgICAgICAgICAgIFswLCAxXSwKICAgICAgICAgICAgICAgICAgIFsxLCAwXSwKICAgICAgICAgICAgICAgICAgIFsxLCAxXV0pCgpvdXRwdXRzID0gbnAuYXJyYXkoW1swXSwgWzFdLCBbMV0sIFswXV0p"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>inputs = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]])

outputs = np.array([[0], [1], [1], [0]])</pre></div><div class='content'></div><h2><p>Solution</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKZGVmIHNpZ21vaWRfZGVyaXZhdGl2ZSh4KToKICAgIHJldHVybiB4ICogKDEgLSB4KQoKaW5wdXRzID0gbnAuYXJyYXkoW1swLCAwXSwKICAgICAgICAgICAgICAgICAgIFswLCAxXSwKICAgICAgICAgICAgICAgICAgIFsxLCAwXSwKICAgICAgICAgICAgICAgICAgIFsxLCAxXV0pCgpvdXRwdXRzID0gbnAuYXJyYXkoW1swXSwgWzFdLCBbMV0sIFswXV0pCgpucC5yYW5kb20uc2VlZCgxKQoKd2VpZ2h0c19pbnB1dF9oaWRkZW4gPSAyICogbnAucmFuZG9tLnJhbmRvbSgoMiwgMykpIC0gMQp3ZWlnaHRzX2hpZGRlbl9vdXRwdXQgPSAyICogbnAucmFuZG9tLnJhbmRvbSgoMywgMSkpIC0gMQoKZm9yIGVwb2NoIGluIHJhbmdlKDEwMDAwKToKICAgIGlucHV0X2xheWVyID0gaW5wdXRzCiAgICBoaWRkZW5fbGF5ZXIgPSBzaWdtb2lkKG5wLmRvdChpbnB1dF9sYXllciwgd2VpZ2h0c19pbnB1dF9oaWRkZW4pKQogICAgb3V0cHV0X2xheWVyID0gc2lnbW9pZChucC5kb3QoaGlkZGVuX2xheWVyLCB3ZWlnaHRzX2hpZGRlbl9vdXRwdXQpKQogICAgCiAgICBvdXRwdXRfZXJyb3IgPSBvdXRwdXRzIC0gb3V0cHV0X2xheWVyCiAgICAKICAgIG91dHB1dF9kZWx0YSA9IG91dHB1dF9lcnJvciAqIHNpZ21vaWRfZGVyaXZhdGl2ZShvdXRwdXRfbGF5ZXIpCiAgICBoaWRkZW5fZXJyb3IgPSBvdXRwdXRfZGVsdGEuZG90KHdlaWdodHNfaGlkZGVuX291dHB1dC5UKQogICAgaGlkZGVuX2RlbHRhID0gaGlkZGVuX2Vycm9yICogc2lnbW9pZF9kZXJpdmF0aXZlKGhpZGRlbl9sYXllcikKICAgIAogICAgd2VpZ2h0c19oaWRkZW5fb3V0cHV0ICs9IGhpZGRlbl9sYXllci5ULmRvdChvdXRwdXRfZGVsdGEpCiAgICB3ZWlnaHRzX2lucHV0X2hpZGRlbiArPSBpbnB1dF9sYXllci5ULmRvdChoaWRkZW5fZGVsdGEpCgpwcmludCgiT3V0cHV0IGFmdGVyIHRyYWluaW5nOiIpCnByaW50KG91dHB1dF9sYXllcik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

inputs = np.array([[0, 0],
                   [0, 1],
                   [1, 0],
                   [1, 1]])

outputs = np.array([[0], [1], [1], [0]])

np.random.seed(1)

weights_input_hidden = 2 * np.random.random((2, 3)) - 1
weights_hidden_output = 2 * np.random.random((3, 1)) - 1

for epoch in range(10000):
    input_layer = inputs
    hidden_layer = sigmoid(np.dot(input_layer, weights_input_hidden))
    output_layer = sigmoid(np.dot(hidden_layer, weights_hidden_output))
    
    output_error = outputs - output_layer
    
    output_delta = output_error * sigmoid_derivative(output_layer)
    hidden_error = output_delta.dot(weights_hidden_output.T)
    hidden_delta = hidden_error * sigmoid_derivative(hidden_layer)
    
    weights_hidden_output += hidden_layer.T.dot(output_delta)
    weights_input_hidden += input_layer.T.dot(hidden_delta)

print(&quot;Output after training:&quot;)
print(output_layer)</pre></div><div class='content'></div><h2><p>Common Mistakes and Tips</p>
</h2>
<div class='content'><ul>
<li><strong>Incorrect Weight Initialization</strong>: Ensure weights are initialized randomly but within a small range.</li>
<li><strong>Learning Rate</strong>: Adjust the learning rate if the network is not converging.</li>
<li><strong>Overfitting</strong>: Use techniques like regularization if the network performs well on training data but poorly on test data.</li>
</ul>
</div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we explored the architecture of neural networks, including neurons, layers, activation functions, weights, biases, forward propagation, and backpropagation. We also implemented a simple neural network and provided an exercise to reinforce the concepts. Understanding these fundamentals is crucial for building more complex AI models and diving deeper into advanced topics like deep learning.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-01-introduction-neural-networks' title="Introduction to Neural Networks">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='05-03-deep-learning-applications' title="Deep Learning and its Applications">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
