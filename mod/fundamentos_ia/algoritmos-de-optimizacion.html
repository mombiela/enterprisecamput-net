<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimization Algorithms</title>

    <link rel="alternate" href="https://campusempresa.com/mod/fundamentos_ia/algoritmos-de-optimizacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/fundamentos_ia/algoritmos-de-optimizacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/fundamentos_ia/algoritmos-de-optimizacion" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body  class="test" >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/fundamentos_ia/algoritmos-de-optimizacion" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/fundamentos_ia/algoritmos-de-optimizacion" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
									<div class="assert">
						<p><b>Attention!</b> There has been an error in the course generation, and it may contain translation errors.We are working to resolve this issue, so please use the content with caution.You can check the correct content in another language at the following link:<br>
						<a href="https://campusempresa.com/mod/fundamentos_ia/algoritmos-de-optimizacion">https://campusempresa.com/mod/fundamentos_ia/algoritmos-de-optimizacion</a></p>
					</div>
								<div class='content'></div><h1>Introduction to Optimization Algorithms</h1>
<div class='content'><p>Optimization algorithms are fundamental in the field of Artificial Intelligence (AI) and machine learning. These algorithms are used to find the best possible solution (or a sufficiently good solution) to a given problem within a set of possible solutions. Optimization is crucial in various applications, from route planning to hyperparameter tuning in machine learning models.</p>
</div><h1>Types of Optimization Algorithms</h1>
<div class='content'></div><h2>Convex Optimization</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: Convex optimization refers to a subfield of optimization where the objective and constraints are convex functions. This means that any linear combination of two points in the function's domain is also in the domain.</li>
<li><strong>Examples of problems</strong>:
<ul>
<li>Linear programming</li>
<li>Quadratic programming</li>
</ul>
</li>
<li><strong>Common methods</strong>:
<ul>
<li>Gradient method</li>
<li>Conjugate gradient method</li>
<li>Newton's method</li>
</ul>
</li>
</ul>
</div><h2>Non-Convex Optimization</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: In non-convex optimization, the objective or constraints are not necessarily convex, which can lead to multiple local optima.</li>
<li><strong>Examples of problems</strong>:
<ul>
<li>Deep neural networks</li>
<li>Non-linear function optimization</li>
</ul>
</li>
<li><strong>Common methods</strong>:
<ul>
<li>Genetic algorithms</li>
<li>Simulated Annealing</li>
<li>Particle Swarm Optimization (PSO)</li>
</ul>
</li>
</ul>
</div><h2>Discrete Optimization</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: Discrete optimization deals with problems where the decision variables are discrete, such as integers or booleans.</li>
<li><strong>Examples of problems</strong>:
<ul>
<li>Traveling Salesman Problem (TSP)</li>
<li>Knapsack problem</li>
</ul>
</li>
<li><strong>Common methods</strong>:
<ul>
<li>Dynamic programming</li>
<li>Search algorithms (A*, BFS, DFS)</li>
</ul>
</li>
</ul>
</div><h1>Common Optimization Methods</h1>
<div class='content'></div><h2>Gradient Descent Method</h2>
<div class='content'><ul>
<li>
<p><strong>Description</strong>: It is an iterative algorithm used to minimize an objective function. It is based on moving in the opposite direction of the function's gradient at the current point.</p>
</li>
<li>
<p><strong>Formula</strong>:</p>
<pre><code class="language-python">θ = θ - α * ∇J(θ)
</code></pre>
<p>where:</p>
<ul>
<li><code>θ</code> is the parameter vector.</li>
<li><code>α</code> is the learning rate.</li>
<li><code>∇J(θ)</code> is the gradient of the cost function <code>J</code> with respect to <code>θ</code>.</li>
</ul>
</li>
<li>
<p><strong>Code example</strong>:</p>
<pre><code class="language-python">import numpy as np

def gradient_descent(X, y, theta, learning_rate, iterations):
    m = len(y)
    for _ in range(iterations):
        gradient = X.T.dot(X.dot(theta) - y) / m
        theta = theta - learning_rate * gradient
    return theta

# Example data
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3
theta = np.zeros(2)

# Apply gradient descent method
theta = gradient_descent(X, y, theta, 0.01, 1000)
print(theta)
</code></pre>
</li>
</ul>
</div><h2>Genetic Algorithms</h2>
<div class='content'><ul>
<li>
<p><strong>Description</strong>: Genetic algorithms are search methods inspired by Darwin's theory of evolution. They use operations such as selection, crossover, and mutation to evolve a population of solutions.</p>
</li>
<li>
<p><strong>Main steps</strong>:</p>
<ul>
<li>Initialization</li>
<li>Evaluation</li>
<li>Selection</li>
<li>Crossover</li>
<li>Mutation</li>
</ul>
</li>
<li>
<p><strong>Code example</strong>:</p>
<pre><code class="language-python">import random

def genetic_algorithm(population, fitness_fn, mutation_rate=0.01, generations=100):
    for _ in range(generations):
        population = sorted(population, key=fitness_fn, reverse=True)
        next_generation = population[:2]  # Elitism: keep the best two
        for _ in range(len(population) // 2 - 1):
            parents = random.sample(population[:10], 2)
            crossover_point = random.randint(1, len(parents[0]) - 1)
            child1 = parents[0][:crossover_point] + parents[1][crossover_point:]
            child2 = parents[1][:crossover_point] + parents[0][crossover_point:]
            next_generation += [mutate(child1, mutation_rate), mutate(child2, mutation_rate)]
        population = next_generation
    return max(population, key=fitness_fn)

def mutate(individual, mutation_rate):
    return [gene if random.random() &gt; mutation_rate else random.choice([0, 1]) for gene in individual]

# Example data
population = [[random.choice([0, 1]) for _ in range(10)] for _ in range(20)]
fitness_fn = lambda x: sum(x)  # Example fitness function

# Apply genetic algorithm
best_solution = genetic_algorithm(population, fitness_fn)
print(best_solution)
</code></pre>
</li>
</ul>
</div><h1>Comparison of Optimization Methods</h1>
<div class='content'><table>
<thead>
<tr>
<th>Method</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gradient Descent</td>
<td>Simple, efficient for convex problems</td>
<td>Can get stuck in local optima</td>
</tr>
<tr>
<td>Genetic Algorithms</td>
<td>No derivatives required, handles local optima well</td>
<td>Computationally expensive, parameters hard to tune</td>
</tr>
<tr>
<td>Simulated Annealing</td>
<td>Escapes local optima</td>
<td>Slow, requires parameter tuning</td>
</tr>
<tr>
<td>Particle Swarm Optimization (PSO)</td>
<td>Easy to implement, good exploration</td>
<td>Can converge prematurely, parameters hard to tune</td>
</tr>
</tbody>
</table>
</div><h1>Conclusion</h1>
<div class='content'><p>Optimization algorithms are powerful tools in Artificial Intelligence, allowing for finding optimal or near-optimal solutions to complex problems. From classic methods like gradient descent to more advanced techniques like genetic algorithms, each approach has its own advantages and disadvantages. Understanding these methods and knowing when to apply them is essential for any professional in the AI field.</p>
</div>
			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
