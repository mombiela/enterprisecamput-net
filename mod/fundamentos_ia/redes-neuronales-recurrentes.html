<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recurrent Neural Networks (RNN)</title>

    <link rel="alternate" href="https://campusempresa.com/mod/fundamentos_ia/redes-neuronales-recurrentes" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/fundamentos_ia/redes-neuronales-recurrentes" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/fundamentos_ia/redes-neuronales-recurrentes" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body  class="test" >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/fundamentos_ia/redes-neuronales-recurrentes" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/fundamentos_ia/redes-neuronales-recurrentes" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
									<div class="assert">
						<p><b>Attention!</b> There has been an error in the course generation, and it may contain translation errors.We are working to resolve this issue, so please use the content with caution.You can check the correct content in another language at the following link:<br>
						<a href="https://campusempresa.com/mod/fundamentos_ia/redes-neuronales-recurrentes">https://campusempresa.com/mod/fundamentos_ia/redes-neuronales-recurrentes</a></p>
					</div>
								<div class='content'></div><h1>Introduction to Recurrent Neural Networks (RNN)</h1>
<div class='content'><p>Recurrent Neural Networks (RNN) are a type of neural network designed to recognize patterns in sequences of data, such as text, time series, and audio data. Unlike traditional neural networks, RNNs have recurrent connections that allow information to persist.</p>
<ul>
<li><strong>Data sequences</strong>: RNNs are especially useful for sequential data where the order of the data is important.</li>
<li><strong>Memory</strong>: RNNs have a &quot;memory&quot; that allows them to remember information about previous data in the sequence.</li>
<li><strong>Applications</strong>: Natural language processing (NLP), machine translation, speech recognition, time series analysis.</li>
</ul>
</div><h1>Architecture of an RNN</h1>
<div class='content'><p>A typical RNN consists of an input layer, one or more recurrent layers, and an output layer.</p>
<ul>
<li><strong>Input layer</strong>: Receives the data sequence.</li>
<li><strong>Recurrent layer</strong>: Processes the data sequence, maintaining a memory of previous states.</li>
<li><strong>Output layer</strong>: Produces the final output after processing the entire sequence.</li>
</ul>
</div><h2>Structure of an RNN</h2>
<div class='content'><p>The basic structure of an RNN can be represented as:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("eF90IC0+IGhfdCAtPiB5X3Q="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>x_t -&gt; h_t -&gt; y_t</pre></div><div class='content'><p>Where:</p>
<ul>
<li><code>x_t</code> is the input at time <code>t</code>.</li>
<li><code>h_t</code> is the hidden state at time <code>t</code>.</li>
<li><code>y_t</code> is the output at time <code>t</code>.</li>
</ul>
<p>The relationship between these elements can be expressed through the following equations:</p>
<ul>
<li><code>h_t = f(W_hh * h_{t-1} + W_xh * x_t + b_h)</code></li>
<li><code>y_t = g(W_hy * h_t + b_y)</code></li>
</ul>
<p>Here, <code>W_hh</code>, <code>W_xh</code>, and <code>W_hy</code> are weight matrices, <code>b_h</code> and <code>b_y</code> are biases, and <code>f</code> and <code>g</code> are activation functions.</p>
</div><h1>Code Example: Basic Implementation of an RNN in Python</h1>
<div class='content'><p>Below is a basic example of an RNN using the <code>TensorFlow</code> library in Python.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBTaW1wbGVSTk4sIERlbnNlCgojIERlZmluZSB0aGUgaW5wdXQgZGF0YSBzZXF1ZW5jZQppbnB1dF9kYXRhID0gdGYucmFuZG9tLm5vcm1hbChbMzIsIDEwLCA4XSkgICMgKGJhdGNoX3NpemUsIHRpbWVzdGVwcywgaW5wdXRfZGltKQoKIyBDcmVhdGUgdGhlIFJOTiBtb2RlbAptb2RlbCA9IFNlcXVlbnRpYWwoKQptb2RlbC5hZGQoU2ltcGxlUk5OKDE2LCBpbnB1dF9zaGFwZT0oMTAsIDgpKSkgICMgMTYgdW5pdHMgaW4gdGhlIFJOTiBsYXllcgptb2RlbC5hZGQoRGVuc2UoMSkpICAjIE91dHB1dCBsYXllcgoKIyBDb21waWxlIHRoZSBtb2RlbAptb2RlbC5jb21waWxlKG9wdGltaXplcj0nYWRhbScsIGxvc3M9J21zZScpCgojIE1vZGVsIHN1bW1hcnkKbW9kZWwuc3VtbWFyeSgp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# Define the input data sequence
input_data = tf.random.normal([32, 10, 8])  # (batch_size, timesteps, input_dim)

# Create the RNN model
model = Sequential()
model.add(SimpleRNN(16, input_shape=(10, 8)))  # 16 units in the RNN layer
model.add(Dense(1))  # Output layer

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Model summary
model.summary()</pre></div><div class='content'><p>In this example:</p>
<ul>
<li><code>input_data</code> is a tensor of input data with a batch size of 32, 10 timesteps, and 8 features per timestep.</li>
<li><code>SimpleRNN</code> is a recurrent layer with 16 units.</li>
<li><code>Dense</code> is a dense layer that produces a single output value.</li>
</ul>
</div><h1>Advantages and Disadvantages of RNNs</h1>
<h2>Advantages</h2>
<div class='content'><ul>
<li><strong>Ability to handle variable-length sequences</strong>: RNNs can process sequences of different lengths.</li>
<li><strong>Short-term memory</strong>: RNNs can remember information from previous timesteps.</li>
</ul>
</div><h2>Disadvantages</h2>
<div class='content'><ul>
<li><strong>Gradient problems</strong>: RNNs can suffer from vanishing and exploding gradient problems, making training difficult.</li>
<li><strong>Limited long-term memory</strong>: Standard RNNs have difficulty remembering information from very previous timesteps.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>Recurrent Neural Networks (RNN) are a powerful tool for processing sequential data. Although they have some limitations, such as gradient problems and limited long-term memory, they are fundamental in applications like natural language processing and time series analysis. With the advancement of techniques like LSTM and GRU, many of these limitations have been mitigated, allowing RNNs to handle longer and more complex sequences more effectively.</p>
</div>
			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
