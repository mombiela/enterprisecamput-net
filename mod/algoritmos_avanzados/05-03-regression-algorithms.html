<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regression Algorithms</title>

    <link rel="alternate" href="https://campusempresa.com/mod/algoritmos_avanzados/05-03-algoritmos-regresion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/algoritmos_avanzados/05-03-algoritmes-regressio" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/algoritmos_avanzados/05-03-regression-algorithms" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/algoritmos_avanzados/05-03-algoritmos-regresion" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/algoritmos_avanzados/05-03-algoritmes-regressio" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='05-02-classification-algorithms' title="Classification Algorithms">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Regression Algorithms</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='05-04-neural-networks' title="Neural Networks and Deep Learning">Next &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Regression algorithms are a subset of supervised learning techniques used in machine learning to predict a continuous output variable based on one or more input features. These algorithms are fundamental in various fields such as finance, economics, biology, and engineering for tasks like forecasting, trend analysis, and risk management.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ol>
<li>
<p><strong>Dependent and Independent Variables</strong>:</p>
<ul>
<li><strong>Dependent Variable (Y)</strong>: The variable we aim to predict.</li>
<li><strong>Independent Variables (X)</strong>: The variables used to make predictions.</li>
</ul>
</li>
<li>
<p><strong>Linear Regression</strong>:</p>
<ul>
<li>A simple and widely used regression technique that models the relationship between the dependent and independent variables by fitting a linear equation to the observed data.</li>
</ul>
</li>
<li>
<p><strong>Polynomial Regression</strong>:</p>
<ul>
<li>An extension of linear regression where the relationship between the dependent and independent variables is modeled as an nth degree polynomial.</li>
</ul>
</li>
<li>
<p><strong>Regularization Techniques</strong>:</p>
<ul>
<li>Methods like Ridge Regression and Lasso Regression that add a penalty to the model to prevent overfitting.</li>
</ul>
</li>
<li>
<p><strong>Evaluation Metrics</strong>:</p>
<ul>
<li>Metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and R-squared (RÂ²) used to assess the performance of regression models.</li>
</ul>
</li>
</ol>
</div><h1>Linear Regression</h1>
<div class='content'></div><h2>Explanation</h2>
<div class='content'><p>Linear regression aims to model the relationship between two variables by fitting a linear equation to the observed data. The equation of a simple linear regression model is:</p>
<p>\[ Y = \beta_0 + \beta_1X + \epsilon \]</p>
<p>Where:</p>
<ul>
<li>\( Y \) is the dependent variable.</li>
<li>\( X \) is the independent variable.</li>
<li>\( \beta_0 \) is the y-intercept.</li>
<li>\( \beta_1 \) is the slope of the line.</li>
<li>\( \epsilon \) is the error term.</li>
</ul>
</div><h2>Example</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgTGluZWFyUmVncmVzc2lvbgoKIyBTYW1wbGUgZGF0YQpYID0gbnAuYXJyYXkoWzEsIDIsIDMsIDQsIDVdKS5yZXNoYXBlKC0xLCAxKQpZID0gbnAuYXJyYXkoWzIsIDMsIDUsIDcsIDExXSkKCiMgQ3JlYXRlIGFuZCBmaXQgdGhlIG1vZGVsCm1vZGVsID0gTGluZWFyUmVncmVzc2lvbigpCm1vZGVsLmZpdChYLCBZKQoKIyBQcmVkaWN0CllfcHJlZCA9IG1vZGVsLnByZWRpY3QoWCkKCiMgUGxvdHRpbmcgdGhlIHJlc3VsdHMKcGx0LnNjYXR0ZXIoWCwgWSwgY29sb3I9J2JsdWUnKQpwbHQucGxvdChYLCBZX3ByZWQsIGNvbG9yPSdyZWQnKQpwbHQueGxhYmVsKCdYJykKcGx0LnlsYWJlbCgnWScpCnBsdC50aXRsZSgnTGluZWFyIFJlZ3Jlc3Npb24gRXhhbXBsZScpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
Y = np.array([2, 3, 5, 7, 11])

# Create and fit the model
model = LinearRegression()
model.fit(X, Y)

# Predict
Y_pred = model.predict(X)

# Plotting the results
plt.scatter(X, Y, color='blue')
plt.plot(X, Y_pred, color='red')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Linear Regression Example')
plt.show()</pre></div><div class='content'></div><h2>Explanation of Code</h2>
<div class='content'><ol>
<li>
<p><strong>Data Preparation</strong>:</p>
<ul>
<li><code>X</code> and <code>Y</code> are the independent and dependent variables, respectively.</li>
<li><code>X</code> is reshaped to be a 2D array as required by <code>sklearn</code>.</li>
</ul>
</li>
<li>
<p><strong>Model Creation and Fitting</strong>:</p>
<ul>
<li><code>LinearRegression()</code> creates a linear regression model.</li>
<li><code>model.fit(X, Y)</code> fits the model to the data.</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:</p>
<ul>
<li><code>model.predict(X)</code> predicts the dependent variable using the fitted model.</li>
</ul>
</li>
<li>
<p><strong>Plotting</strong>:</p>
<ul>
<li>The scatter plot shows the actual data points.</li>
<li>The red line represents the fitted linear regression model.</li>
</ul>
</li>
</ol>
</div><h1>Polynomial Regression</h1>
<div class='content'></div><h2>Explanation</h2>
<div class='content'><p>Polynomial regression models the relationship between the dependent and independent variables as an nth degree polynomial. The equation for a polynomial regression model is:</p>
<p>\[ Y = \beta_0 + \beta_1X + \beta_2X^2 + ... + \beta_nX^n + \epsilon \]</p>
</div><h2>Example</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFBvbHlub21pYWxGZWF0dXJlcwoKIyBTYW1wbGUgZGF0YQpYID0gbnAuYXJyYXkoWzEsIDIsIDMsIDQsIDVdKS5yZXNoYXBlKC0xLCAxKQpZID0gbnAuYXJyYXkoWzIsIDMsIDUsIDcsIDExXSkKCiMgVHJhbnNmb3JtIHRoZSBkYXRhIHRvIGluY2x1ZGUgcG9seW5vbWlhbCBmZWF0dXJlcwpwb2x5ID0gUG9seW5vbWlhbEZlYXR1cmVzKGRlZ3JlZT0yKQpYX3BvbHkgPSBwb2x5LmZpdF90cmFuc2Zvcm0oWCkKCiMgQ3JlYXRlIGFuZCBmaXQgdGhlIG1vZGVsCm1vZGVsID0gTGluZWFyUmVncmVzc2lvbigpCm1vZGVsLmZpdChYX3BvbHksIFkpCgojIFByZWRpY3QKWV9wcmVkID0gbW9kZWwucHJlZGljdChYX3BvbHkpCgojIFBsb3R0aW5nIHRoZSByZXN1bHRzCnBsdC5zY2F0dGVyKFgsIFksIGNvbG9yPSdibHVlJykKcGx0LnBsb3QoWCwgWV9wcmVkLCBjb2xvcj0ncmVkJykKcGx0LnhsYWJlbCgnWCcpCnBsdC55bGFiZWwoJ1knKQpwbHQudGl0bGUoJ1BvbHlub21pYWwgUmVncmVzc2lvbiBFeGFtcGxlJykKcGx0LnNob3coKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.preprocessing import PolynomialFeatures

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
Y = np.array([2, 3, 5, 7, 11])

# Transform the data to include polynomial features
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Create and fit the model
model = LinearRegression()
model.fit(X_poly, Y)

# Predict
Y_pred = model.predict(X_poly)

# Plotting the results
plt.scatter(X, Y, color='blue')
plt.plot(X, Y_pred, color='red')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Polynomial Regression Example')
plt.show()</pre></div><div class='content'></div><h2>Explanation of Code</h2>
<div class='content'><ol>
<li>
<p><strong>Data Preparation</strong>:</p>
<ul>
<li><code>X</code> and <code>Y</code> are the independent and dependent variables, respectively.</li>
<li><code>X</code> is reshaped to be a 2D array as required by <code>sklearn</code>.</li>
</ul>
</li>
<li>
<p><strong>Polynomial Features Transformation</strong>:</p>
<ul>
<li><code>PolynomialFeatures(degree=2)</code> creates polynomial features up to the 2nd degree.</li>
<li><code>X_poly = poly.fit_transform(X)</code> transforms the original <code>X</code> to include polynomial features.</li>
</ul>
</li>
<li>
<p><strong>Model Creation and Fitting</strong>:</p>
<ul>
<li><code>LinearRegression()</code> creates a linear regression model.</li>
<li><code>model.fit(X_poly, Y)</code> fits the model to the polynomial features.</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:</p>
<ul>
<li><code>model.predict(X_poly)</code> predicts the dependent variable using the fitted model.</li>
</ul>
</li>
<li>
<p><strong>Plotting</strong>:</p>
<ul>
<li>The scatter plot shows the actual data points.</li>
<li>The red line represents the fitted polynomial regression model.</li>
</ul>
</li>
</ol>
</div><h1>Regularization Techniques</h1>
<div class='content'></div><h2>Ridge Regression</h2>
<div class='content'><p>Ridge Regression adds a penalty term to the linear regression cost function to prevent overfitting. The cost function for Ridge Regression is:</p>
<p>\[ J(\beta) = \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1x_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \]</p>
<p>Where \( \lambda \) is the regularization parameter.</p>
</div><h2>Example</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgUmlkZ2UKCiMgU2FtcGxlIGRhdGEKWCA9IG5wLmFycmF5KFsxLCAyLCAzLCA0LCA1XSkucmVzaGFwZSgtMSwgMSkKWSA9IG5wLmFycmF5KFsyLCAzLCA1LCA3LCAxMV0pCgojIENyZWF0ZSBhbmQgZml0IHRoZSBtb2RlbApyaWRnZV9tb2RlbCA9IFJpZGdlKGFscGhhPTEuMCkKcmlkZ2VfbW9kZWwuZml0KFgsIFkpCgojIFByZWRpY3QKWV9wcmVkID0gcmlkZ2VfbW9kZWwucHJlZGljdChYKQoKIyBQbG90dGluZyB0aGUgcmVzdWx0cwpwbHQuc2NhdHRlcihYLCBZLCBjb2xvcj0nYmx1ZScpCnBsdC5wbG90KFgsIFlfcHJlZCwgY29sb3I9J3JlZCcpCnBsdC54bGFiZWwoJ1gnKQpwbHQueWxhYmVsKCdZJykKcGx0LnRpdGxlKCdSaWRnZSBSZWdyZXNzaW9uIEV4YW1wbGUnKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.linear_model import Ridge

# Sample data
X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)
Y = np.array([2, 3, 5, 7, 11])

# Create and fit the model
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X, Y)

# Predict
Y_pred = ridge_model.predict(X)

# Plotting the results
plt.scatter(X, Y, color='blue')
plt.plot(X, Y_pred, color='red')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Ridge Regression Example')
plt.show()</pre></div><div class='content'></div><h2>Explanation of Code</h2>
<div class='content'><ol>
<li>
<p><strong>Data Preparation</strong>:</p>
<ul>
<li><code>X</code> and <code>Y</code> are the independent and dependent variables, respectively.</li>
<li><code>X</code> is reshaped to be a 2D array as required by <code>sklearn</code>.</li>
</ul>
</li>
<li>
<p><strong>Model Creation and Fitting</strong>:</p>
<ul>
<li><code>Ridge(alpha=1.0)</code> creates a Ridge Regression model with a regularization parameter \( \alpha \).</li>
<li><code>ridge_model.fit(X, Y)</code> fits the model to the data.</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:</p>
<ul>
<li><code>ridge_model.predict(X)</code> predicts the dependent variable using the fitted model.</li>
</ul>
</li>
<li>
<p><strong>Plotting</strong>:</p>
<ul>
<li>The scatter plot shows the actual data points.</li>
<li>The red line represents the fitted Ridge Regression model.</li>
</ul>
</li>
</ol>
</div><h1>Evaluation Metrics</h1>
<div class='content'></div><h2>Mean Squared Error (MSE)</h2>
<div class='content'><p>\[ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \]</p>
</div><h2>Root Mean Squared Error (RMSE)</h2>
<div class='content'><p>\[ RMSE = \sqrt{MSE} \]</p>
</div><h2>R-squared (RÂ²)</h2>
<div class='content'><p>\[ R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}<em>i)^2}{\sum</em>{i=1}^{n} (y_i - \bar{y})^2} \]</p>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Exercise</h2>
<div class='content'><ol>
<li>Load a dataset (e.g., Boston Housing dataset).</li>
<li>Split the data into training and testing sets.</li>
<li>Apply Linear Regression, Polynomial Regression, and Ridge Regression.</li>
<li>Evaluate the models using MSE, RMSE, and RÂ².</li>
</ol>
</div><h2>Solution</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2Jvc3Rvbgpmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCB0cmFpbl90ZXN0X3NwbGl0CmZyb20gc2tsZWFybi5tZXRyaWNzIGltcG9ydCBtZWFuX3NxdWFyZWRfZXJyb3IsIHIyX3Njb3JlCgojIExvYWQgZGF0YXNldApib3N0b24gPSBsb2FkX2Jvc3RvbigpClggPSBib3N0b24uZGF0YQpZID0gYm9zdG9uLnRhcmdldAoKIyBTcGxpdCB0aGUgZGF0YQpYX3RyYWluLCBYX3Rlc3QsIFlfdHJhaW4sIFlfdGVzdCA9IHRyYWluX3Rlc3Rfc3BsaXQoWCwgWSwgdGVzdF9zaXplPTAuMiwgcmFuZG9tX3N0YXRlPTQyKQoKIyBMaW5lYXIgUmVncmVzc2lvbgpsaW5lYXJfbW9kZWwgPSBMaW5lYXJSZWdyZXNzaW9uKCkKbGluZWFyX21vZGVsLmZpdChYX3RyYWluLCBZX3RyYWluKQpZX3ByZWRfbGluZWFyID0gbGluZWFyX21vZGVsLnByZWRpY3QoWF90ZXN0KQoKIyBQb2x5bm9taWFsIFJlZ3Jlc3Npb24KcG9seSA9IFBvbHlub21pYWxGZWF0dXJlcyhkZWdyZWU9MikKWF90cmFpbl9wb2x5ID0gcG9seS5maXRfdHJhbnNmb3JtKFhfdHJhaW4pClhfdGVzdF9wb2x5ID0gcG9seS50cmFuc2Zvcm0oWF90ZXN0KQpwb2x5X21vZGVsID0gTGluZWFyUmVncmVzc2lvbigpCnBvbHlfbW9kZWwuZml0KFhfdHJhaW5fcG9seSwgWV90cmFpbikKWV9wcmVkX3BvbHkgPSBwb2x5X21vZGVsLnByZWRpY3QoWF90ZXN0X3BvbHkpCgojIFJpZGdlIFJlZ3Jlc3Npb24KcmlkZ2VfbW9kZWwgPSBSaWRnZShhbHBoYT0xLjApCnJpZGdlX21vZGVsLmZpdChYX3RyYWluLCBZX3RyYWluKQpZX3ByZWRfcmlkZ2UgPSByaWRnZV9tb2RlbC5wcmVkaWN0KFhfdGVzdCkKCiMgRXZhbHVhdGlvbgpkZWYgZXZhbHVhdGVfbW9kZWwoWV90ZXN0LCBZX3ByZWQpOgogICAgbXNlID0gbWVhbl9zcXVhcmVkX2Vycm9yKFlfdGVzdCwgWV9wcmVkKQogICAgcm1zZSA9IG5wLnNxcnQobXNlKQogICAgcjIgPSByMl9zY29yZShZX3Rlc3QsIFlfcHJlZCkKICAgIHJldHVybiBtc2UsIHJtc2UsIHIyCgptc2VfbGluZWFyLCBybXNlX2xpbmVhciwgcjJfbGluZWFyID0gZXZhbHVhdGVfbW9kZWwoWV90ZXN0LCBZX3ByZWRfbGluZWFyKQptc2VfcG9seSwgcm1zZV9wb2x5LCByMl9wb2x5ID0gZXZhbHVhdGVfbW9kZWwoWV90ZXN0LCBZX3ByZWRfcG9seSkKbXNlX3JpZGdlLCBybXNlX3JpZGdlLCByMl9yaWRnZSA9IGV2YWx1YXRlX21vZGVsKFlfdGVzdCwgWV9wcmVkX3JpZGdlKQoKIyBEaXNwbGF5IHJlc3VsdHMKcHJpbnQoZiJMaW5lYXIgUmVncmVzc2lvbiAtIE1TRToge21zZV9saW5lYXJ9LCBSTVNFOiB7cm1zZV9saW5lYXJ9LCBSwrI6IHtyMl9saW5lYXJ9IikKcHJpbnQoZiJQb2x5bm9taWFsIFJlZ3Jlc3Npb24gLSBNU0U6IHttc2VfcG9seX0sIFJNU0U6IHtybXNlX3BvbHl9LCBSwrI6IHtyMl9wb2x5fSIpCnByaW50KGYiUmlkZ2UgUmVncmVzc2lvbiAtIE1TRToge21zZV9yaWRnZX0sIFJNU0U6IHtybXNlX3JpZGdlfSwgUsKyOiB7cjJfcmlkZ2V9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
boston = load_boston()
X = boston.data
Y = boston.target

# Split the data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, Y_train)
Y_pred_linear = linear_model.predict(X_test)

# Polynomial Regression
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)
poly_model = LinearRegression()
poly_model.fit(X_train_poly, Y_train)
Y_pred_poly = poly_model.predict(X_test_poly)

# Ridge Regression
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, Y_train)
Y_pred_ridge = ridge_model.predict(X_test)

# Evaluation
def evaluate_model(Y_test, Y_pred):
    mse = mean_squared_error(Y_test, Y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(Y_test, Y_pred)
    return mse, rmse, r2

mse_linear, rmse_linear, r2_linear = evaluate_model(Y_test, Y_pred_linear)
mse_poly, rmse_poly, r2_poly = evaluate_model(Y_test, Y_pred_poly)
mse_ridge, rmse_ridge, r2_ridge = evaluate_model(Y_test, Y_pred_ridge)

# Display results
print(f&quot;Linear Regression - MSE: {mse_linear}, RMSE: {rmse_linear}, R&sup2;: {r2_linear}&quot;)
print(f&quot;Polynomial Regression - MSE: {mse_poly}, RMSE: {rmse_poly}, R&sup2;: {r2_poly}&quot;)
print(f&quot;Ridge Regression - MSE: {mse_ridge}, RMSE: {rmse_ridge}, R&sup2;: {r2_ridge}&quot;)</pre></div><div class='content'></div><h2>Explanation of Code</h2>
<div class='content'><ol>
<li>
<p><strong>Data Loading and Splitting</strong>:</p>
<ul>
<li>The Boston Housing dataset is loaded.</li>
<li>The data is split into training and testing sets.</li>
</ul>
</li>
<li>
<p><strong>Model Creation and Fitting</strong>:</p>
<ul>
<li>Linear Regression, Polynomial Regression, and Ridge Regression models are created and fitted to the training data.</li>
</ul>
</li>
<li>
<p><strong>Prediction</strong>:</p>
<ul>
<li>Predictions are made on the test data using the fitted models.</li>
</ul>
</li>
<li>
<p><strong>Evaluation</strong>:</p>
<ul>
<li>The models are evaluated using MSE, RMSE, and RÂ² metrics.</li>
<li>The results are printed for comparison.</li>
</ul>
</li>
</ol>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we covered various regression algorithms including Linear Regression, Polynomial Regression, and Ridge Regression. We also discussed how to evaluate these models using common metrics such as MSE, RMSE, and RÂ². By understanding and applying these techniques, you can effectively model and predict continuous variables in various real-world scenarios.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='05-02-classification-algorithms' title="Classification Algorithms">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='05-04-neural-networks' title="Neural Networks and Deep Learning">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiÃ¨ncia d'Ãºs i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
