<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross-Validation and Validation Techniques</title>

    <link rel="alternate" href="https://campusempresa.com/mod/analisis_datos/05-02-validacion-cruzada" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/analisis_datos/05-02-validacio-creuada" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/analisis_datos/05-02-cross-validation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/analisis_datos/05-02-cross-validation" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/analisis_datos/05-02-cross-validation" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='05-01-evaluation-metrics' title="Model Evaluation Metrics">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Cross-Validation and Validation Techniques</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='05-03-tuning-optimization' title="Model Tuning and Optimization">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Cross-validation is a crucial technique in data analysis and machine learning used to assess the performance of a model. It helps in understanding how the results of a statistical analysis will generalize to an independent data set. This section will cover the basics of cross-validation, different cross-validation techniques, and practical examples to solidify your understanding.</p>
</div><h1>Key Concepts of Cross-Validation</h1>
<div class='content'><ol>
<li>
<p><strong>Overfitting and Underfitting</strong>:</p>
<ul>
<li><strong>Overfitting</strong>: When a model learns the training data too well, including noise and outliers, leading to poor performance on unseen data.</li>
<li><strong>Underfitting</strong>: When a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and unseen data.</li>
</ul>
</li>
<li>
<p><strong>Training and Testing Data</strong>:</p>
<ul>
<li><strong>Training Data</strong>: The subset of data used to train the model.</li>
<li><strong>Testing Data</strong>: The subset of data used to evaluate the model's performance.</li>
</ul>
</li>
<li>
<p><strong>Validation Data</strong>:</p>
<ul>
<li>A separate subset of data used to tune the model's hyperparameters and prevent overfitting.</li>
</ul>
</li>
</ol>
</div><h1>Types of Cross-Validation Techniques</h1>
<div class='content'></div><h2>1. Holdout Method</h2>
<div class='content'><ul>
<li><strong>Description</strong>: The dataset is split into two parts: a training set and a testing set.</li>
<li><strong>Advantages</strong>: Simple and quick to implement.</li>
<li><strong>Disadvantages</strong>: Performance can vary significantly depending on how the data is split.</li>
</ul>
</div><h2>2. K-Fold Cross-Validation</h2>
<div class='content'><ul>
<li><strong>Description</strong>: The dataset is divided into <code>k</code> equally sized folds. The model is trained <code>k</code> times, each time using a different fold as the testing set and the remaining <code>k-1</code> folds as the training set.</li>
<li><strong>Advantages</strong>: Provides a more reliable estimate of model performance.</li>
<li><strong>Disadvantages</strong>: Computationally expensive for large datasets.</li>
</ul>
</div><h2>3. Stratified K-Fold Cross-Validation</h2>
<div class='content'><ul>
<li><strong>Description</strong>: Similar to K-Fold Cross-Validation, but ensures that each fold has the same proportion of classes as the original dataset.</li>
<li><strong>Advantages</strong>: Better for imbalanced datasets.</li>
<li><strong>Disadvantages</strong>: Slightly more complex to implement.</li>
</ul>
</div><h2>4. Leave-One-Out Cross-Validation (LOOCV)</h2>
<div class='content'><ul>
<li><strong>Description</strong>: Each data point is used as a single test case, and the model is trained on the remaining data points.</li>
<li><strong>Advantages</strong>: Uses all data points for training and testing.</li>
<li><strong>Disadvantages</strong>: Extremely computationally expensive.</li>
</ul>
</div><h2>5. Time Series Cross-Validation</h2>
<div class='content'><ul>
<li><strong>Description</strong>: Specifically for time series data, where the training set is incrementally increased with each fold.</li>
<li><strong>Advantages</strong>: Maintains the temporal order of data.</li>
<li><strong>Disadvantages</strong>: May not be suitable for non-time series data.</li>
</ul>
</div><h1>Practical Example: K-Fold Cross-Validation</h1>
<div class='content'><p>Let's implement K-Fold Cross-Validation using Python and the <code>scikit-learn</code> library.</p>
</div><h2>Step-by-Step Implementation</h2>
<div class='content'><ol>
<li>
<p><strong>Import Necessary Libraries</strong>:</p>
<pre><code class="language-python">import numpy as np
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
</code></pre>
</li>
<li>
<p><strong>Generate Sample Data</strong>:</p>
<pre><code class="language-python"># Generating synthetic data
X = np.random.rand(100, 1) * 10  # 100 data points, single feature
y = 2.5 * X.squeeze() + np.random.randn(100) * 2  # Linear relationship with noise
</code></pre>
</li>
<li>
<p><strong>Initialize K-Fold Cross-Validation</strong>:</p>
<pre><code class="language-python">kf = KFold(n_splits=5, shuffle=True, random_state=42)
</code></pre>
</li>
<li>
<p><strong>Perform Cross-Validation</strong>:</p>
<pre><code class="language-python">model = LinearRegression()
mse_scores = []

for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)

print(f'Mean Squared Error for each fold: {mse_scores}')
print(f'Average Mean Squared Error: {np.mean(mse_scores)}')
</code></pre>
</li>
</ol>
</div><h2>Explanation of the Code</h2>
<div class='content'><ul>
<li><strong>Data Generation</strong>: We create synthetic data with a linear relationship.</li>
<li><strong>K-Fold Initialization</strong>: We initialize K-Fold with 5 splits, shuffling the data and setting a random state for reproducibility.</li>
<li><strong>Model Training and Evaluation</strong>: For each fold, we split the data into training and testing sets, train the model, make predictions, and compute the Mean Squared Error (MSE). Finally, we print the MSE for each fold and the average MSE.</li>
</ul>
</div><h1>Exercises</h1>
<div class='content'></div><h2>Exercise 1: Implement Stratified K-Fold Cross-Validation</h2>
<div class='content'><p><strong>Task</strong>: Implement Stratified K-Fold Cross-Validation on a classification dataset using <code>scikit-learn</code>.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgU3RyYXRpZmllZEtGb2xkCmZyb20gc2tsZWFybi5kYXRhc2V0cyBpbXBvcnQgbG9hZF9pcmlzCmZyb20gc2tsZWFybi5lbnNlbWJsZSBpbXBvcnQgUmFuZG9tRm9yZXN0Q2xhc3NpZmllcgpmcm9tIHNrbGVhcm4ubWV0cmljcyBpbXBvcnQgYWNjdXJhY3lfc2NvcmUKCiMgTG9hZCBkYXRhc2V0CmRhdGEgPSBsb2FkX2lyaXMoKQpYLCB5ID0gZGF0YS5kYXRhLCBkYXRhLnRhcmdldAoKIyBJbml0aWFsaXplIFN0cmF0aWZpZWQgSy1Gb2xkCnNrZiA9IFN0cmF0aWZpZWRLRm9sZChuX3NwbGl0cz01LCBzaHVmZmxlPVRydWUsIHJhbmRvbV9zdGF0ZT00MikKCm1vZGVsID0gUmFuZG9tRm9yZXN0Q2xhc3NpZmllcigpCmFjY3VyYWN5X3Njb3JlcyA9IFtdCgpmb3IgdHJhaW5faW5kZXgsIHRlc3RfaW5kZXggaW4gc2tmLnNwbGl0KFgsIHkpOgogICAgWF90cmFpbiwgWF90ZXN0ID0gWFt0cmFpbl9pbmRleF0sIFhbdGVzdF9pbmRleF0KICAgIHlfdHJhaW4sIHlfdGVzdCA9IHlbdHJhaW5faW5kZXhdLCB5W3Rlc3RfaW5kZXhdCgogICAgbW9kZWwuZml0KFhfdHJhaW4sIHlfdHJhaW4pCiAgICB5X3ByZWQgPSBtb2RlbC5wcmVkaWN0KFhfdGVzdCkKICAgIGFjY3VyYWN5ID0gYWNjdXJhY3lfc2NvcmUoeV90ZXN0LCB5X3ByZWQpCiAgICBhY2N1cmFjeV9zY29yZXMuYXBwZW5kKGFjY3VyYWN5KQoKcHJpbnQoZidBY2N1cmFjeSBmb3IgZWFjaCBmb2xkOiB7YWNjdXJhY3lfc2NvcmVzfScpCnByaW50KGYnQXZlcmFnZSBBY2N1cmFjeToge25wLm1lYW4oYWNjdXJhY3lfc2NvcmVzKX0nKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import StratifiedKFold
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Initialize Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

model = RandomForestClassifier()
accuracy_scores = []

for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracy_scores.append(accuracy)

print(f'Accuracy for each fold: {accuracy_scores}')
print(f'Average Accuracy: {np.mean(accuracy_scores)}')</pre></div><div class='content'></div><h2>Exercise 2: Implement Leave-One-Out Cross-Validation</h2>
<div class='content'><p><strong>Task</strong>: Implement Leave-One-Out Cross-Validation on a regression dataset using <code>scikit-learn</code>.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgTGVhdmVPbmVPdXQKZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgTGluZWFyUmVncmVzc2lvbgpmcm9tIHNrbGVhcm4ubWV0cmljcyBpbXBvcnQgbWVhbl9zcXVhcmVkX2Vycm9yCgojIEdlbmVyYXRlIHN5bnRoZXRpYyBkYXRhClggPSBucC5yYW5kb20ucmFuZCg1MCwgMSkgKiAxMCAgIyA1MCBkYXRhIHBvaW50cywgc2luZ2xlIGZlYXR1cmUKeSA9IDMuNSAqIFguc3F1ZWV6ZSgpICsgbnAucmFuZG9tLnJhbmRuKDUwKSAqIDEuNSAgIyBMaW5lYXIgcmVsYXRpb25zaGlwIHdpdGggbm9pc2UKCiMgSW5pdGlhbGl6ZSBMZWF2ZS1PbmUtT3V0IENyb3NzLVZhbGlkYXRpb24KbG9vID0gTGVhdmVPbmVPdXQoKQoKbW9kZWwgPSBMaW5lYXJSZWdyZXNzaW9uKCkKbXNlX3Njb3JlcyA9IFtdCgpmb3IgdHJhaW5faW5kZXgsIHRlc3RfaW5kZXggaW4gbG9vLnNwbGl0KFgpOgogICAgWF90cmFpbiwgWF90ZXN0ID0gWFt0cmFpbl9pbmRleF0sIFhbdGVzdF9pbmRleF0KICAgIHlfdHJhaW4sIHlfdGVzdCA9IHlbdHJhaW5faW5kZXhdLCB5W3Rlc3RfaW5kZXhdCgogICAgbW9kZWwuZml0KFhfdHJhaW4sIHlfdHJhaW4pCiAgICB5X3ByZWQgPSBtb2RlbC5wcmVkaWN0KFhfdGVzdCkKICAgIG1zZSA9IG1lYW5fc3F1YXJlZF9lcnJvcih5X3Rlc3QsIHlfcHJlZCkKICAgIG1zZV9zY29yZXMuYXBwZW5kKG1zZSkKCnByaW50KGYnTWVhbiBTcXVhcmVkIEVycm9yIGZvciBlYWNoIGZvbGQ6IHttc2Vfc2NvcmVzfScpCnByaW50KGYnQXZlcmFnZSBNZWFuIFNxdWFyZWQgRXJyb3I6IHtucC5tZWFuKG1zZV9zY29yZXMpfScp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import LeaveOneOut
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# Generate synthetic data
X = np.random.rand(50, 1) * 10  # 50 data points, single feature
y = 3.5 * X.squeeze() + np.random.randn(50) * 1.5  # Linear relationship with noise

# Initialize Leave-One-Out Cross-Validation
loo = LeaveOneOut()

model = LinearRegression()
mse_scores = []

for train_index, test_index in loo.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mse_scores.append(mse)

print(f'Mean Squared Error for each fold: {mse_scores}')
print(f'Average Mean Squared Error: {np.mean(mse_scores)}')</pre></div><div class='content'></div><h1>Common Mistakes and Tips</h1>
<div class='content'><ol>
<li><strong>Not Shuffling Data</strong>: Always shuffle your data before splitting to ensure that each fold is representative of the whole dataset.</li>
<li><strong>Ignoring Class Imbalance</strong>: Use Stratified K-Fold for imbalanced datasets to maintain the proportion of classes in each fold.</li>
<li><strong>Computational Cost</strong>: Be mindful of the computational cost, especially with LOOCV, as it can be very expensive for large datasets.</li>
</ol>
</div><h1>Conclusion</h1>
<div class='content'><p>Cross-validation is an essential technique for evaluating the performance of your models and ensuring they generalize well to unseen data. By understanding and implementing different cross-validation techniques, you can improve the robustness and reliability of your data analysis and machine learning models. In the next section, we will delve into model tuning and optimization to further enhance model performance.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='05-01-evaluation-metrics' title="Model Evaluation Metrics">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='05-03-tuning-optimization' title="Model Tuning and Optimization">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
