<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Using BigQuery with Dataflow</title>

    <link rel="alternate" href="https://campusempresa.com/mod/bigquery/08-02-using-dataflow" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/bigquery/08-02-using-dataflow" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/bigquery/08-02-using-dataflow" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/bigquery/08-02-using-dataflow" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/bigquery/08-02-using-dataflow" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='08-01-integrating-google-cloud' title="Integrating with Google Cloud Services">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Using BigQuery with Dataflow</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='08-03-automating-workflows' title="Automating Workflows with Cloud Functions">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>In this section, we will explore how to integrate BigQuery with Dataflow, a fully managed service for stream and batch data processing. Dataflow allows you to create data pipelines that can read from and write to BigQuery, enabling powerful data transformations and real-time analytics.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ol>
<li><strong>Dataflow</strong>: A service for executing Apache Beam pipelines within the Google Cloud Platform.</li>
<li><strong>Apache Beam</strong>: An open-source, unified model for defining both batch and streaming data-parallel processing pipelines.</li>
<li><strong>PCollection</strong>: The primary data abstraction in Apache Beam, representing a distributed dataset.</li>
<li><strong>Transforms</strong>: Operations that process data in a PCollection.</li>
</ol>
</div><h1>Setting Up Dataflow</h1>
<div class='content'><p>Before we dive into using Dataflow with BigQuery, ensure you have the following prerequisites:</p>
<ul>
<li>A Google Cloud Platform (GCP) project with billing enabled.</li>
<li>BigQuery and Dataflow APIs enabled.</li>
<li>Google Cloud SDK installed on your local machine.</li>
<li>Python or Java development environment set up for Apache Beam.</li>
</ul>
</div><h1>Reading from BigQuery</h1>
<div class='content'><p>To read data from BigQuery using Dataflow, you can use the <code>ReadFromBigQuery</code> transform provided by Apache Beam. Below is an example in Python:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGFwYWNoZV9iZWFtIGFzIGJlYW0KZnJvbSBhcGFjaGVfYmVhbS5vcHRpb25zLnBpcGVsaW5lX29wdGlvbnMgaW1wb3J0IFBpcGVsaW5lT3B0aW9ucwoKIyBEZWZpbmUgeW91ciBwaXBlbGluZSBvcHRpb25zCm9wdGlvbnMgPSBQaXBlbGluZU9wdGlvbnMoCiAgICBwcm9qZWN0PSd5b3VyLWdjcC1wcm9qZWN0LWlkJywKICAgIHJlZ2lvbj0neW91ci1yZWdpb24nLAogICAgdGVtcF9sb2NhdGlvbj0nZ3M6Ly95b3VyLWJ1Y2tldC90ZW1wJywKICAgIHN0YWdpbmdfbG9jYXRpb249J2dzOi8veW91ci1idWNrZXQvc3RhZ2luZycsCiAgICBydW5uZXI9J0RhdGFmbG93UnVubmVyJwopCgojIERlZmluZSB0aGUgcGlwZWxpbmUKd2l0aCBiZWFtLlBpcGVsaW5lKG9wdGlvbnM9b3B0aW9ucykgYXMgcDoKICAgIHF1ZXJ5ID0gJ1NFTEVDVCBuYW1lLCBhZ2UgRlJPTSBgeW91ci1kYXRhc2V0LnlvdXItdGFibGVgJwogICAgCiAgICAjIFJlYWQgZnJvbSBCaWdRdWVyeQogICAgcm93cyA9IHAgfCAnUmVhZEZyb21CaWdRdWVyeScgPj4gYmVhbS5pby5SZWFkRnJvbUJpZ1F1ZXJ5KHF1ZXJ5PXF1ZXJ5KQogICAgCiAgICAjIFByb2Nlc3MgdGhlIGRhdGEgKGV4YW1wbGU6IHByaW50IGVhY2ggcm93KQogICAgcm93cyB8ICdQcmludFJvd3MnID4+IGJlYW0uTWFwKHByaW50KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# Define your pipeline options
options = PipelineOptions(
    project='your-gcp-project-id',
    region='your-region',
    temp_location='gs://your-bucket/temp',
    staging_location='gs://your-bucket/staging',
    runner='DataflowRunner'
)

# Define the pipeline
with beam.Pipeline(options=options) as p:
    query = 'SELECT name, age FROM `your-dataset.your-table`'
    
    # Read from BigQuery
    rows = p | 'ReadFromBigQuery' &gt;&gt; beam.io.ReadFromBigQuery(query=query)
    
    # Process the data (example: print each row)
    rows | 'PrintRows' &gt;&gt; beam.Map(print)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>PipelineOptions</strong>: Configures the pipeline execution environment.</li>
<li><strong>ReadFromBigQuery</strong>: Reads data from a BigQuery table or query.</li>
<li><strong>beam.Map(print)</strong>: A simple transform to print each row.</li>
</ul>
</div><h1>Writing to BigQuery</h1>
<div class='content'><p>To write data to BigQuery, you can use the <code>WriteToBigQuery</code> transform. Below is an example in Python:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGFwYWNoZV9iZWFtIGFzIGJlYW0KZnJvbSBhcGFjaGVfYmVhbS5vcHRpb25zLnBpcGVsaW5lX29wdGlvbnMgaW1wb3J0IFBpcGVsaW5lT3B0aW9ucwoKIyBEZWZpbmUgeW91ciBwaXBlbGluZSBvcHRpb25zCm9wdGlvbnMgPSBQaXBlbGluZU9wdGlvbnMoCiAgICBwcm9qZWN0PSd5b3VyLWdjcC1wcm9qZWN0LWlkJywKICAgIHJlZ2lvbj0neW91ci1yZWdpb24nLAogICAgdGVtcF9sb2NhdGlvbj0nZ3M6Ly95b3VyLWJ1Y2tldC90ZW1wJywKICAgIHN0YWdpbmdfbG9jYXRpb249J2dzOi8veW91ci1idWNrZXQvc3RhZ2luZycsCiAgICBydW5uZXI9J0RhdGFmbG93UnVubmVyJwopCgojIERlZmluZSB0aGUgcGlwZWxpbmUKd2l0aCBiZWFtLlBpcGVsaW5lKG9wdGlvbnM9b3B0aW9ucykgYXMgcDoKICAgIGRhdGEgPSBbCiAgICAgICAgeyduYW1lJzogJ0FsaWNlJywgJ2FnZSc6IDMwfSwKICAgICAgICB7J25hbWUnOiAnQm9iJywgJ2FnZSc6IDI1fQogICAgXQogICAgCiAgICAjIENyZWF0ZSBhIFBDb2xsZWN0aW9uIGZyb20gdGhlIGRhdGEKICAgIHJvd3MgPSBwIHwgJ0NyZWF0ZURhdGEnID4+IGJlYW0uQ3JlYXRlKGRhdGEpCiAgICAKICAgICMgV3JpdGUgdG8gQmlnUXVlcnkKICAgIHJvd3MgfCAnV3JpdGVUb0JpZ1F1ZXJ5JyA+PiBiZWFtLmlvLldyaXRlVG9CaWdRdWVyeSgKICAgICAgICAneW91ci1kYXRhc2V0LnlvdXItdGFibGUnLAogICAgICAgIHNjaGVtYT0nbmFtZTpTVFJJTkcsIGFnZTpJTlRFR0VSJywKICAgICAgICB3cml0ZV9kaXNwb3NpdGlvbj1iZWFtLmlvLkJpZ1F1ZXJ5RGlzcG9zaXRpb24uV1JJVEVfVFJVTkNBVEUKICAgICk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# Define your pipeline options
options = PipelineOptions(
    project='your-gcp-project-id',
    region='your-region',
    temp_location='gs://your-bucket/temp',
    staging_location='gs://your-bucket/staging',
    runner='DataflowRunner'
)

# Define the pipeline
with beam.Pipeline(options=options) as p:
    data = [
        {'name': 'Alice', 'age': 30},
        {'name': 'Bob', 'age': 25}
    ]
    
    # Create a PCollection from the data
    rows = p | 'CreateData' &gt;&gt; beam.Create(data)
    
    # Write to BigQuery
    rows | 'WriteToBigQuery' &gt;&gt; beam.io.WriteToBigQuery(
        'your-dataset.your-table',
        schema='name:STRING, age:INTEGER',
        write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE
    )</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>beam.Create(data)</strong>: Creates a PCollection from an in-memory list.</li>
<li><strong>WriteToBigQuery</strong>: Writes data to a BigQuery table.</li>
<li><strong>schema</strong>: Defines the schema of the BigQuery table.</li>
<li><strong>write_disposition</strong>: Specifies the write behavior (e.g., <code>WRITE_TRUNCATE</code> to overwrite the table).</li>
</ul>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Exercise: Create a Dataflow Pipeline to Transform and Load Data</h2>
<div class='content'><ol>
<li><strong>Objective</strong>: Create a Dataflow pipeline that reads data from a BigQuery table, transforms it, and writes the transformed data to another BigQuery table.</li>
<li><strong>Steps</strong>:
<ul>
<li>Read data from a BigQuery table.</li>
<li>Apply a transformation (e.g., filter rows where age &gt; 25).</li>
<li>Write the transformed data to a new BigQuery table.</li>
</ul>
</li>
</ol>
</div><h2>Solution</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGFwYWNoZV9iZWFtIGFzIGJlYW0KZnJvbSBhcGFjaGVfYmVhbS5vcHRpb25zLnBpcGVsaW5lX29wdGlvbnMgaW1wb3J0IFBpcGVsaW5lT3B0aW9ucwoKIyBEZWZpbmUgeW91ciBwaXBlbGluZSBvcHRpb25zCm9wdGlvbnMgPSBQaXBlbGluZU9wdGlvbnMoCiAgICBwcm9qZWN0PSd5b3VyLWdjcC1wcm9qZWN0LWlkJywKICAgIHJlZ2lvbj0neW91ci1yZWdpb24nLAogICAgdGVtcF9sb2NhdGlvbj0nZ3M6Ly95b3VyLWJ1Y2tldC90ZW1wJywKICAgIHN0YWdpbmdfbG9jYXRpb249J2dzOi8veW91ci1idWNrZXQvc3RhZ2luZycsCiAgICBydW5uZXI9J0RhdGFmbG93UnVubmVyJwopCgojIERlZmluZSB0aGUgcGlwZWxpbmUKd2l0aCBiZWFtLlBpcGVsaW5lKG9wdGlvbnM9b3B0aW9ucykgYXMgcDoKICAgIHF1ZXJ5ID0gJ1NFTEVDVCBuYW1lLCBhZ2UgRlJPTSBgeW91ci1kYXRhc2V0LnNvdXJjZS10YWJsZWAnCiAgICAKICAgICMgUmVhZCBmcm9tIEJpZ1F1ZXJ5CiAgICByb3dzID0gcCB8ICdSZWFkRnJvbUJpZ1F1ZXJ5JyA+PiBiZWFtLmlvLlJlYWRGcm9tQmlnUXVlcnkocXVlcnk9cXVlcnkpCiAgICAKICAgICMgRmlsdGVyIHJvd3Mgd2hlcmUgYWdlID4gMjUKICAgIGZpbHRlcmVkX3Jvd3MgPSByb3dzIHwgJ0ZpbHRlckFnZScgPj4gYmVhbS5GaWx0ZXIobGFtYmRhIHJvdzogcm93WydhZ2UnXSA+IDI1KQogICAgCiAgICAjIFdyaXRlIHRvIEJpZ1F1ZXJ5CiAgICBmaWx0ZXJlZF9yb3dzIHwgJ1dyaXRlVG9CaWdRdWVyeScgPj4gYmVhbS5pby5Xcml0ZVRvQmlnUXVlcnkoCiAgICAgICAgJ3lvdXItZGF0YXNldC5kZXN0aW5hdGlvbi10YWJsZScsCiAgICAgICAgc2NoZW1hPSduYW1lOlNUUklORywgYWdlOklOVEVHRVInLAogICAgICAgIHdyaXRlX2Rpc3Bvc2l0aW9uPWJlYW0uaW8uQmlnUXVlcnlEaXNwb3NpdGlvbi5XUklURV9UUlVOQ0FURQogICAgKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions

# Define your pipeline options
options = PipelineOptions(
    project='your-gcp-project-id',
    region='your-region',
    temp_location='gs://your-bucket/temp',
    staging_location='gs://your-bucket/staging',
    runner='DataflowRunner'
)

# Define the pipeline
with beam.Pipeline(options=options) as p:
    query = 'SELECT name, age FROM `your-dataset.source-table`'
    
    # Read from BigQuery
    rows = p | 'ReadFromBigQuery' &gt;&gt; beam.io.ReadFromBigQuery(query=query)
    
    # Filter rows where age &gt; 25
    filtered_rows = rows | 'FilterAge' &gt;&gt; beam.Filter(lambda row: row['age'] &gt; 25)
    
    # Write to BigQuery
    filtered_rows | 'WriteToBigQuery' &gt;&gt; beam.io.WriteToBigQuery(
        'your-dataset.destination-table',
        schema='name:STRING, age:INTEGER',
        write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE
    )</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>beam.Filter(lambda row: row['age'] &gt; 25)</strong>: Filters rows where the age is greater than 25.</li>
<li><strong>WriteToBigQuery</strong>: Writes the filtered data to a new BigQuery table.</li>
</ul>
</div><h1>Common Mistakes and Tips</h1>
<div class='content'><ul>
<li><strong>Incorrect Schema</strong>: Ensure the schema specified in <code>WriteToBigQuery</code> matches the data structure.</li>
<li><strong>Permissions</strong>: Make sure your service account has the necessary permissions to read from and write to BigQuery.</li>
<li><strong>Resource Management</strong>: Use appropriate resource settings in <code>PipelineOptions</code> to manage costs and performance.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we learned how to integrate BigQuery with Dataflow to create powerful data pipelines. We covered reading from and writing to BigQuery, and provided a practical exercise to reinforce the concepts. In the next section, we will explore how to automate workflows with Cloud Functions.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='08-01-integrating-google-cloud' title="Integrating with Google Cloud Services">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='08-03-automating-workflows' title="Automating Workflows with Cloud Functions">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
