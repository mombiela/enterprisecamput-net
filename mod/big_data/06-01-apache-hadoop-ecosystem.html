<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Apache Hadoop Ecosystem</title>

    <link rel="alternate" href="https://campusempresa.com/mod/big_data/06-01-apache-hadoop-ecosystem" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/big_data/06-01-apache-hadoop-ecosystem" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/big_data/06-01-apache-hadoop-ecosystem" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=4" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/big_data/06-01-apache-hadoop-ecosystem" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/big_data/06-01-apache-hadoop-ecosystem" class="px-2">CA</a>
</span>
			<span class="d-none d-md-inline"><br><cite>All the knowledge within your reach</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
					 				<a href="/"><i class="bi bi-house-fill"></i> HOME</a>
											</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-03-practical-projects' title="Practical Projects">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Apache Hadoop Ecosystem</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='06-02-apache-spark-ecosystem' title="Apache Spark Ecosystem">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>The Apache Hadoop Ecosystem is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. The ecosystem includes a variety of tools and technologies that work together to provide a comprehensive solution for big data storage, processing, and analysis.</p>
</div><h1>Key Components of the Hadoop Ecosystem</h1>
<div class='content'></div><h2><ol>
<li>Hadoop Distributed File System (HDFS)</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: HDFS is the primary storage system used by Hadoop applications. It provides high-throughput access to application data and is designed to scale out across large clusters of commodity servers.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Fault Tolerance</strong>: Data is replicated across multiple nodes to ensure reliability.</li>
<li><strong>Scalability</strong>: Can handle large volumes of data by adding more nodes.</li>
<li><strong>High Availability</strong>: Ensures data is available even if some nodes fail.</li>
</ul>
</li>
</ul>
</div><h2><ol start="2">
<li>MapReduce</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: MapReduce is a programming model and processing engine for large-scale data processing. It divides the processing into two main steps: Map and Reduce.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Parallel Processing</strong>: Processes data in parallel across multiple nodes.</li>
<li><strong>Scalability</strong>: Can handle petabytes of data.</li>
<li><strong>Fault Tolerance</strong>: Automatically handles failures during processing.</li>
</ul>
</li>
</ul>
</div><h2><ol start="3">
<li>YARN (Yet Another Resource Negotiator)</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: YARN is the resource management layer of Hadoop. It manages and schedules resources across the cluster.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Resource Allocation</strong>: Efficiently allocates resources to various applications.</li>
<li><strong>Multi-tenancy</strong>: Supports multiple applications running simultaneously.</li>
<li><strong>Scalability</strong>: Can manage thousands of nodes and applications.</li>
</ul>
</li>
</ul>
</div><h2><ol start="4">
<li>Hadoop Common</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: Hadoop Common provides a set of shared utilities and libraries that support other Hadoop modules.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Common Utilities</strong>: Includes file system and serialization libraries.</li>
<li><strong>Cross-Module Support</strong>: Provides support for other Hadoop components.</li>
</ul>
</li>
</ul>
</div><h1>Additional Tools in the Hadoop Ecosystem</h1>
<div class='content'></div><h2><ol>
<li>Apache Hive</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: Hive is a data warehousing and SQL-like query language for Hadoop.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>SQL Interface</strong>: Allows users to query data using SQL-like syntax.</li>
<li><strong>Data Warehousing</strong>: Supports data summarization and analysis.</li>
<li><strong>Integration</strong>: Works seamlessly with HDFS and other Hadoop components.</li>
</ul>
</li>
</ul>
</div><h2><ol start="2">
<li>Apache HBase</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: HBase is a distributed, scalable, NoSQL database built on top of HDFS.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Real-time Read/Write</strong>: Supports real-time data access.</li>
<li><strong>Scalability</strong>: Can handle large tables with billions of rows and millions of columns.</li>
<li><strong>Fault Tolerance</strong>: Ensures data reliability through replication.</li>
</ul>
</li>
</ul>
</div><h2><ol start="3">
<li>Apache Pig</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: Pig is a high-level platform for creating MapReduce programs used with Hadoop.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Scripting Language</strong>: Uses Pig Latin, a high-level scripting language.</li>
<li><strong>Data Transformation</strong>: Simplifies the process of writing complex data transformations.</li>
<li><strong>Integration</strong>: Works well with HDFS and MapReduce.</li>
</ul>
</li>
</ul>
</div><h2><ol start="4">
<li>Apache Sqoop</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: Sqoop is a tool designed for efficiently transferring bulk data between Hadoop and structured data stores such as relational databases.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Data Import/Export</strong>: Facilitates data transfer between Hadoop and RDBMS.</li>
<li><strong>Integration</strong>: Works with HDFS, Hive, and HBase.</li>
<li><strong>Performance</strong>: Optimized for high-speed data transfer.</li>
</ul>
</li>
</ul>
</div><h2><ol start="5">
<li>Apache Flume</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Purpose</strong>: Flume is a distributed service for efficiently collecting, aggregating, and moving large amounts of log data.</li>
<li><strong>Features</strong>:
<ul>
<li><strong>Data Ingestion</strong>: Designed for streaming data into Hadoop.</li>
<li><strong>Scalability</strong>: Can handle large volumes of data.</li>
<li><strong>Reliability</strong>: Ensures data is reliably delivered.</li>
</ul>
</li>
</ul>
</div><h1>Practical Example: Word Count with Hadoop MapReduce</h1>
<div class='content'><p>Let's look at a simple example of a MapReduce program to count the occurrences of each word in a text file.</p>
</div><h2>Mapper Class</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGphdmEuaW8uSU9FeGNlcHRpb247CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5pby5JbnRXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLkxvbmdXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLlRleHQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UuTWFwcGVyOwoKcHVibGljIGNsYXNzIFdvcmRDb3VudE1hcHBlciBleHRlbmRzIE1hcHBlcjxMb25nV3JpdGFibGUsIFRleHQsIFRleHQsIEludFdyaXRhYmxlPiB7CiAgICBwcml2YXRlIGZpbmFsIHN0YXRpYyBJbnRXcml0YWJsZSBvbmUgPSBuZXcgSW50V3JpdGFibGUoMSk7CiAgICBwcml2YXRlIFRleHQgd29yZCA9IG5ldyBUZXh0KCk7CgogICAgQE92ZXJyaWRlCiAgICBwcm90ZWN0ZWQgdm9pZCBtYXAoTG9uZ1dyaXRhYmxlIGtleSwgVGV4dCB2YWx1ZSwgQ29udGV4dCBjb250ZXh0KSB0aHJvd3MgSU9FeGNlcHRpb24sIEludGVycnVwdGVkRXhjZXB0aW9uIHsKICAgICAgICBTdHJpbmdbXSB3b3JkcyA9IHZhbHVlLnRvU3RyaW5nKCkuc3BsaXQoIlxccysiKTsKICAgICAgICBmb3IgKFN0cmluZyBzdHIgOiB3b3JkcykgewogICAgICAgICAgICB3b3JkLnNldChzdHIpOwogICAgICAgICAgICBjb250ZXh0LndyaXRlKHdvcmQsIG9uZSk7CiAgICAgICAgfQogICAgfQp9"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] words = value.toString().split(&quot;\\s+&quot;);
        for (String str : words) {
            word.set(str);
            context.write(word, one);
        }
    }
}</pre></div><h2>Reducer Class</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGphdmEuaW8uSU9FeGNlcHRpb247CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5pby5JbnRXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLlRleHQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UuUmVkdWNlcjsKCnB1YmxpYyBjbGFzcyBXb3JkQ291bnRSZWR1Y2VyIGV4dGVuZHMgUmVkdWNlcjxUZXh0LCBJbnRXcml0YWJsZSwgVGV4dCwgSW50V3JpdGFibGU+IHsKICAgIEBPdmVycmlkZQogICAgcHJvdGVjdGVkIHZvaWQgcmVkdWNlKFRleHQga2V5LCBJdGVyYWJsZTxJbnRXcml0YWJsZT4gdmFsdWVzLCBDb250ZXh0IGNvbnRleHQpIHRocm93cyBJT0V4Y2VwdGlvbiwgSW50ZXJydXB0ZWRFeGNlcHRpb24gewogICAgICAgIGludCBzdW0gPSAwOwogICAgICAgIGZvciAoSW50V3JpdGFibGUgdmFsIDogdmFsdWVzKSB7CiAgICAgICAgICAgIHN1bSArPSB2YWwuZ2V0KCk7CiAgICAgICAgfQogICAgICAgIGNvbnRleHQud3JpdGUoa2V5LCBuZXcgSW50V3JpdGFibGUoc3VtKSk7CiAgICB9Cn0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    @Override
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        for (IntWritable val : values) {
            sum += val.get();
        }
        context.write(key, new IntWritable(sum));
    }
}</pre></div><h2>Driver Class</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmNvbmYuQ29uZmlndXJhdGlvbjsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmZzLlBhdGg7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5pby5JbnRXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLlRleHQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UuSm9iOwppbXBvcnQgb3JnLmFwYWNoZS5oYWRvb3AubWFwcmVkdWNlLmxpYi5pbnB1dC5GaWxlSW5wdXRGb3JtYXQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UubGliLm91dHB1dC5GaWxlT3V0cHV0Rm9ybWF0OwoKcHVibGljIGNsYXNzIFdvcmRDb3VudERyaXZlciB7CiAgICBwdWJsaWMgc3RhdGljIHZvaWQgbWFpbihTdHJpbmdbXSBhcmdzKSB0aHJvd3MgRXhjZXB0aW9uIHsKICAgICAgICBDb25maWd1cmF0aW9uIGNvbmYgPSBuZXcgQ29uZmlndXJhdGlvbigpOwogICAgICAgIEpvYiBqb2IgPSBKb2IuZ2V0SW5zdGFuY2UoY29uZiwgIndvcmQgY291bnQiKTsKICAgICAgICBqb2Iuc2V0SmFyQnlDbGFzcyhXb3JkQ291bnREcml2ZXIuY2xhc3MpOwogICAgICAgIGpvYi5zZXRNYXBwZXJDbGFzcyhXb3JkQ291bnRNYXBwZXIuY2xhc3MpOwogICAgICAgIGpvYi5zZXRDb21iaW5lckNsYXNzKFdvcmRDb3VudFJlZHVjZXIuY2xhc3MpOwogICAgICAgIGpvYi5zZXRSZWR1Y2VyQ2xhc3MoV29yZENvdW50UmVkdWNlci5jbGFzcyk7CiAgICAgICAgam9iLnNldE91dHB1dEtleUNsYXNzKFRleHQuY2xhc3MpOwogICAgICAgIGpvYi5zZXRPdXRwdXRWYWx1ZUNsYXNzKEludFdyaXRhYmxlLmNsYXNzKTsKICAgICAgICBGaWxlSW5wdXRGb3JtYXQuYWRkSW5wdXRQYXRoKGpvYiwgbmV3IFBhdGgoYXJnc1swXSkpOwogICAgICAgIEZpbGVPdXRwdXRGb3JtYXQuc2V0T3V0cHV0UGF0aChqb2IsIG5ldyBQYXRoKGFyZ3NbMV0pKTsKICAgICAgICBTeXN0ZW0uZXhpdChqb2Iud2FpdEZvckNvbXBsZXRpb24odHJ1ZSkgPyAwIDogMSk7CiAgICB9Cn0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCountDriver {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, &quot;word count&quot;);
        job.setJarByClass(WordCountDriver.class);
        job.setMapperClass(WordCountMapper.class);
        job.setCombinerClass(WordCountReducer.class);
        job.setReducerClass(WordCountReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Mapper Class</strong>: The <code>WordCountMapper</code> class reads the input text line by line, splits each line into words, and emits each word with a count of one.</li>
<li><strong>Reducer Class</strong>: The <code>WordCountReducer</code> class aggregates the counts for each word and emits the total count for each word.</li>
<li><strong>Driver Class</strong>: The <code>WordCountDriver</code> class sets up the job configuration, specifying the mapper, reducer, input, and output paths.</li>
</ul>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Task</h2>
<div class='content'><p>Write a MapReduce program to calculate the average length of words in a text file.</p>
</div><h2>Solution</h2>
<div class='content'><h4>Mapper Class</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGphdmEuaW8uSU9FeGNlcHRpb247CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5pby5JbnRXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLkxvbmdXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLlRleHQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UuTWFwcGVyOwoKcHVibGljIGNsYXNzIEF2Z1dvcmRMZW5ndGhNYXBwZXIgZXh0ZW5kcyBNYXBwZXI8TG9uZ1dyaXRhYmxlLCBUZXh0LCBUZXh0LCBJbnRXcml0YWJsZT4gewogICAgcHJpdmF0ZSBmaW5hbCBzdGF0aWMgVGV4dCB3b3JkTGVuZ3RoID0gbmV3IFRleHQoIndvcmRMZW5ndGgiKTsKCiAgICBAT3ZlcnJpZGUKICAgIHByb3RlY3RlZCB2b2lkIG1hcChMb25nV3JpdGFibGUga2V5LCBUZXh0IHZhbHVlLCBDb250ZXh0IGNvbnRleHQpIHRocm93cyBJT0V4Y2VwdGlvbiwgSW50ZXJydXB0ZWRFeGNlcHRpb24gewogICAgICAgIFN0cmluZ1tdIHdvcmRzID0gdmFsdWUudG9TdHJpbmcoKS5zcGxpdCgiXFxzKyIpOwogICAgICAgIGZvciAoU3RyaW5nIHN0ciA6IHdvcmRzKSB7CiAgICAgICAgICAgIGNvbnRleHQud3JpdGUod29yZExlbmd0aCwgbmV3IEludFdyaXRhYmxlKHN0ci5sZW5ndGgoKSkpOwogICAgICAgIH0KICAgIH0KfQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

public class AvgWordLengthMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {
    private final static Text wordLength = new Text(&quot;wordLength&quot;);

    @Override
    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
        String[] words = value.toString().split(&quot;\\s+&quot;);
        for (String str : words) {
            context.write(wordLength, new IntWritable(str.length()));
        }
    }
}</pre></div><div class='content'><h4>Reducer Class</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGphdmEuaW8uSU9FeGNlcHRpb247CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5pby5JbnRXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLlRleHQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UuUmVkdWNlcjsKCnB1YmxpYyBjbGFzcyBBdmdXb3JkTGVuZ3RoUmVkdWNlciBleHRlbmRzIFJlZHVjZXI8VGV4dCwgSW50V3JpdGFibGUsIFRleHQsIEludFdyaXRhYmxlPiB7CiAgICBAT3ZlcnJpZGUKICAgIHByb3RlY3RlZCB2b2lkIHJlZHVjZShUZXh0IGtleSwgSXRlcmFibGU8SW50V3JpdGFibGU+IHZhbHVlcywgQ29udGV4dCBjb250ZXh0KSB0aHJvd3MgSU9FeGNlcHRpb24sIEludGVycnVwdGVkRXhjZXB0aW9uIHsKICAgICAgICBpbnQgc3VtID0gMDsKICAgICAgICBpbnQgY291bnQgPSAwOwogICAgICAgIGZvciAoSW50V3JpdGFibGUgdmFsIDogdmFsdWVzKSB7CiAgICAgICAgICAgIHN1bSArPSB2YWwuZ2V0KCk7CiAgICAgICAgICAgIGNvdW50Kys7CiAgICAgICAgfQogICAgICAgIGludCBhdmVyYWdlID0gc3VtIC8gY291bnQ7CiAgICAgICAgY29udGV4dC53cml0ZShuZXcgVGV4dCgiQXZlcmFnZSBXb3JkIExlbmd0aCIpLCBuZXcgSW50V3JpdGFibGUoYXZlcmFnZSkpOwogICAgfQp9"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import java.io.IOException;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class AvgWordLengthReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {
    @Override
    protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
        int sum = 0;
        int count = 0;
        for (IntWritable val : values) {
            sum += val.get();
            count++;
        }
        int average = sum / count;
        context.write(new Text(&quot;Average Word Length&quot;), new IntWritable(average));
    }
}</pre></div><div class='content'><h4>Driver Class</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmNvbmYuQ29uZmlndXJhdGlvbjsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmZzLlBhdGg7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5pby5JbnRXcml0YWJsZTsKaW1wb3J0IG9yZy5hcGFjaGUuaGFkb29wLmlvLlRleHQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UuSm9iOwppbXBvcnQgb3JnLmFwYWNoZS5oYWRvb3AubWFwcmVkdWNlLmxpYi5pbnB1dC5GaWxlSW5wdXRGb3JtYXQ7CmltcG9ydCBvcmcuYXBhY2hlLmhhZG9vcC5tYXByZWR1Y2UubGliLm91dHB1dC5GaWxlT3V0cHV0Rm9ybWF0OwoKcHVibGljIGNsYXNzIEF2Z1dvcmRMZW5ndGhEcml2ZXIgewogICAgcHVibGljIHN0YXRpYyB2b2lkIG1haW4oU3RyaW5nW10gYXJncykgdGhyb3dzIEV4Y2VwdGlvbiB7CiAgICAgICAgQ29uZmlndXJhdGlvbiBjb25mID0gbmV3IENvbmZpZ3VyYXRpb24oKTsKICAgICAgICBKb2Igam9iID0gSm9iLmdldEluc3RhbmNlKGNvbmYsICJhdmVyYWdlIHdvcmQgbGVuZ3RoIik7CiAgICAgICAgam9iLnNldEphckJ5Q2xhc3MoQXZnV29yZExlbmd0aERyaXZlci5jbGFzcyk7CiAgICAgICAgam9iLnNldE1hcHBlckNsYXNzKEF2Z1dvcmRMZW5ndGhNYXBwZXIuY2xhc3MpOwogICAgICAgIGpvYi5zZXRDb21iaW5lckNsYXNzKEF2Z1dvcmRMZW5ndGhSZWR1Y2VyLmNsYXNzKTsKICAgICAgICBqb2Iuc2V0UmVkdWNlckNsYXNzKEF2Z1dvcmRMZW5ndGhSZWR1Y2VyLmNsYXNzKTsKICAgICAgICBqb2Iuc2V0T3V0cHV0S2V5Q2xhc3MoVGV4dC5jbGFzcyk7CiAgICAgICAgam9iLnNldE91dHB1dFZhbHVlQ2xhc3MoSW50V3JpdGFibGUuY2xhc3MpOwogICAgICAgIEZpbGVJbnB1dEZvcm1hdC5hZGRJbnB1dFBhdGgoam9iLCBuZXcgUGF0aChhcmdzWzBdKSk7CiAgICAgICAgRmlsZU91dHB1dEZvcm1hdC5zZXRPdXRwdXRQYXRoKGpvYiwgbmV3IFBhdGgoYXJnc1sxXSkpOwogICAgICAgIFN5c3RlbS5leGl0KGpvYi53YWl0Rm9yQ29tcGxldGlvbih0cnVlKSA/IDAgOiAxKTsKICAgIH0KfQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class AvgWordLengthDriver {
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, &quot;average word length&quot;);
        job.setJarByClass(AvgWordLengthDriver.class);
        job.setMapperClass(AvgWordLengthMapper.class);
        job.setCombinerClass(AvgWordLengthReducer.class);
        job.setReducerClass(AvgWordLengthReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Mapper Class</strong>: The <code>AvgWordLengthMapper</code> class reads the input text line by line, splits each line into words, and emits the length of each word.</li>
<li><strong>Reducer Class</strong>: The <code>AvgWordLengthReducer</code> class calculates the average length of the words by summing the lengths and dividing by the count.</li>
<li><strong>Driver Class</strong>: The <code>AvgWordLengthDriver</code> class sets up the job configuration, specifying the mapper, reducer, input, and output paths.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we explored the Apache Hadoop Ecosystem, its key components, and additional tools that enhance its capabilities. We also provided practical examples and exercises to help you understand how to implement MapReduce programs. Understanding the Hadoop Ecosystem is crucial for efficiently storing, processing, and analyzing large volumes of data in a distributed environment.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-03-practical-projects' title="Practical Projects">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='06-02-apache-spark-ecosystem' title="Apache Spark Ecosystem">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Advertising</h1>
<p>This space is reserved for advertising.</p>
<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>Thank you for collaborating!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
			









		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
