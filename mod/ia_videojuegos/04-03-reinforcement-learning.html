<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning</title>

    <link rel="alternate" href="https://campusempresa.com/mod/ia_videojuegos/04-03-aprendizaje-refuerzo" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/ia_videojuegos/04-03-aprenentatge-reforc" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/ia_videojuegos/04-03-reinforcement-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/ia_videojuegos/04-03-reinforcement-learning" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/ia_videojuegos/04-03-reinforcement-learning" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-neural-networks' title="Neural Networks in Video Games">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Reinforcement Learning</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-implementation-agent' title="Implementation of a Learning Agent">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize some notion of cumulative reward. This module will cover the fundamental concepts of RL, its applications in video games, and practical examples to help you understand how to implement RL in your projects.</p>
</div><h1>Key Concepts in Reinforcement Learning</h1>
<div class='content'><ol>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: The external system with which the agent interacts.</li>
<li><strong>State (s)</strong>: A representation of the current situation of the agent.</li>
<li><strong>Action (a)</strong>: A set of all possible moves the agent can make.</li>
<li><strong>Reward (r)</strong>: The feedback from the environment based on the action taken.</li>
<li><strong>Policy (Ï€)</strong>: A strategy that the agent employs to determine the next action based on the current state.</li>
<li><strong>Value Function (V)</strong>: A function that estimates the expected reward for a given state.</li>
<li><strong>Q-Function (Q)</strong>: A function that estimates the expected reward for a given state-action pair.</li>
</ol>
</div><h1>How Reinforcement Learning Works</h1>
<div class='content'><ol>
<li><strong>Initialization</strong>: The agent starts with an initial state.</li>
<li><strong>Action Selection</strong>: The agent selects an action based on its policy.</li>
<li><strong>Transition</strong>: The action causes a transition to a new state.</li>
<li><strong>Reward</strong>: The agent receives a reward from the environment.</li>
<li><strong>Update</strong>: The agent updates its policy based on the reward and the new state.</li>
</ol>
</div><h1>Types of Reinforcement Learning</h1>
<div class='content'><ol>
<li>
<p><strong>Model-Free RL</strong>: The agent learns directly from interactions with the environment without a model of the environment.</p>
<ul>
<li><strong>Q-Learning</strong>: A value-based method where the agent learns the value of actions in states.</li>
<li><strong>SARSA (State-Action-Reward-State-Action)</strong>: Similar to Q-Learning but updates the policy based on the action taken in the next state.</li>
</ul>
</li>
<li>
<p><strong>Model-Based RL</strong>: The agent uses a model of the environment to make decisions.</p>
<ul>
<li><strong>Dynamic Programming</strong>: Uses a model of the environment to compute the optimal policy.</li>
</ul>
</li>
</ol>
</div><h1>Q-Learning Algorithm</h1>
<div class='content'><p>Q-Learning is one of the most popular model-free RL algorithms. It aims to learn the value of the optimal action-selection policy.</p>
</div><h2>Q-Learning Formula</h2>
<div class='content'><p>\[ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \]</p>
<p>Where:</p>
<ul>
<li>\( Q(s, a) \) is the current Q-value.</li>
<li>\( \alpha \) is the learning rate.</li>
<li>\( r \) is the reward received after taking action \( a \) from state \( s \).</li>
<li>\( \gamma \) is the discount factor.</li>
<li>\( \max_{a'} Q(s', a') \) is the maximum Q-value for the next state \( s' \).</li>
</ul>
</div><h2>Example: Implementing Q-Learning in Python</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCByYW5kb20KCiMgSW5pdGlhbGl6ZSBwYXJhbWV0ZXJzCmFscGhhID0gMC4xICAjIExlYXJuaW5nIHJhdGUKZ2FtbWEgPSAwLjYgICMgRGlzY291bnQgZmFjdG9yCmVwc2lsb24gPSAwLjEgICMgRXhwbG9yYXRpb24gZmFjdG9yCgojIERlZmluZSB0aGUgZW52aXJvbm1lbnQKc3RhdGVzID0gWyJBIiwgIkIiLCAiQyIsICJEIl0KYWN0aW9ucyA9IFsibGVmdCIsICJyaWdodCJdCnJld2FyZHMgPSB7CiAgICAoIkEiLCAibGVmdCIpOiAxLAogICAgKCJBIiwgInJpZ2h0Iik6IDAsCiAgICAoIkIiLCAibGVmdCIpOiAwLAogICAgKCJCIiwgInJpZ2h0Iik6IDEsCiAgICAoIkMiLCAibGVmdCIpOiAxLAogICAgKCJDIiwgInJpZ2h0Iik6IDAsCiAgICAoIkQiLCAibGVmdCIpOiAwLAogICAgKCJEIiwgInJpZ2h0Iik6IDEsCn0KCiMgSW5pdGlhbGl6ZSBRLXRhYmxlClEgPSB7fQpmb3Igc3RhdGUgaW4gc3RhdGVzOgogICAgZm9yIGFjdGlvbiBpbiBhY3Rpb25zOgogICAgICAgIFFbKHN0YXRlLCBhY3Rpb24pXSA9IDAKCiMgRGVmaW5lIHRoZSBwb2xpY3kKZGVmIGNob29zZV9hY3Rpb24oc3RhdGUpOgogICAgaWYgcmFuZG9tLnVuaWZvcm0oMCwgMSkgPCBlcHNpbG9uOgogICAgICAgIHJldHVybiByYW5kb20uY2hvaWNlKGFjdGlvbnMpCiAgICBlbHNlOgogICAgICAgIHJldHVybiBtYXgoYWN0aW9ucywga2V5PWxhbWJkYSBhY3Rpb246IFFbKHN0YXRlLCBhY3Rpb24pXSkKCiMgVHJhaW5pbmcgdGhlIGFnZW50CmZvciBlcGlzb2RlIGluIHJhbmdlKDEwMDApOgogICAgc3RhdGUgPSByYW5kb20uY2hvaWNlKHN0YXRlcykKICAgIHdoaWxlIHN0YXRlICE9ICJEIjogICMgVGVybWluYWwgc3RhdGUKICAgICAgICBhY3Rpb24gPSBjaG9vc2VfYWN0aW9uKHN0YXRlKQogICAgICAgIG5leHRfc3RhdGUgPSAiRCIgaWYgc3RhdGUgPT0gIkMiIGFuZCBhY3Rpb24gPT0gInJpZ2h0IiBlbHNlIHJhbmRvbS5jaG9pY2Uoc3RhdGVzKQogICAgICAgIHJld2FyZCA9IHJld2FyZHMuZ2V0KChzdGF0ZSwgYWN0aW9uKSwgMCkKICAgICAgICBvbGRfdmFsdWUgPSBRWyhzdGF0ZSwgYWN0aW9uKV0KICAgICAgICBuZXh0X21heCA9IG1heChRWyhuZXh0X3N0YXRlLCBhKV0gZm9yIGEgaW4gYWN0aW9ucykKICAgICAgICBRWyhzdGF0ZSwgYWN0aW9uKV0gPSBvbGRfdmFsdWUgKyBhbHBoYSAqIChyZXdhcmQgKyBnYW1tYSAqIG5leHRfbWF4IC0gb2xkX3ZhbHVlKQogICAgICAgIHN0YXRlID0gbmV4dF9zdGF0ZQoKIyBEaXNwbGF5IHRoZSBsZWFybmVkIFEtdmFsdWVzCmZvciBzdGF0ZSBpbiBzdGF0ZXM6CiAgICBmb3IgYWN0aW9uIGluIGFjdGlvbnM6CiAgICAgICAgcHJpbnQoZiJRKHtzdGF0ZX0sIHthY3Rpb259KSA9IHtRWyhzdGF0ZSwgYWN0aW9uKV06LjJmfSIp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Initialize parameters
alpha = 0.1  # Learning rate
gamma = 0.6  # Discount factor
epsilon = 0.1  # Exploration factor

# Define the environment
states = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]
actions = [&quot;left&quot;, &quot;right&quot;]
rewards = {
    (&quot;A&quot;, &quot;left&quot;): 1,
    (&quot;A&quot;, &quot;right&quot;): 0,
    (&quot;B&quot;, &quot;left&quot;): 0,
    (&quot;B&quot;, &quot;right&quot;): 1,
    (&quot;C&quot;, &quot;left&quot;): 1,
    (&quot;C&quot;, &quot;right&quot;): 0,
    (&quot;D&quot;, &quot;left&quot;): 0,
    (&quot;D&quot;, &quot;right&quot;): 1,
}

# Initialize Q-table
Q = {}
for state in states:
    for action in actions:
        Q[(state, action)] = 0

# Define the policy
def choose_action(state):
    if random.uniform(0, 1) &lt; epsilon:
        return random.choice(actions)
    else:
        return max(actions, key=lambda action: Q[(state, action)])

# Training the agent
for episode in range(1000):
    state = random.choice(states)
    while state != &quot;D&quot;:  # Terminal state
        action = choose_action(state)
        next_state = &quot;D&quot; if state == &quot;C&quot; and action == &quot;right&quot; else random.choice(states)
        reward = rewards.get((state, action), 0)
        old_value = Q[(state, action)]
        next_max = max(Q[(next_state, a)] for a in actions)
        Q[(state, action)] = old_value + alpha * (reward + gamma * next_max - old_value)
        state = next_state

# Display the learned Q-values
for state in states:
    for action in actions:
        print(f&quot;Q({state}, {action}) = {Q[(state, action)]:.2f}&quot;)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ol>
<li><strong>Initialization</strong>: We initialize the learning rate, discount factor, exploration factor, and Q-table.</li>
<li><strong>Policy</strong>: The agent chooses an action based on the epsilon-greedy policy.</li>
<li><strong>Training</strong>: The agent interacts with the environment, updates the Q-values based on the received rewards, and transitions to the next state.</li>
<li><strong>Output</strong>: The learned Q-values for each state-action pair are displayed.</li>
</ol>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Exercise: Implement Q-Learning for a Simple Grid World</h2>
<div class='content'><ol>
<li><strong>Environment</strong>: Create a 4x4 grid world where the agent starts at the top-left corner and the goal is at the bottom-right corner.</li>
<li><strong>Actions</strong>: The agent can move up, down, left, or right.</li>
<li><strong>Rewards</strong>: The agent receives a reward of +1 for reaching the goal and -1 for hitting a wall.</li>
</ol>
</div><h2>Solution</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Define the environment
grid_size = 4
actions = [&quot;up&quot;, &quot;down&quot;, &quot;left&quot;, &quot;right&quot;]
rewards = np.zeros((grid_size, grid_size))
rewards[3, 3] = 1  # Goal state

# Initialize Q-table
Q = np.zeros((grid_size, grid_size, len(actions)))

# Define the policy
def choose_action(state, epsilon):
    if np.random.uniform(0, 1) &lt; epsilon:
        return np.random.choice(actions)
    else:
        return actions[np.argmax(Q[state[0], state[1], :])]

# Define the next state function
def next_state(state, action):
    if action == &quot;up&quot;:
        return (max(state[0] - 1, 0), state[1])
    elif action == &quot;down&quot;:
        return (min(state[0] + 1, grid_size - 1), state[1])
    elif action == &quot;left&quot;:
        return (state[0], max(state[1] - 1, 0))
    elif action == &quot;right&quot;:
        return (state[0], min(state[1] + 1, grid_size - 1))

# Training the agent
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.1  # Exploration factor

for episode in range(1000):
    state = (0, 0)  # Start state
    while state != (3, 3):  # Goal state
        action = choose_action(state, epsilon)
        next_s = next_state(state, action)
        reward = rewards[next_s[0], next_s[1]]
        old_value = Q[state[0], state[1], actions.index(action)]
        next_max = np.max(Q[next_s[0], next_s[1], :])
        Q[state[0], state[1], actions.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)
        state = next_s

# Display the learned Q-values
for i in range(grid_size):
    for j in range(grid_size):
        print(f&quot;Q({i}, {j}) = {Q[i, j, :]}&quot;)

# Display the optimal policy
policy = np.zeros((grid_size, grid_size), dtype=str)
for i in range(grid_size):
    for j in range(grid_size):
        policy[i, j] = actions[np.argmax(Q[i, j, :])]
print(&quot;Optimal Policy:&quot;)
print(policy)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ol>
<li><strong>Environment</strong>: A 4x4 grid world is defined with rewards and actions.</li>
<li><strong>Q-Table</strong>: The Q-table is initialized to zeros.</li>
<li><strong>Policy</strong>: The agent chooses an action based on the epsilon-greedy policy.</li>
<li><strong>Training</strong>: The agent interacts with the environment, updates the Q-values, and transitions to the next state.</li>
<li><strong>Output</strong>: The learned Q-values and the optimal policy are displayed.</li>
</ol>
</div><h1>Summary</h1>
<div class='content'><p>In this section, we covered the basics of Reinforcement Learning, focusing on key concepts, types of RL, and the Q-Learning algorithm. We provided a detailed example of implementing Q-Learning in Python and a practical exercise to reinforce the learned concepts. Reinforcement Learning is a powerful tool for developing intelligent behaviors in game characters, and mastering it will significantly enhance your game development skills.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-neural-networks' title="Neural Networks in Video Games">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-implementation-agent' title="Implementation of a Learning Agent">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiÃ¨ncia d'Ãºs i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
