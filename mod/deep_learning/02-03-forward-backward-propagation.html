<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Forward and Backward Propagation</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-03-forward-backward-propagation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/02-03-propagacion-hacia-adelante-atras" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='02-02-activation-function' title="Activation Function">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Forward and Backward Propagation</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='02-04-optimization-loss-function' title="Optimization and Loss Function">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Forward and backward propagation are fundamental processes in training neural networks. They are the mechanisms through which a neural network learns from data by adjusting its weights and biases. This section will cover the concepts, mathematical foundations, and practical implementation of forward and backward propagation.</p>
</div><h1>Forward Propagation</h1>
<div class='content'><p>Forward propagation is the process of passing input data through the neural network to obtain an output. This involves computing the activations of each neuron in the network layer by layer, starting from the input layer and moving towards the output layer.</p>
</div><h2>Steps in Forward Propagation</h2>
<div class='content'><ol>
<li><strong>Input Layer</strong>: The input data is fed into the input layer of the neural network.</li>
<li><strong>Weighted Sum</strong>: For each neuron in the subsequent layers, compute the weighted sum of inputs:
\[
z = \sum_{i=1}^{n} w_i x_i + b
\]
where \( w_i \) are the weights, \( x_i \) are the inputs, and \( b \) is the bias.</li>
<li><strong>Activation Function</strong>: Apply an activation function \( f \) to the weighted sum to get the neuron's output:
\[
a = f(z)
\]</li>
<li><strong>Output Layer</strong>: The final layer's activations are the network's output.</li>
</ol>
</div><h2>Example Code for Forward Propagation</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh6KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXopKQoKZGVmIGZvcndhcmRfcHJvcGFnYXRpb24oWCwgd2VpZ2h0cywgYmlhc2VzKToKICAgIGFjdGl2YXRpb25zID0gWAogICAgZm9yIHcsIGIgaW4gemlwKHdlaWdodHMsIGJpYXNlcyk6CiAgICAgICAgeiA9IG5wLmRvdChhY3RpdmF0aW9ucywgdykgKyBiCiAgICAgICAgYWN0aXZhdGlvbnMgPSBzaWdtb2lkKHopCiAgICByZXR1cm4gYWN0aXZhdGlvbnMKCiMgRXhhbXBsZSB1c2FnZQpYID0gbnAuYXJyYXkoW1swLjUsIDAuMV1dKSAgIyBJbnB1dCBkYXRhCndlaWdodHMgPSBbbnAuYXJyYXkoW1swLjIsIDAuOF0sIFswLjUsIDAuM11dKSwgbnAuYXJyYXkoW1swLjddLCBbMC45XV0pXSAgIyBXZWlnaHRzCmJpYXNlcyA9IFtucC5hcnJheShbMC4xLCAwLjJdKSwgbnAuYXJyYXkoWzAuM10pXSAgIyBCaWFzZXMKCm91dHB1dCA9IGZvcndhcmRfcHJvcGFnYXRpb24oWCwgd2VpZ2h0cywgYmlhc2VzKQpwcmludCgiT3V0cHV0OiIsIG91dHB1dCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def forward_propagation(X, weights, biases):
    activations = X
    for w, b in zip(weights, biases):
        z = np.dot(activations, w) + b
        activations = sigmoid(z)
    return activations

# Example usage
X = np.array([[0.5, 0.1]])  # Input data
weights = [np.array([[0.2, 0.8], [0.5, 0.3]]), np.array([[0.7], [0.9]])]  # Weights
biases = [np.array([0.1, 0.2]), np.array([0.3])]  # Biases

output = forward_propagation(X, weights, biases)
print(&quot;Output:&quot;, output)</pre></div><div class='content'></div><h1>Backward Propagation</h1>
<div class='content'><p>Backward propagation (backpropagation) is the process of updating the weights and biases of the network to minimize the error between the predicted output and the actual output. This is done by computing the gradient of the loss function with respect to each weight and bias and then adjusting them in the direction that reduces the loss.</p>
</div><h2>Steps in Backward Propagation</h2>
<div class='content'><ol>
<li><strong>Compute Loss</strong>: Calculate the loss (error) between the predicted output and the actual output using a loss function \( L \).</li>
<li><strong>Output Layer Gradient</strong>: Compute the gradient of the loss with respect to the output layer's activation.</li>
<li><strong>Backpropagate the Error</strong>: For each layer, starting from the output layer and moving backward:
<ul>
<li>Compute the gradient of the loss with respect to the weighted sum \( z \).</li>
<li>Compute the gradient of the loss with respect to the weights and biases.</li>
<li>Update the weights and biases using the gradients and a learning rate \( \eta \).</li>
</ul>
</li>
</ol>
</div><h2>Mathematical Formulation</h2>
<div class='content'><p>For a single training example, the gradients are computed as follows:</p>
<ol>
<li><strong>Loss Function</strong>: Assume a simple mean squared error loss:
\[
L = \frac{1}{2} (y_{\text{pred}} - y_{\text{true}})^2
\]</li>
<li><strong>Gradient of Loss w.r.t. Output Activation</strong>:
\[
\delta = \frac{\partial L}{\partial a} = (a - y_{\text{true}})
\]</li>
<li><strong>Gradient of Loss w.r.t. Weighted Sum</strong>:
\[
\delta_z = \delta \cdot f'(z)
\]
where \( f'(z) \) is the derivative of the activation function.</li>
<li><strong>Gradient of Loss w.r.t. Weights and Biases</strong>:
\[
\frac{\partial L}{\partial w} = \delta_z \cdot a_{\text{prev}}
\]
\[
\frac{\partial L}{\partial b} = \delta_z
\]</li>
</ol>
</div><h2>Example Code for Backward Propagation</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHNpZ21vaWRfZGVyaXZhdGl2ZSh6KToKICAgIHJldHVybiBzaWdtb2lkKHopICogKDEgLSBzaWdtb2lkKHopKQoKZGVmIGJhY2t3YXJkX3Byb3BhZ2F0aW9uKFgsIFksIHdlaWdodHMsIGJpYXNlcywgbGVhcm5pbmdfcmF0ZSk6CiAgICBhY3RpdmF0aW9ucyA9IFtYXQogICAgenMgPSBbXQogICAgCiAgICAjIEZvcndhcmQgcGFzcwogICAgZm9yIHcsIGIgaW4gemlwKHdlaWdodHMsIGJpYXNlcyk6CiAgICAgICAgeiA9IG5wLmRvdChhY3RpdmF0aW9uc1stMV0sIHcpICsgYgogICAgICAgIHpzLmFwcGVuZCh6KQogICAgICAgIGFjdGl2YXRpb25zLmFwcGVuZChzaWdtb2lkKHopKQogICAgCiAgICAjIEJhY2t3YXJkIHBhc3MKICAgIGRlbHRhID0gYWN0aXZhdGlvbnNbLTFdIC0gWQogICAgZGVsdGFzID0gW2RlbHRhXQogICAgCiAgICBmb3IgbCBpbiByYW5nZSgyLCBsZW4od2VpZ2h0cykgKyAxKToKICAgICAgICB6ID0genNbLWxdCiAgICAgICAgc3AgPSBzaWdtb2lkX2Rlcml2YXRpdmUoeikKICAgICAgICBkZWx0YSA9IG5wLmRvdChkZWx0YXNbLTFdLCB3ZWlnaHRzWy1sICsgMV0uVCkgKiBzcAogICAgICAgIGRlbHRhcy5hcHBlbmQoZGVsdGEpCiAgICAKICAgIGRlbHRhcy5yZXZlcnNlKCkKICAgIAogICAgIyBHcmFkaWVudCBkZXNjZW50IHVwZGF0ZQogICAgZm9yIGkgaW4gcmFuZ2UobGVuKHdlaWdodHMpKToKICAgICAgICB3ZWlnaHRzW2ldIC09IGxlYXJuaW5nX3JhdGUgKiBucC5kb3QoYWN0aXZhdGlvbnNbaV0uVCwgZGVsdGFzW2ldKQogICAgICAgIGJpYXNlc1tpXSAtPSBsZWFybmluZ19yYXRlICogbnAuc3VtKGRlbHRhc1tpXSwgYXhpcz0wLCBrZWVwZGltcz1UcnVlKQogICAgCiAgICByZXR1cm4gd2VpZ2h0cywgYmlhc2VzCgojIEV4YW1wbGUgdXNhZ2UKWSA9IG5wLmFycmF5KFtbMV1dKSAgIyBUcnVlIG91dHB1dApsZWFybmluZ19yYXRlID0gMC4xCgp3ZWlnaHRzLCBiaWFzZXMgPSBiYWNrd2FyZF9wcm9wYWdhdGlvbihYLCBZLCB3ZWlnaHRzLCBiaWFzZXMsIGxlYXJuaW5nX3JhdGUpCnByaW50KCJVcGRhdGVkIFdlaWdodHM6Iiwgd2VpZ2h0cykKcHJpbnQoIlVwZGF0ZWQgQmlhc2VzOiIsIGJpYXNlcyk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

def backward_propagation(X, Y, weights, biases, learning_rate):
    activations = [X]
    zs = []
    
    # Forward pass
    for w, b in zip(weights, biases):
        z = np.dot(activations[-1], w) + b
        zs.append(z)
        activations.append(sigmoid(z))
    
    # Backward pass
    delta = activations[-1] - Y
    deltas = [delta]
    
    for l in range(2, len(weights) + 1):
        z = zs[-l]
        sp = sigmoid_derivative(z)
        delta = np.dot(deltas[-1], weights[-l + 1].T) * sp
        deltas.append(delta)
    
    deltas.reverse()
    
    # Gradient descent update
    for i in range(len(weights)):
        weights[i] -= learning_rate * np.dot(activations[i].T, deltas[i])
        biases[i] -= learning_rate * np.sum(deltas[i], axis=0, keepdims=True)
    
    return weights, biases

# Example usage
Y = np.array([[1]])  # True output
learning_rate = 0.1

weights, biases = backward_propagation(X, Y, weights, biases, learning_rate)
print(&quot;Updated Weights:&quot;, weights)
print(&quot;Updated Biases:&quot;, biases)</pre></div><div class='content'></div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Exercise: Implement Forward and Backward Propagation</h2>
<div class='content'><p><strong>Task</strong>: Implement a simple neural network with one hidden layer and train it using forward and backward propagation.</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Initialize the weights and biases.</li>
<li>Implement forward propagation.</li>
<li>Implement backward propagation.</li>
<li>Train the network on a simple dataset.</li>
</ol>
<p><strong>Dataset</strong>: XOR problem</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# XOR dataset
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
Y = np.array([[0], [1], [1], [0]])

# Initialize weights and biases
np.random.seed(42)
weights = [np.random.randn(2, 2), np.random.randn(2, 1)]
biases = [np.random.randn(1, 2), np.random.randn(1, 1)]

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

def forward_propagation(X, weights, biases):
    activations = X
    for w, b in zip(weights, biases):
        z = np.dot(activations, w) + b
        activations = sigmoid(z)
    return activations

def backward_propagation(X, Y, weights, biases, learning_rate):
    activations = [X]
    zs = []
    
    # Forward pass
    for w, b in zip(weights, biases):
        z = np.dot(activations[-1], w) + b
        zs.append(z)
        activations.append(sigmoid(z))
    
    # Backward pass
    delta = activations[-1] - Y
    deltas = [delta]
    
    for l in range(2, len(weights) + 1):
        z = zs[-l]
        sp = sigmoid_derivative(z)
        delta = np.dot(deltas[-1], weights[-l + 1].T) * sp
        deltas.append(delta)
    
    deltas.reverse()
    
    # Gradient descent update
    for i in range(len(weights)):
        weights[i] -= learning_rate * np.dot(activations[i].T, deltas[i])
        biases[i] -= learning_rate * np.sum(deltas[i], axis=0, keepdims=True)
    
    return weights, biases

# Training the network
learning_rate = 0.1
epochs = 10000

for epoch in range(epochs):
    weights, biases = backward_propagation(X, Y, weights, biases, learning_rate)

# Testing the network
output = forward_propagation(X, weights, biases)
print(&quot;Predicted Output:\n&quot;, output)</pre></div><div class='content'></div><h2>Solution Explanation</h2>
<div class='content'><ol>
<li><strong>Initialization</strong>: Randomly initialize the weights and biases.</li>
<li><strong>Forward Propagation</strong>: Compute the activations for each layer.</li>
<li><strong>Backward Propagation</strong>: Compute the gradients and update the weights and biases.</li>
<li><strong>Training</strong>: Iterate the forward and backward propagation steps for a specified number of epochs.</li>
</ol>
</div><h1>Summary</h1>
<div class='content'><p>In this section, we covered the essential concepts of forward and backward propagation in neural networks. We explored the mathematical foundations, implemented the processes in code, and applied them to a practical exercise. Understanding these concepts is crucial for training neural networks effectively and forms the basis for more advanced deep learning techniques.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='02-02-activation-function' title="Activation Function">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='02-04-optimization-loss-function' title="Optimization and Loss Function">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
