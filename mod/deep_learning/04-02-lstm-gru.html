<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LSTM and GRU</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/04-02-lstm-gru" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/04-02-lstm-gru" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/04-02-lstm-gru" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/deep_learning/04-02-lstm-gru" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/deep_learning/04-02-lstm-gru" class="px-2">CA</a>
<br>
			<cite>Building today's and tomorrow's society</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Programming Languages</a>
				 					<a href="/categ/frameworks">Frameworks and Libraries</a>
				 					<a href="/categ/tech-tools">Technical Tools</a>
				 					<a href="/categ/foundations">Theoretical Foundations</a>
				 					<a href="/categ/soft-skills">Social Skills</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-01-introduction-rnn' title="Introduction to RNN">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">LSTM and GRU</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-03-rnn-applications-nlp' title="RNN Applications in Natural Language Processing">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>In this section, we will delve into Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks, which are advanced types of Recurrent Neural Networks (RNNs) designed to address the limitations of traditional RNNs, particularly the vanishing gradient problem.</p>
</div><h1><ol>
<li>Introduction to LSTM</li>
</ol>
</h1>
<div class='content'></div><h2><p>What is LSTM?</p>
</h2>
<div class='content'><p>LSTM stands for Long Short-Term Memory. It is a type of RNN architecture that is capable of learning long-term dependencies. LSTMs were introduced by Hochreiter and Schmidhuber in 1997 and have been refined and popularized over the years.</p>
</div><h2><p>Key Components of LSTM</p>
</h2>
<div class='content'><p>LSTM networks have a more complex structure compared to traditional RNNs. The key components include:</p>
<ul>
<li><strong>Cell State</strong>: The cell state is the memory of the network. It carries information across different time steps.</li>
<li><strong>Gates</strong>: LSTMs use gates to control the flow of information. There are three types of gates:
<ul>
<li><strong>Forget Gate</strong>: Decides what information to discard from the cell state.</li>
<li><strong>Input Gate</strong>: Decides which values from the input to update the cell state.</li>
<li><strong>Output Gate</strong>: Decides what part of the cell state to output.</li>
</ul>
</li>
</ul>
</div><h2><p>LSTM Cell Structure</p>
</h2>
<div class='content'><p>The structure of an LSTM cell can be visualized as follows:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ICAgICAgRm9yZ2V0IEdhdGUgICAgICAgSW5wdXQgR2F0ZSAgICAgICAgT3V0cHV0IEdhdGUKICAgICAgICAgfCAgICAgICAgICAgICAgICAgIHwgICAgICAgICAgICAgICAgICB8CiAgICAgICAgIHYgICAgICAgICAgICAgICAgICB2ICAgICAgICAgICAgICAgICAgdgogICAgIFtzaWdtb2lkXSAgICAgICAgICAgW3NpZ21vaWRdICAgICAgICAgIFtzaWdtb2lkXQogICAgICAgICB8ICAgICAgICAgICAgICAgICAgfCAgICAgICAgICAgICAgICAgIHwKICAgICAgICAgdiAgICAgICAgICAgICAgICAgIHYgICAgICAgICAgICAgICAgICB2CiAgICAgW2VsZW1lbnQtd2lzZV0gICAgIFtlbGVtZW50LXdpc2VdICAgICBbZWxlbWVudC13aXNlXQogICAgICAgICB8ICAgICAgICAgICAgICAgICAgfCAgICAgICAgICAgICAgICAgIHwKICAgICAgICAgdiAgICAgICAgICAgICAgICAgIHYgICAgICAgICAgICAgICAgICB2CiAgICAgW2NlbGwgc3RhdGVdICAgICAgIFtjZWxsIHN0YXRlXSAgICAgICBbY2VsbCBzdGF0ZV0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>      Forget Gate       Input Gate        Output Gate
         |                  |                  |
         v                  v                  v
     [sigmoid]           [sigmoid]          [sigmoid]
         |                  |                  |
         v                  v                  v
     [element-wise]     [element-wise]     [element-wise]
         |                  |                  |
         v                  v                  v
     [cell state]       [cell state]       [cell state]</pre></div><div class='content'></div><h2><p>LSTM Equations</p>
</h2>
<div class='content'><p>The operations within an LSTM cell can be described by the following equations:</p>
<ol>
<li><strong>Forget Gate</strong>:
\[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\]</li>
<li><strong>Input Gate</strong>:
\[
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\]
\[
\tilde{C}<em>t = \tanh(W_C \cdot [h</em>{t-1}, x_t] + b_C)
\]</li>
<li><strong>Cell State Update</strong>:
\[
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
\]</li>
<li><strong>Output Gate</strong>:
\[
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\]
\[
h_t = o_t * \tanh(C_t)
\]</li>
</ol>
</div><h1><ol start="2">
<li>Introduction to GRU</li>
</ol>
</h1>
<div class='content'></div><h2><p>What is GRU?</p>
</h2>
<div class='content'><p>GRU stands for Gated Recurrent Unit. It is a variant of the LSTM network introduced by Cho et al. in 2014. GRUs aim to simplify the LSTM architecture while maintaining its performance.</p>
</div><h2><p>Key Components of GRU</p>
</h2>
<div class='content'><p>GRUs combine the forget and input gates into a single update gate and merge the cell state and hidden state. The key components include:</p>
<ul>
<li><strong>Update Gate</strong>: Controls how much of the past information needs to be passed along to the future.</li>
<li><strong>Reset Gate</strong>: Controls how much of the past information to forget.</li>
</ul>
</div><h2><p>GRU Cell Structure</p>
</h2>
<div class='content'><p>The structure of a GRU cell can be visualized as follows:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ICAgICAgVXBkYXRlIEdhdGUgICAgICAgIFJlc2V0IEdhdGUKICAgICAgICAgfCAgICAgICAgICAgICAgICAgIHwKICAgICAgICAgdiAgICAgICAgICAgICAgICAgIHYKICAgICBbc2lnbW9pZF0gICAgICAgICAgIFtzaWdtb2lkXQogICAgICAgICB8ICAgICAgICAgICAgICAgICAgfAogICAgICAgICB2ICAgICAgICAgICAgICAgICAgdgogICAgIFtlbGVtZW50LXdpc2VdICAgICBbZWxlbWVudC13aXNlXQogICAgICAgICB8ICAgICAgICAgICAgICAgICAgfAogICAgICAgICB2ICAgICAgICAgICAgICAgICAgdgogICAgIFtoaWRkZW4gc3RhdGVdICAgICBbaGlkZGVuIHN0YXRlXQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>      Update Gate        Reset Gate
         |                  |
         v                  v
     [sigmoid]           [sigmoid]
         |                  |
         v                  v
     [element-wise]     [element-wise]
         |                  |
         v                  v
     [hidden state]     [hidden state]</pre></div><div class='content'></div><h2><p>GRU Equations</p>
</h2>
<div class='content'><p>The operations within a GRU cell can be described by the following equations:</p>
<ol>
<li><strong>Update Gate</strong>:
\[
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
\]</li>
<li><strong>Reset Gate</strong>:
\[
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
\]</li>
<li><strong>Candidate Hidden State</strong>:
\[
\tilde{h}<em>t = \tanh(W \cdot [r_t * h</em>{t-1}, x_t] + b)
\]</li>
<li><strong>Final Hidden State</strong>:
\[
h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t
\]</li>
</ol>
</div><h1><ol start="3">
<li>Practical Example</li>
</ol>
</h1>
<div class='content'></div><h2><p>Implementing LSTM in Python with TensorFlow</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBMU1RNLCBEZW5zZQoKIyBTYW1wbGUgZGF0YQpYX3RyYWluID0gW1swLjEsIDAuMiwgMC4zXSwgWzAuMiwgMC4zLCAwLjRdLCBbMC4zLCAwLjQsIDAuNV1dCnlfdHJhaW4gPSBbMC40LCAwLjUsIDAuNl0KCiMgUmVzaGFwZSBkYXRhIHRvIFtzYW1wbGVzLCB0aW1lIHN0ZXBzLCBmZWF0dXJlc10KWF90cmFpbiA9IHRmLnJlc2hhcGUoWF90cmFpbiwgKDMsIDMsIDEpKQoKIyBCdWlsZCBMU1RNIG1vZGVsCm1vZGVsID0gU2VxdWVudGlhbCgpCm1vZGVsLmFkZChMU1RNKDUwLCBhY3RpdmF0aW9uPSdyZWx1JywgaW5wdXRfc2hhcGU9KDMsIDEpKSkKbW9kZWwuYWRkKERlbnNlKDEpKQptb2RlbC5jb21waWxlKG9wdGltaXplcj0nYWRhbScsIGxvc3M9J21zZScpCgojIFRyYWluIHRoZSBtb2RlbAptb2RlbC5maXQoWF90cmFpbiwgeV90cmFpbiwgZXBvY2hzPTIwMCwgdmVyYm9zZT0wKQoKIyBNYWtlIHByZWRpY3Rpb25zCnByZWRpY3Rpb25zID0gbW9kZWwucHJlZGljdChYX3RyYWluKQpwcmludChwcmVkaWN0aW9ucyk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Sample data
X_train = [[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5]]
y_train = [0.4, 0.5, 0.6]

# Reshape data to [samples, time steps, features]
X_train = tf.reshape(X_train, (3, 3, 1))

# Build LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(3, 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X_train, y_train, epochs=200, verbose=0)

# Make predictions
predictions = model.predict(X_train)
print(predictions)</pre></div><div class='content'></div><h2><p>Implementing GRU in Python with TensorFlow</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLm1vZGVscyBpbXBvcnQgU2VxdWVudGlhbApmcm9tIHRlbnNvcmZsb3cua2VyYXMubGF5ZXJzIGltcG9ydCBHUlUsIERlbnNlCgojIFNhbXBsZSBkYXRhClhfdHJhaW4gPSBbWzAuMSwgMC4yLCAwLjNdLCBbMC4yLCAwLjMsIDAuNF0sIFswLjMsIDAuNCwgMC41XV0KeV90cmFpbiA9IFswLjQsIDAuNSwgMC42XQoKIyBSZXNoYXBlIGRhdGEgdG8gW3NhbXBsZXMsIHRpbWUgc3RlcHMsIGZlYXR1cmVzXQpYX3RyYWluID0gdGYucmVzaGFwZShYX3RyYWluLCAoMywgMywgMSkpCgojIEJ1aWxkIEdSVSBtb2RlbAptb2RlbCA9IFNlcXVlbnRpYWwoKQptb2RlbC5hZGQoR1JVKDUwLCBhY3RpdmF0aW9uPSdyZWx1JywgaW5wdXRfc2hhcGU9KDMsIDEpKSkKbW9kZWwuYWRkKERlbnNlKDEpKQptb2RlbC5jb21waWxlKG9wdGltaXplcj0nYWRhbScsIGxvc3M9J21zZScpCgojIFRyYWluIHRoZSBtb2RlbAptb2RlbC5maXQoWF90cmFpbiwgeV90cmFpbiwgZXBvY2hzPTIwMCwgdmVyYm9zZT0wKQoKIyBNYWtlIHByZWRpY3Rpb25zCnByZWRpY3Rpb25zID0gbW9kZWwucHJlZGljdChYX3RyYWluKQpwcmludChwcmVkaWN0aW9ucyk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense

# Sample data
X_train = [[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5]]
y_train = [0.4, 0.5, 0.6]

# Reshape data to [samples, time steps, features]
X_train = tf.reshape(X_train, (3, 3, 1))

# Build GRU model
model = Sequential()
model.add(GRU(50, activation='relu', input_shape=(3, 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X_train, y_train, epochs=200, verbose=0)

# Make predictions
predictions = model.predict(X_train)
print(predictions)</pre></div><div class='content'></div><h1><ol start="4">
<li>Practical Exercises</li>
</ol>
</h1>
<div class='content'></div><h2><p>Exercise 1: Implementing LSTM for Time Series Prediction</p>
</h2>
<div class='content'><p><strong>Task</strong>: Use LSTM to predict the next value in a given time series.</p>
<p><strong>Dataset</strong>: Use a simple sine wave dataset.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCB0ZW5zb3JmbG93IGFzIHRmCmZyb20gdGVuc29yZmxvdy5rZXJhcy5tb2RlbHMgaW1wb3J0IFNlcXVlbnRpYWwKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgTFNUTSwgRGVuc2UKCiMgR2VuZXJhdGUgc2luZSB3YXZlIGRhdGEKdGltZV9zdGVwcyA9IG5wLmxpbnNwYWNlKDAsIDEwMCwgMTAwMCkKZGF0YSA9IG5wLnNpbih0aW1lX3N0ZXBzKQoKIyBQcmVwYXJlIHRoZSBkYXRhc2V0CmRlZiBjcmVhdGVfZGF0YXNldChkYXRhLCB0aW1lX3N0ZXA9MSk6CiAgICBYLCB5ID0gW10sIFtdCiAgICBmb3IgaSBpbiByYW5nZShsZW4oZGF0YSkgLSB0aW1lX3N0ZXAgLSAxKToKICAgICAgICBYLmFwcGVuZChkYXRhW2k6KGkgKyB0aW1lX3N0ZXApXSkKICAgICAgICB5LmFwcGVuZChkYXRhW2kgKyB0aW1lX3N0ZXBdKQogICAgcmV0dXJuIG5wLmFycmF5KFgpLCBucC5hcnJheSh5KQoKdGltZV9zdGVwID0gMTAKWCwgeSA9IGNyZWF0ZV9kYXRhc2V0KGRhdGEsIHRpbWVfc3RlcCkKCiMgUmVzaGFwZSBkYXRhIHRvIFtzYW1wbGVzLCB0aW1lIHN0ZXBzLCBmZWF0dXJlc10KWCA9IFgucmVzaGFwZShYLnNoYXBlWzBdLCBYLnNoYXBlWzFdLCAxKQoKIyBCdWlsZCBMU1RNIG1vZGVsCm1vZGVsID0gU2VxdWVudGlhbCgpCm1vZGVsLmFkZChMU1RNKDUwLCBhY3RpdmF0aW9uPSdyZWx1JywgaW5wdXRfc2hhcGU9KHRpbWVfc3RlcCwgMSkpKQptb2RlbC5hZGQoRGVuc2UoMSkpCm1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywgbG9zcz0nbXNlJykKCiMgVHJhaW4gdGhlIG1vZGVsCm1vZGVsLmZpdChYLCB5LCBlcG9jaHM9MTAwLCB2ZXJib3NlPTApCgojIE1ha2UgcHJlZGljdGlvbnMKcHJlZGljdGlvbnMgPSBtb2RlbC5wcmVkaWN0KFgpCnByaW50KHByZWRpY3Rpb25zKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Generate sine wave data
time_steps = np.linspace(0, 100, 1000)
data = np.sin(time_steps)

# Prepare the dataset
def create_dataset(data, time_step=1):
    X, y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step)])
        y.append(data[i + time_step])
    return np.array(X), np.array(y)

time_step = 10
X, y = create_dataset(data, time_step)

# Reshape data to [samples, time steps, features]
X = X.reshape(X.shape[0], X.shape[1], 1)

# Build LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(time_step, 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X, y, epochs=100, verbose=0)

# Make predictions
predictions = model.predict(X)
print(predictions)</pre></div><div class='content'></div><h2><p>Exercise 2: Implementing GRU for Text Generation</p>
</h2>
<div class='content'><p><strong>Task</strong>: Use GRU to generate text based on a given input sequence.</p>
<p><strong>Dataset</strong>: Use a simple text dataset.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Embedding

# Sample text data
text = &quot;hello world&quot;
chars = sorted(list(set(text)))
char_to_index = {c: i for i, c in enumerate(chars)}
index_to_char = {i: c for i, c in enumerate(chars)}

# Prepare the dataset
def create_dataset(text, char_to_index, time_step=1):
    X, y = [], []
    for i in range(len(text) - time_step):
        X.append([char_to_index[char] for char in text[i:i + time_step]])
        y.append(char_to_index[text[i + time_step]])
    return np.array(X), np.array(y)

time_step = 3
X, y = create_dataset(text, char_to_index, time_step)

# Reshape data to [samples, time steps, features]
X = X.reshape(X.shape[0], X.shape[1])

# Build GRU model
model = Sequential()
model.add(Embedding(len(chars), 10, input_length=time_step))
model.add(GRU(50, activation='relu'))
model.add(Dense(len(chars), activation='softmax'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')

# Train the model
model.fit(X, y, epochs=100, verbose=0)

# Generate text
def generate_text(model, start_text, char_to_index, index_to_char, length=10):
    for _ in range(length):
        x = np.array([char_to_index[char] for char in start_text[-time_step:]]).reshape(1, time_step)
        prediction = model.predict(x, verbose=0)
        next_char = index_to_char[np.argmax(prediction)]
        start_text += next_char
    return start_text

generated_text = generate_text(model, &quot;hel&quot;, char_to_index, index_to_char)
print(generated_text)</pre></div><div class='content'></div><h1><ol start="5">
<li>Summary</li>
</ol>
</h1>
<div class='content'><p>In this section, we explored LSTM and GRU networks, which are advanced types of RNNs designed to handle long-term dependencies and mitigate the vanishing gradient problem. We discussed their key components, cell structures, and equations. Additionally, we provided practical examples and exercises to implement LSTM and GRU using TensorFlow.</p>
<p>In the next section, we will explore the applications of RNNs in Natural Language Processing (NLP).</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='04-01-introduction-rnn' title="Introduction to RNN">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='04-03-rnn-applications-nlp' title="RNN Applications in Natural Language Processing">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
