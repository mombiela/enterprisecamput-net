<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Perceptron and Multilayer Perceptron</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/perceptron-multicapa" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/perceptron-multicapa" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/perceptron-multicapa" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/perceptron-multicapa" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/perceptron-multicapa" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introduction</h1>
<div class='content'><p>In this section of the Deep Learning course, we will explore the fundamental concepts of the Perceptron and the Multilayer Perceptron (MLP). These are the basic building blocks of neural networks and are essential for understanding more complex models in Deep Learning.</p>
</div><h1>Perceptron</h1>
<div class='content'><p>The Perceptron is the simplest model of a neural network and was introduced by Frank Rosenblatt in 1958. It is a linear classifier that takes an input, performs a weighted sum, and applies an activation function to produce an output.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Neurons</strong>: Basic processing units in a neural network.</li>
<li><strong>Weights</strong>: Coefficients that multiply the inputs.</li>
<li><strong>Bias</strong>: An additional term that helps adjust the model's output.</li>
<li><strong>Activation Function</strong>: Function that decides whether a neuron should activate or not.</li>
</ul>
</div><h2>Perceptron Structure</h2>
<div class='content'><p>The Perceptron can be mathematically represented as:</p>
<p>\[ y = f(\sum_{i=1}^{n} w_i x_i + b) \]</p>
<p>Where:</p>
<ul>
<li>\( y \) is the output.</li>
<li>\( w_i \) are the weights.</li>
<li>\( x_i \) are the inputs.</li>
<li>\( b \) is the bias.</li>
<li>\( f \) is the activation function (e.g., the step function).</li>
</ul>
</div><h2>Code Example</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEFjdGl2YXRpb24gZnVuY3Rpb24gKHN0ZXApCmRlZiBzdGVwX2Z1bmN0aW9uKHgpOgogICAgcmV0dXJuIDEgaWYgeCA+PSAwIGVsc2UgMAoKIyBTaW1wbGUgUGVyY2VwdHJvbgpjbGFzcyBQZXJjZXB0cm9uOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGxlYXJuaW5nX3JhdGU9MC4xKToKICAgICAgICBzZWxmLndlaWdodHMgPSBucC56ZXJvcyhpbnB1dF9zaXplICsgMSkgICMgKzEgZm9yIHRoZSBiaWFzCiAgICAgICAgc2VsZi5sZWFybmluZ19yYXRlID0gbGVhcm5pbmdfcmF0ZQoKICAgIGRlZiBwcmVkaWN0KHNlbGYsIHgpOgogICAgICAgIHggPSBucC5pbnNlcnQoeCwgMCwgMSkgICMgSW5zZXJ0IHRoZSBiaWFzCiAgICAgICAgc3VtbWF0aW9uID0gbnAuZG90KHNlbGYud2VpZ2h0cywgeCkKICAgICAgICByZXR1cm4gc3RlcF9mdW5jdGlvbihzdW1tYXRpb24pCgogICAgZGVmIHRyYWluKHNlbGYsIHRyYWluaW5nX2RhdGEsIGxhYmVscywgZXBvY2hzPTEwKToKICAgICAgICBmb3IgXyBpbiByYW5nZShlcG9jaHMpOgogICAgICAgICAgICBmb3IgeCwgbGFiZWwgaW4gemlwKHRyYWluaW5nX2RhdGEsIGxhYmVscyk6CiAgICAgICAgICAgICAgICBwcmVkaWN0aW9uID0gc2VsZi5wcmVkaWN0KHgpCiAgICAgICAgICAgICAgICBzZWxmLndlaWdodHMgKz0gc2VsZi5sZWFybmluZ19yYXRlICogKGxhYmVsIC0gcHJlZGljdGlvbikgKiBucC5pbnNlcnQoeCwgMCwgMSkKCiMgVHJhaW5pbmcgZGF0YSAobG9naWNhbCBBTkQpCnRyYWluaW5nX2RhdGEgPSBucC5hcnJheShbWzAsIDBdLCBbMCwgMV0sIFsxLCAwXSwgWzEsIDFdXSkKbGFiZWxzID0gbnAuYXJyYXkoWzAsIDAsIDAsIDFdKQoKIyBDcmVhdGUgYW5kIHRyYWluIHRoZSBwZXJjZXB0cm9uCnBlcmNlcHRyb24gPSBQZXJjZXB0cm9uKGlucHV0X3NpemU9MikKcGVyY2VwdHJvbi50cmFpbih0cmFpbmluZ19kYXRhLCBsYWJlbHMpCgojIFRlc3QgdGhlIHBlcmNlcHRyb24KZm9yIHggaW4gdHJhaW5pbmdfZGF0YToKICAgIHByaW50KGYiSW5wdXQ6IHt4fSwgUHJlZGljdGlvbjoge3BlcmNlcHRyb24ucHJlZGljdCh4KX0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation function (step)
def step_function(x):
    return 1 if x &gt;= 0 else 0

# Simple Perceptron
class Perceptron:
    def __init__(self, input_size, learning_rate=0.1):
        self.weights = np.zeros(input_size + 1)  # +1 for the bias
        self.learning_rate = learning_rate

    def predict(self, x):
        x = np.insert(x, 0, 1)  # Insert the bias
        summation = np.dot(self.weights, x)
        return step_function(summation)

    def train(self, training_data, labels, epochs=10):
        for _ in range(epochs):
            for x, label in zip(training_data, labels):
                prediction = self.predict(x)
                self.weights += self.learning_rate * (label - prediction) * np.insert(x, 0, 1)

# Training data (logical AND)
training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 0, 0, 1])

# Create and train the perceptron
perceptron = Perceptron(input_size=2)
perceptron.train(training_data, labels)

# Test the perceptron
for x in training_data:
    print(f&quot;Input: {x}, Prediction: {perceptron.predict(x)}&quot;)</pre></div><div class='content'></div><h1>Multilayer Perceptron (MLP)</h1>
<div class='content'><p>The Multilayer Perceptron (MLP) is an extension of the simple Perceptron and can solve non-linear problems. It consists of multiple layers of neurons: an input layer, one or more hidden layers, and an output layer.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Hidden Layers</strong>: Intermediate layers between the input and output that allow modeling of non-linear relationships.</li>
<li><strong>Backpropagation</strong>: Algorithm for training neural networks by adjusting weights through error gradient calculation.</li>
</ul>
</div><h2>MLP Structure</h2>
<div class='content'><p>An MLP with one hidden layer can be represented as:</p>
<p>\[ y = f_2(\sum_{j=1}^{m} w_{2j} f_1(\sum_{i=1}^{n} w_{1ij} x_i + b_{1j}) + b_2) \]</p>
<p>Where:</p>
<ul>
<li>\( f_1 \) and \( f_2 \) are activation functions for the hidden layer and the output layer, respectively.</li>
<li>\( w_{1ij} \) and \( w_{2j} \) are the weights of the first and second layer.</li>
<li>\( b_{1j} \) and \( b_2 \) are the biases of the first and second layer.</li>
</ul>
</div><h2>Code Example</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation function (sigmoid)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of the sigmoid function
def sigmoid_derivative(x):
    return x * (1 - x)

# Multilayer Perceptron
class MLP:
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.learning_rate = learning_rate

    def forward(self, x):
        self.hidden_input = np.dot(x, self.weights_input_hidden)
        self.hidden_output = sigmoid(self.hidden_input)
        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output)
        self.final_output = sigmoid(self.final_input)
        return self.final_output

    def backward(self, x, y, output):
        error = y - output
        d_output = error * sigmoid_derivative(output)
        error_hidden_layer = d_output.dot(self.weights_hidden_output.T)
        d_hidden_layer = error_hidden_layer * sigmoid_derivative(self.hidden_output)

        self.weights_hidden_output += self.hidden_output.T.dot(d_output) * self.learning_rate
        self.weights_input_hidden += x.T.dot(d_hidden_layer) * self.learning_rate

    def train(self, training_data, labels, epochs=10000):
        for _ in range(epochs):
            for x, y in zip(training_data, labels):
                output = self.forward(x)
                self.backward(x, y, output)

# Training data (logical XOR)
training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([[0], [1], [1], [0]])

# Create and train the MLP
mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_data, labels)

# Test the MLP
for x in training_data:
    print(f&quot;Input: {x}, Prediction: {mlp.forward(x)}&quot;)</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>In this section, we have covered the basic concepts of the Perceptron and the Multilayer Perceptron. The Perceptron is a simple linear classifier, while the MLP can handle non-linear problems by including hidden layers and using the backpropagation algorithm. These models are fundamental for understanding more complex neural networks and are the basis of many advanced Deep Learning algorithms.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
