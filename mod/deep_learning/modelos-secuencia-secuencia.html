<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sequence to Sequence Models</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/modelos-secuencia-secuencia" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/modelos-secuencia-secuencia" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/modelos-secuencia-secuencia" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body  class="test" >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/modelos-secuencia-secuencia" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/modelos-secuencia-secuencia" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
									<div class="assert">
						<p><b>Attention!</b> There has been an error in the course generation, and it may contain translation errors.We are working to resolve this issue, so please use the content with caution.You can check the correct content in another language at the following link:<br>
						<a href="https://campusempresa.com/mod/deep_learning/modelos-secuencia-secuencia">https://campusempresa.com/mod/deep_learning/modelos-secuencia-secuencia</a></p>
					</div>
								<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Sequence to sequence (Seq2Seq) models are a class of deep learning models designed to transform a sequence of elements into another sequence. These models are widely used in tasks such as machine translation, text summarization, and text generation.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ul>
<li><strong>Encoder</strong>: Processes the input sequence and converts it into a state representation.</li>
<li><strong>Decoder</strong>: Takes the state representation from the encoder and generates the output sequence.</li>
<li><strong>Attention</strong>: Mechanism that allows the decoder to focus on different parts of the input sequence at each step of generating the output sequence.</li>
<li><strong>Embedding</strong>: Dense, low-dimensional representation of words or tokens.</li>
<li><strong>RNN, LSTM, GRU</strong>: Types of recurrent neural networks commonly used in Seq2Seq models.</li>
</ul>
</div><h1>Architecture of a Seq2Seq Model</h1>
<div class='content'></div><h2>Encoder</h2>
<div class='content'><p>The encoder takes an input sequence and processes it through a recurrent neural network (RNN), Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU) to produce a state representation.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgTFNUTSwgRW1iZWRkaW5nLCBEZW5zZQoKIyBFbmNvZGVyIGRlZmluaXRpb24KZW5jb2Rlcl9pbnB1dHMgPSB0Zi5rZXJhcy5JbnB1dChzaGFwZT0oTm9uZSwpKQp4ID0gRW1iZWRkaW5nKGlucHV0X2RpbT12b2NhYl9zaXplLCBvdXRwdXRfZGltPWVtYmVkZGluZ19kaW0pKGVuY29kZXJfaW5wdXRzKQplbmNvZGVyX291dHB1dHMsIHN0YXRlX2gsIHN0YXRlX2MgPSBMU1RNKHVuaXRzPWxhdGVudF9kaW0sIHJldHVybl9zdGF0ZT1UcnVlKSh4KQplbmNvZGVyX3N0YXRlcyA9IFtzdGF0ZV9oLCBzdGF0ZV9jXQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.layers import LSTM, Embedding, Dense

# Encoder definition
encoder_inputs = tf.keras.Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]</pre></div><div class='content'></div><h2>Decoder</h2>
<div class='content'><p>The decoder uses the state representation from the encoder to generate the output sequence. At each step, the decoder predicts the next token in the output sequence.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWNvZGVyIGRlZmluaXRpb24KZGVjb2Rlcl9pbnB1dHMgPSB0Zi5rZXJhcy5JbnB1dChzaGFwZT0oTm9uZSwpKQp4ID0gRW1iZWRkaW5nKGlucHV0X2RpbT12b2NhYl9zaXplLCBvdXRwdXRfZGltPWVtYmVkZGluZ19kaW0pKGRlY29kZXJfaW5wdXRzKQp4ID0gTFNUTSh1bml0cz1sYXRlbnRfZGltLCByZXR1cm5fc2VxdWVuY2VzPVRydWUpKHgsIGluaXRpYWxfc3RhdGU9ZW5jb2Rlcl9zdGF0ZXMpCmRlY29kZXJfb3V0cHV0cyA9IERlbnNlKHVuaXRzPXZvY2FiX3NpemUsIGFjdGl2YXRpb249J3NvZnRtYXgnKSh4KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Decoder definition
decoder_inputs = tf.keras.Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
x = LSTM(units=latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(units=vocab_size, activation='softmax')(x)</pre></div><div class='content'></div><h2>Attention Mechanism</h2>
<div class='content'><p>The attention mechanism allows the decoder to focus on different parts of the input sequence at each step of generating the output sequence.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgQXR0ZW50aW9uCgojIEF0dGVudGlvbiBtZWNoYW5pc20gZGVmaW5pdGlvbgphdHRlbnRpb24gPSBBdHRlbnRpb24oKQpjb250ZXh0X3ZlY3RvciwgYXR0ZW50aW9uX3dlaWdodHMgPSBhdHRlbnRpb24oW2VuY29kZXJfb3V0cHV0cywgZGVjb2Rlcl9pbnB1dHNdKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from tensorflow.keras.layers import Attention

# Attention mechanism definition
attention = Attention()
context_vector, attention_weights = attention([encoder_outputs, decoder_inputs])</pre></div><div class='content'></div><h1>Comparison of RNN, LSTM, and GRU</h1>
<div class='content'><p>| Feature        | RNN | LSTM | GRU |
|----------------|-----|------|-----|
| Structure      | Simple | Complex (with memory cells) | Intermediate |
| Performance    | Lower | High | High |
| Speed          | Fast | Slow | Intermediate |
| Memory Usage   | Low | High | Intermediate |</p>
</div><h1>Complete Example</h1>
<div class='content'><p>Below is a complete example of a Seq2Seq model with an encoder and a decoder using LSTM.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmxheWVycyBpbXBvcnQgSW5wdXQsIExTVE0sIEVtYmVkZGluZywgRGVuc2UKCiMgUGFyYW1ldGVycwp2b2NhYl9zaXplID0gMTAwMDAKZW1iZWRkaW5nX2RpbSA9IDI1NgpsYXRlbnRfZGltID0gNTEyCgojIEVuY29kZXIKZW5jb2Rlcl9pbnB1dHMgPSBJbnB1dChzaGFwZT0oTm9uZSwpKQp4ID0gRW1iZWRkaW5nKGlucHV0X2RpbT12b2NhYl9zaXplLCBvdXRwdXRfZGltPWVtYmVkZGluZ19kaW0pKGVuY29kZXJfaW5wdXRzKQplbmNvZGVyX291dHB1dHMsIHN0YXRlX2gsIHN0YXRlX2MgPSBMU1RNKHVuaXRzPWxhdGVudF9kaW0sIHJldHVybl9zdGF0ZT1UcnVlKSh4KQplbmNvZGVyX3N0YXRlcyA9IFtzdGF0ZV9oLCBzdGF0ZV9jXQoKIyBEZWNvZGVyCmRlY29kZXJfaW5wdXRzID0gSW5wdXQoc2hhcGU9KE5vbmUsKSkKeCA9IEVtYmVkZGluZyhpbnB1dF9kaW09dm9jYWJfc2l6ZSwgb3V0cHV0X2RpbT1lbWJlZGRpbmdfZGltKShkZWNvZGVyX2lucHV0cykKeCA9IExTVE0odW5pdHM9bGF0ZW50X2RpbSwgcmV0dXJuX3NlcXVlbmNlcz1UcnVlKSh4LCBpbml0aWFsX3N0YXRlPWVuY29kZXJfc3RhdGVzKQpkZWNvZGVyX291dHB1dHMgPSBEZW5zZSh1bml0cz12b2NhYl9zaXplLCBhY3RpdmF0aW9uPSdzb2Z0bWF4JykoeCkKCiMgU2VxMlNlcSBNb2RlbAptb2RlbCA9IHRmLmtlcmFzLk1vZGVsKFtlbmNvZGVyX2lucHV0cywgZGVjb2Rlcl9pbnB1dHNdLCBkZWNvZGVyX291dHB1dHMpCm1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywgbG9zcz0nc3BhcnNlX2NhdGVnb3JpY2FsX2Nyb3NzZW50cm9weScp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense

# Parameters
vocab_size = 10000
embedding_dim = 256
latent_dim = 512

# Encoder
encoder_inputs = Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(encoder_inputs)
encoder_outputs, state_h, state_c = LSTM(units=latent_dim, return_state=True)(x)
encoder_states = [state_h, state_c]

# Decoder
decoder_inputs = Input(shape=(None,))
x = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)
x = LSTM(units=latent_dim, return_sequences=True)(x, initial_state=encoder_states)
decoder_outputs = Dense(units=vocab_size, activation='softmax')(x)

# Seq2Seq Model
model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Sequence to sequence models are powerful tools in the field of deep learning, especially useful for tasks that require transforming one sequence of data into another. Understanding the architecture and key components of these models, such as the encoder, decoder, and attention mechanism, is fundamental for their implementation and optimization in various applications.</p>
</div>
			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
