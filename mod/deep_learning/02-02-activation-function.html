<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activation Function</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-02-funcion-de-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-02-activation-function" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/02-02-funcion-de-activacion" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/02-02-funcion-de-activacion" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Activation Function</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Activation functions play a crucial role in the functioning of neural networks. They introduce non-linearity into the network, enabling it to learn complex patterns and relationships in the data. In this section, we will explore different types of activation functions, their properties, and their applications.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ol>
<li><strong>Definition</strong>: An activation function is a mathematical function applied to the output of a neuron. It determines whether a neuron should be activated or not, based on the weighted sum of its inputs.</li>
<li><strong>Purpose</strong>: The primary purpose of an activation function is to introduce non-linearity into the neural network, allowing it to learn and model complex data.</li>
<li><strong>Types of Activation Functions</strong>: There are several types of activation functions, each with its own advantages and disadvantages.</li>
</ol>
</div><h1>Common Activation Functions</h1>
<div class='content'></div><h2>1. Sigmoid Function</h2>
<div class='content'><p>The sigmoid function maps any input value to a value between 0 and 1.</p>
<p><strong>Formula</strong>:
\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (0, 1)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>: \(\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))\)</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgp4ID0gbnAubGluc3BhY2UoLTEwLCAxMCwgMTAwKQp5ID0gc2lnbW9pZCh4KQoKcGx0LnBsb3QoeCwgeSkKcGx0LnRpdGxlKCdTaWdtb2lkIEZ1bmN0aW9uJykKcGx0LnhsYWJlbCgnSW5wdXQnKQpwbHQueWxhYmVsKCdPdXRwdXQnKQpwbHQuZ3JpZCgpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title('Sigmoid Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h2>2. Hyperbolic Tangent (Tanh) Function</h2>
<div class='content'><p>The tanh function maps any input value to a value between -1 and 1.</p>
<p><strong>Formula</strong>:
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (-1, 1)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>: \(\tanh'(x) = 1 - \tanh^2(x)\)</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKeSA9IHRhbmgoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnVGFuaCBGdW5jdGlvbicpCnBsdC54bGFiZWwoJ0lucHV0JykKcGx0LnlsYWJlbCgnT3V0cHV0JykKcGx0LmdyaWQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

x = np.linspace(-10, 10, 100)
y = tanh(x)

plt.plot(x, y)
plt.title('Tanh Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h2>3. Rectified Linear Unit (ReLU) Function</h2>
<div class='content'><p>The ReLU function is one of the most popular activation functions in deep learning.</p>
<p><strong>Formula</strong>:
\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: [0, ∞)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>:
\[
\text{ReLU}'(x) =
\begin{cases}
1 &amp; \text{if } x &gt; 0 <br>  0 &amp; \text{if } x \leq 0
\end{cases}
\]</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKeSA9IHJlbHUoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnUmVMVSBGdW5jdGlvbicpCnBsdC54bGFiZWwoJ0lucHV0JykKcGx0LnlsYWJlbCgnT3V0cHV0JykKcGx0LmdyaWQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

x = np.linspace(-10, 10, 100)
y = relu(x)

plt.plot(x, y)
plt.title('ReLU Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h2>4. Leaky ReLU Function</h2>
<div class='content'><p>The Leaky ReLU function is a variation of the ReLU function that allows a small, non-zero gradient when the input is negative.</p>
<p><strong>Formula</strong>:
\[ \text{Leaky ReLU}(x) =
\begin{cases}
x &amp; \text{if } x &gt; 0 <br>\alpha x &amp; \text{if } x \leq 0
\end{cases}
\]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (-∞, ∞)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>:
\[
\text{Leaky ReLU}'(x) =
\begin{cases}
1 &amp; \text{if } x &gt; 0 <br>  \alpha &amp; \text{if } x \leq 0
\end{cases}
\]</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCnggPSBucC5saW5zcGFjZSgtMTAsIDEwLCAxMDApCnkgPSBsZWFreV9yZWx1KHgpCgpwbHQucGxvdCh4LCB5KQpwbHQudGl0bGUoJ0xlYWt5IFJlTFUgRnVuY3Rpb24nKQpwbHQueGxhYmVsKCdJbnB1dCcpCnBsdC55bGFiZWwoJ091dHB1dCcpCnBsdC5ncmlkKCkKcGx0LnNob3coKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

x = np.linspace(-10, 10, 100)
y = leaky_relu(x)

plt.plot(x, y)
plt.title('Leaky ReLU Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h2>5. Softmax Function</h2>
<div class='content'><p>The softmax function is often used in the output layer of a neural network for classification tasks.</p>
<p><strong>Formula</strong>:
\[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (0, 1)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>: Complex, but ensures the sum of outputs is 1</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCnggPSBucC5hcnJheShbMS4wLCAyLjAsIDMuMF0pCnkgPSBzb2Z0bWF4KHgpCgpwcmludCgiU29mdG1heCBPdXRwdXQ6IiwgeSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

x = np.array([1.0, 2.0, 3.0])
y = softmax(x)

print(&quot;Softmax Output:&quot;, y)</pre></div><div class='content'></div><h1>Practical Exercises</h1>
<div class='content'></div><h2>Exercise 1: Implementing Activation Functions</h2>
<div class='content'><p><strong>Task</strong>: Implement the sigmoid, tanh, ReLU, and softmax functions in Python.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCiMgVGVzdCB0aGUgZnVuY3Rpb25zCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludCgiU2lnbW9pZDoiLCBzaWdtb2lkKHgpKQpwcmludCgiVGFuaDoiLCB0YW5oKHgpKQpwcmludCgiUmVMVToiLCByZWx1KHgpKQpwcmludCgiU29mdG1heDoiLCBzb2Z0bWF4KHgpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# Test the functions
x = np.array([-1.0, 0.0, 1.0])
print(&quot;Sigmoid:&quot;, sigmoid(x))
print(&quot;Tanh:&quot;, tanh(x))
print(&quot;ReLU:&quot;, relu(x))
print(&quot;Softmax:&quot;, softmax(x))</pre></div><div class='content'></div><h2>Exercise 2: Visualizing Activation Functions</h2>
<div class='content'><p><strong>Task</strong>: Plot the sigmoid, tanh, ReLU, and leaky ReLU functions using Matplotlib.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCnggPSBucC5saW5zcGFjZSgtMTAsIDEwLCAxMDApCgojIFBsb3QgU2lnbW9pZApwbHQucGxvdCh4LCBzaWdtb2lkKHgpLCBsYWJlbD0nU2lnbW9pZCcpCiMgUGxvdCBUYW5oCnBsdC5wbG90KHgsIHRhbmgoeCksIGxhYmVsPSdUYW5oJykKIyBQbG90IFJlTFUKcGx0LnBsb3QoeCwgcmVsdSh4KSwgbGFiZWw9J1JlTFUnKQojIFBsb3QgTGVha3kgUmVMVQpwbHQucGxvdCh4LCBsZWFreV9yZWx1KHgpLCBsYWJlbD0nTGVha3kgUmVMVScpCgpwbHQudGl0bGUoJ0FjdGl2YXRpb24gRnVuY3Rpb25zJykKcGx0LnhsYWJlbCgnSW5wdXQnKQpwbHQueWxhYmVsKCdPdXRwdXQnKQpwbHQubGVnZW5kKCkKcGx0LmdyaWQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import matplotlib.pyplot as plt

def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

x = np.linspace(-10, 10, 100)

# Plot Sigmoid
plt.plot(x, sigmoid(x), label='Sigmoid')
# Plot Tanh
plt.plot(x, tanh(x), label='Tanh')
# Plot ReLU
plt.plot(x, relu(x), label='ReLU')
# Plot Leaky ReLU
plt.plot(x, leaky_relu(x), label='Leaky ReLU')

plt.title('Activation Functions')
plt.xlabel('Input')
plt.ylabel('Output')
plt.legend()
plt.grid()
plt.show()</pre></div><div class='content'></div><h1>Common Mistakes and Tips</h1>
<div class='content'><ul>
<li><strong>Vanishing Gradient Problem</strong>: Sigmoid and tanh functions can cause the vanishing gradient problem, where gradients become very small, slowing down the training process. ReLU and its variants are often preferred to mitigate this issue.</li>
<li><strong>Choosing the Right Activation Function</strong>: The choice of activation function can significantly impact the performance of your neural network. Experiment with different functions to find the best one for your specific problem.</li>
<li><strong>Softmax for Classification</strong>: Use the softmax function in the output layer for multi-class classification problems to ensure the outputs sum to 1.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we explored various activation functions, their properties, and their applications. Understanding these functions is crucial for designing effective neural networks. In the next section, we will delve into forward and backward propagation, which are essential for training neural networks.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
