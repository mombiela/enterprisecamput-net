<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activation Functions</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/funciones-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/funciones-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/funciones-activacion" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/funciones-activacion" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/funciones-activacion" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introduction</h1>
<div class='content'><p>In the context of Deep Learning, activation functions play a crucial role in the ability of neural networks to learn and model complex data. These functions determine whether a neuron should be activated or not, introducing nonlinearities that allow the neural network to learn complex relationships in the data.</p>
</div><h1>Types of Activation Functions</h1>
<div class='content'><p>There are several activation functions commonly used in neural networks. Below are some of the most important ones:</p>
</div><h2>Sigmoid Function</h2>
<div class='content'><p>The sigmoid function is one of the oldest activation functions and is defined as:</p>
<p>\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<ul>
<li><strong>Output Range:</strong> (0, 1)</li>
<li><strong>Advantages:</strong>
<ul>
<li>Smooth and differentiable.</li>
<li>Output in a limited range (0 to 1), useful for probabilities.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Gradient vanishing problem.</li>
<li>Outputs not centered at zero.</li>
</ul>
</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKIyBVc2FnZSBleGFtcGxlCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChzaWdtb2lkKHgpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Usage example
x = np.array([-1.0, 0.0, 1.0])
print(sigmoid(x))</pre></div><div class='content'></div><h2>Tanh Function</h2>
<div class='content'><p>The hyperbolic tangent (tanh) function is similar to the sigmoid but its outputs are centered at zero:</p>
<p>\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<ul>
<li><strong>Output Range:</strong> (-1, 1)</li>
<li><strong>Advantages:</strong>
<ul>
<li>Outputs centered at zero.</li>
<li>Smooth and differentiable.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Can still suffer from the gradient vanishing problem.</li>
</ul>
</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKIyBVc2FnZSBleGFtcGxlCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludCh0YW5oKHgpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

# Usage example
x = np.array([-1.0, 0.0, 1.0])
print(tanh(x))</pre></div><div class='content'></div><h2>ReLU (Rectified Linear Unit) Function</h2>
<div class='content'><p>The ReLU function is currently one of the most popular activation functions:</p>
<p>\[ \text{ReLU}(x) = \max(0, x) \]</p>
<ul>
<li><strong>Output Range:</strong> [0, ∞)</li>
<li><strong>Advantages:</strong>
<ul>
<li>Simple and efficient.</li>
<li>Mitigates the gradient vanishing problem.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Dead neurons problem (when negative values become zero).</li>
</ul>
</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKIyBVc2FnZSBleGFtcGxlCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludChyZWx1KHgpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

# Usage example
x = np.array([-1.0, 0.0, 1.0])
print(relu(x))</pre></div><div class='content'></div><h2>Leaky ReLU Function</h2>
<div class='content'><p>The Leaky ReLU function is a variant of ReLU that attempts to solve the dead neurons problem:</p>
<p>\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{if } x \geq 0 <br>\alpha x &amp; \text{if } x &lt; 0
\end{cases} \]</p>
<p>where \( \alpha \) is a small positive value.</p>
<ul>
<li><strong>Output Range:</strong> (-∞, ∞)</li>
<li><strong>Advantages:</strong>
<ul>
<li>Mitigates the dead neurons problem.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Introduces a small bias in negative outputs.</li>
</ul>
</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCiMgVXNhZ2UgZXhhbXBsZQp4ID0gbnAuYXJyYXkoWy0xLjAsIDAuMCwgMS4wXSkKcHJpbnQobGVha3lfcmVsdSh4KSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

# Usage example
x = np.array([-1.0, 0.0, 1.0])
print(leaky_relu(x))</pre></div><div class='content'></div><h1>Comparison of Activation Functions</h1>
<div class='content'><p>| Function      | Output Range    | Advantages                              | Disadvantages                          |
|---------------|-----------------|-----------------------------------------|----------------------------------------|
| Sigmoid       | (0, 1)          | Smooth, differentiable, useful for probabilities | Gradient vanishing, not centered at zero |
| Tanh          | (-1, 1)         | Outputs centered at zero, smooth        | Gradient vanishing                      |
| ReLU          | [0, ∞)          | Simple, efficient, mitigates gradient vanishing | Dead neurons                            |
| Leaky ReLU    | (-∞, ∞)         | Mitigates dead neurons                  | Introduces bias in negative outputs     |</p>
</div><h1>Conclusion</h1>
<div class='content'><p>Activation functions are essential components in neural networks as they allow the introduction of nonlinearities and, therefore, the ability to model complex relationships in the data. The choice of the appropriate activation function can have a significant impact on the performance and efficiency of the model. It is important to understand the characteristics, advantages, and disadvantages of each function to make informed decisions in the design of neural networks.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
