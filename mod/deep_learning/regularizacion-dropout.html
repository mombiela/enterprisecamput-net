<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regularization and Dropout</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/regularizacion-dropout" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/regularizacion-dropout" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/regularizacion-dropout" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body  class="test" >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/regularizacion-dropout" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/regularizacion-dropout" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
									<div class="assert">
						<p><b>Attention!</b> There has been an error in the course generation, and it may contain translation errors.We are working to resolve this issue, so please use the content with caution.You can check the correct content in another language at the following link:<br>
						<a href="https://campusempresa.com/mod/deep_learning/regularizacion-dropout">https://campusempresa.com/mod/deep_learning/regularizacion-dropout</a></p>
					</div>
								<div class='content'><p>In this topic, we will explore two essential techniques in Deep Learning to prevent overfitting: regularization and dropout. These techniques are fundamental for improving the generalization capacity of neural network models.</p>
</div><h1>Regularization</h1>
<div class='content'><p>Regularization is a technique used to reduce overfitting in machine learning models. Overfitting occurs when a model fits too closely to the training data and does not generalize well to new data. There are several forms of regularization, but the most common are L1 and L2.</p>
</div><h2>L1 Regularization (Lasso)</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: L1 regularization adds a penalty equal to the sum of the absolute values of the model coefficients.</li>
<li><strong>Formula</strong>:
\[
\text{Loss} = \text{Original Loss} + \lambda \sum_{i} |w_i|
\]</li>
<li><strong>Advantages</strong>:
<ul>
<li>Can lead to sparse solutions, meaning some coefficients become exactly zero.</li>
</ul>
</li>
<li><strong>Code Example</strong>:
<pre><code class="language-python">from keras.regularizers import l1
model.add(Dense(64, input_dim=64, kernel_regularizer=l1(0.01)))
</code></pre>
</li>
</ul>
</div><h2>L2 Regularization (Ridge)</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: L2 regularization adds a penalty equal to the sum of the squares of the model coefficients.</li>
<li><strong>Formula</strong>:
\[
\text{Loss} = \text{Original Loss} + \lambda \sum_{i} w_i^2
\]</li>
<li><strong>Advantages</strong>:
<ul>
<li>Does not lead to sparse solutions but can reduce the magnitude of the coefficients.</li>
</ul>
</li>
<li><strong>Code Example</strong>:
<pre><code class="language-python">from keras.regularizers import l2
model.add(Dense(64, input_dim=64, kernel_regularizer=l2(0.01)))
</code></pre>
</li>
</ul>
</div><h2>Comparison between L1 and L2</h2>
<div class='content'><p>| Characteristic | L1 (Lasso) | L2 (Ridge) |
|----------------|------------|------------|
| Penalty        | Sum of absolute values | Sum of squares |
| Sparse solutions | Yes | No |
| Common use     | Feature selection | Reduction of coefficient magnitude |</p>
</div><h1>Dropout</h1>
<div class='content'><p>Dropout is a regularization technique that involves randomly &quot;turning off&quot; a set of neurons during training. This helps prevent overfitting by reducing the model's dependency on specific neurons.</p>
</div><h2>How Dropout Works</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: During each training step, each neuron has a probability \( p \) of being &quot;turned off&quot; (i.e., its output is set to zero).</li>
<li><strong>Advantages</strong>:
<ul>
<li>Reduces overfitting.</li>
<li>Improves the model's generalization capacity.</li>
</ul>
</li>
<li><strong>Code Example</strong>:
<pre><code class="language-python">from keras.layers import Dropout
model.add(Dense(64, input_dim=64, activation='relu'))
model.add(Dropout(0.5))
</code></pre>
</li>
</ul>
</div><h2>Practical Example</h2>
<div class='content'><p>Below is a complete example of how to implement regularization and dropout in a neural network model using Keras:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBrZXJhcy5tb2RlbHMgaW1wb3J0IFNlcXVlbnRpYWwKZnJvbSBrZXJhcy5sYXllcnMgaW1wb3J0IERlbnNlLCBEcm9wb3V0CmZyb20ga2VyYXMucmVndWxhcml6ZXJzIGltcG9ydCBsMgoKIyBDcmVhdGUgdGhlIG1vZGVsCm1vZGVsID0gU2VxdWVudGlhbCgpCgojIEFkZCBsYXllcnMgd2l0aCBMMiByZWd1bGFyaXphdGlvbiBhbmQgZHJvcG91dAptb2RlbC5hZGQoRGVuc2UoNjQsIGlucHV0X2RpbT02NCwgYWN0aXZhdGlvbj0ncmVsdScsIGtlcm5lbF9yZWd1bGFyaXplcj1sMigwLjAxKSkpCm1vZGVsLmFkZChEcm9wb3V0KDAuNSkpCm1vZGVsLmFkZChEZW5zZSg2NCwgYWN0aXZhdGlvbj0ncmVsdScsIGtlcm5lbF9yZWd1bGFyaXplcj1sMigwLjAxKSkpCm1vZGVsLmFkZChEcm9wb3V0KDAuNSkpCm1vZGVsLmFkZChEZW5zZSgxMCwgYWN0aXZhdGlvbj0nc29mdG1heCcpKQoKIyBDb21waWxlIHRoZSBtb2RlbAptb2RlbC5jb21waWxlKGxvc3M9J2NhdGVnb3JpY2FsX2Nyb3NzZW50cm9weScsIG9wdGltaXplcj0nYWRhbScsIG1ldHJpY3M9WydhY2N1cmFjeSddKQoKIyBNb2RlbCBzdW1tYXJ5Cm1vZGVsLnN1bW1hcnkoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.regularizers import l2

# Create the model
model = Sequential()

# Add layers with L2 regularization and dropout
model.add(Dense(64, input_dim=64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Model summary
model.summary()</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Regularization and dropout are crucial techniques for improving the generalization capacity of neural network models and preventing overfitting. L1 and L2 regularization help control the magnitude of the model coefficients, while dropout reduces the dependency on specific neurons. By combining these techniques, we can build more robust and efficient models in Deep Learning.</p>
</div>
			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
