<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Perceptron and Multilayer Perceptron</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-01-perceptron-perceptron-multicapa" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-01-perceptron-perceptron-multicapa" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-01-perceptron-multilayer-perceptron" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=3" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/deep_learning/02-01-perceptron-perceptron-multicapa" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/deep_learning/02-01-perceptron-perceptron-multicapa" class="px-2">CA</a>
</span>
			<span class="d-none d-md-inline"><br><cite>Building today's and tomorrow's society</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Languages</a>
				 					<a href="/categ/frameworks">Frameworks</a>
				 					<a href="/categ/tech-tools">Tools</a>
				 					<a href="/categ/foundations">Foundations</a>
				 					<a href="/categ/soft-skills">Skills</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='01-04-basic-concepts-neural-networks' title="Basic Concepts of Neural Networks">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Perceptron and Multilayer Perceptron</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-02-activation-function' title="Activation Function">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introduction</p>
</h1>
<div class='content'><p>In this section, we will explore the foundational elements of neural networks: the perceptron and the multilayer perceptron (MLP). Understanding these concepts is crucial as they form the building blocks for more complex neural network architectures.</p>
</div><h1><p>Perceptron</p>
</h1>
<div class='content'></div><h2><p>What is a Perceptron?</p>
</h2>
<div class='content'><p>A perceptron is the simplest type of artificial neural network and serves as a linear binary classifier. It consists of a single neuron with adjustable weights and a bias term.</p>
</div><h2><p>Structure of a Perceptron</p>
</h2>
<div class='content'><ul>
<li><strong>Inputs (x1, x2, ..., xn):</strong> These are the features of the input data.</li>
<li><strong>Weights (w1, w2, ..., wn):</strong> Each input has an associated weight that adjusts during training.</li>
<li><strong>Bias (b):</strong> An additional parameter that helps the model fit the data better.</li>
<li><strong>Activation Function:</strong> Typically a step function that determines the output based on the weighted sum of inputs.</li>
</ul>
</div><h2><p>Mathematical Representation</p>
</h2>
<div class='content'><p>The output of a perceptron can be represented mathematically as:</p>
<p>\[ y = f\left(\sum_{i=1}^{n} w_i x_i + b\right) \]</p>
<p>Where:</p>
<ul>
<li>\( y \) is the output.</li>
<li>\( f \) is the activation function (e.g., step function).</li>
<li>\( w_i \) are the weights.</li>
<li>\( x_i \) are the inputs.</li>
<li>\( b \) is the bias.</li>
</ul>
</div><h2><p>Example Code</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIFN0ZXAgYWN0aXZhdGlvbiBmdW5jdGlvbgpkZWYgc3RlcF9mdW5jdGlvbih4KToKICAgIHJldHVybiAxIGlmIHggPj0gMCBlbHNlIDAKCiMgUGVyY2VwdHJvbiBjbGFzcwpjbGFzcyBQZXJjZXB0cm9uOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGxlYXJuaW5nX3JhdGU9MC4wMSk6CiAgICAgICAgc2VsZi53ZWlnaHRzID0gbnAuemVyb3MoaW5wdXRfc2l6ZSkKICAgICAgICBzZWxmLmJpYXMgPSAwCiAgICAgICAgc2VsZi5sZWFybmluZ19yYXRlID0gbGVhcm5pbmdfcmF0ZQoKICAgIGRlZiBwcmVkaWN0KHNlbGYsIGlucHV0cyk6CiAgICAgICAgdG90YWxfc3VtID0gbnAuZG90KGlucHV0cywgc2VsZi53ZWlnaHRzKSArIHNlbGYuYmlhcwogICAgICAgIHJldHVybiBzdGVwX2Z1bmN0aW9uKHRvdGFsX3N1bSkKCiAgICBkZWYgdHJhaW4oc2VsZiwgdHJhaW5pbmdfaW5wdXRzLCBsYWJlbHMsIGVwb2Nocyk6CiAgICAgICAgZm9yIF8gaW4gcmFuZ2UoZXBvY2hzKToKICAgICAgICAgICAgZm9yIGlucHV0cywgbGFiZWwgaW4gemlwKHRyYWluaW5nX2lucHV0cywgbGFiZWxzKToKICAgICAgICAgICAgICAgIHByZWRpY3Rpb24gPSBzZWxmLnByZWRpY3QoaW5wdXRzKQogICAgICAgICAgICAgICAgc2VsZi53ZWlnaHRzICs9IHNlbGYubGVhcm5pbmdfcmF0ZSAqIChsYWJlbCAtIHByZWRpY3Rpb24pICogaW5wdXRzCiAgICAgICAgICAgICAgICBzZWxmLmJpYXMgKz0gc2VsZi5sZWFybmluZ19yYXRlICogKGxhYmVsIC0gcHJlZGljdGlvbikKCiMgRXhhbXBsZSB1c2FnZQp0cmFpbmluZ19pbnB1dHMgPSBucC5hcnJheShbWzAsIDBdLCBbMCwgMV0sIFsxLCAwXSwgWzEsIDFdXSkKbGFiZWxzID0gbnAuYXJyYXkoWzAsIDAsIDAsIDFdKSAgIyBBTkQgbG9naWMgZ2F0ZQoKcGVyY2VwdHJvbiA9IFBlcmNlcHRyb24oaW5wdXRfc2l6ZT0yKQpwZXJjZXB0cm9uLnRyYWluKHRyYWluaW5nX2lucHV0cywgbGFiZWxzLCBlcG9jaHM9MTApCgojIFRlc3QgdGhlIHBlcmNlcHRyb24KcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFsxLCAxXSkpKSAgIyBPdXRwdXQ6IDEKcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFswLCAwXSkpKSAgIyBPdXRwdXQ6IDA="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Step activation function
def step_function(x):
    return 1 if x &gt;= 0 else 0

# Perceptron class
class Perceptron:
    def __init__(self, input_size, learning_rate=0.01):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.learning_rate = learning_rate

    def predict(self, inputs):
        total_sum = np.dot(inputs, self.weights) + self.bias
        return step_function(total_sum)

    def train(self, training_inputs, labels, epochs):
        for _ in range(epochs):
            for inputs, label in zip(training_inputs, labels):
                prediction = self.predict(inputs)
                self.weights += self.learning_rate * (label - prediction) * inputs
                self.bias += self.learning_rate * (label - prediction)

# Example usage
training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 0, 0, 1])  # AND logic gate

perceptron = Perceptron(input_size=2)
perceptron.train(training_inputs, labels, epochs=10)

# Test the perceptron
print(perceptron.predict(np.array([1, 1])))  # Output: 1
print(perceptron.predict(np.array([0, 0])))  # Output: 0</pre></div><div class='content'></div><h2><p>Explanation</p>
</h2>
<div class='content'><ul>
<li><strong>Initialization:</strong> The perceptron is initialized with zero weights and a bias of zero.</li>
<li><strong>Training:</strong> The perceptron adjusts its weights and bias based on the error between the predicted and actual labels.</li>
<li><strong>Prediction:</strong> The perceptron uses the step function to classify inputs.</li>
</ul>
</div><h1><p>Multilayer Perceptron (MLP)</p>
</h1>
<div class='content'></div><h2><p>What is a Multilayer Perceptron?</p>
</h2>
<div class='content'><p>An MLP is a class of feedforward artificial neural network that consists of multiple layers of neurons, including one or more hidden layers. Unlike a single-layer perceptron, an MLP can model non-linear relationships.</p>
</div><h2><p>Structure of an MLP</p>
</h2>
<div class='content'><ul>
<li><strong>Input Layer:</strong> Receives the input features.</li>
<li><strong>Hidden Layers:</strong> One or more layers where each neuron applies a non-linear activation function.</li>
<li><strong>Output Layer:</strong> Produces the final output.</li>
</ul>
</div><h2><p>Activation Functions</p>
</h2>
<div class='content'><p>Common activation functions used in MLPs include:</p>
<ul>
<li><strong>Sigmoid:</strong> \( f(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>ReLU (Rectified Linear Unit):</strong> \( f(x) = \max(0, x) \)</li>
<li><strong>Tanh:</strong> \( f(x) = \tanh(x) \)</li>
</ul>
</div><h2><p>Example Code</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Multilayer Perceptron class
class MLP:
    def __init__(self, input_size, hidden_size, output_size):
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.bias_hidden = np.random.rand(hidden_size)
        self.bias_output = np.random.rand(output_size)

    def forward(self, inputs):
        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden
        self.hidden_layer_output = sigmoid(self.hidden_layer_input)
        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output
        self.output = sigmoid(self.output_layer_input)
        return self.output

    def backward(self, inputs, expected_output, learning_rate):
        output_error = expected_output - self.output
        output_delta = output_error * sigmoid_derivative(self.output)

        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)

        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta) * learning_rate
        self.bias_output += np.sum(output_delta, axis=0) * learning_rate
        self.weights_input_hidden += inputs.T.dot(hidden_delta) * learning_rate
        self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate

    def train(self, training_inputs, training_outputs, epochs, learning_rate):
        for _ in range(epochs):
            self.forward(training_inputs)
            self.backward(training_inputs, training_outputs, learning_rate)

# Example usage
training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
training_outputs = np.array([[0], [1], [1], [0]])  # XOR logic gate

mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_inputs, training_outputs, epochs=10000, learning_rate=0.1)

# Test the MLP
print(mlp.forward(np.array([1, 1])))  # Output: ~0 (close to 0)
print(mlp.forward(np.array([0, 1])))  # Output: ~1 (close to 1)</pre></div><div class='content'></div><h2><p>Explanation</p>
</h2>
<div class='content'><ul>
<li><strong>Initialization:</strong> The MLP is initialized with random weights and biases.</li>
<li><strong>Forward Propagation:</strong> The inputs are passed through the network, and activations are computed at each layer.</li>
<li><strong>Backward Propagation:</strong> The error is propagated backward, and weights and biases are updated to minimize the error.</li>
<li><strong>Training:</strong> The MLP is trained over multiple epochs to adjust weights and biases for better performance.</li>
</ul>
</div><h1><p>Practical Exercises</p>
</h1>
<div class='content'></div><h2><p>Exercise 1: Implement a Perceptron</p>
</h2>
<div class='content'><p><strong>Task:</strong> Implement a perceptron to classify the OR logic gate.</p>
<p><strong>Solution:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("dHJhaW5pbmdfaW5wdXRzID0gbnAuYXJyYXkoW1swLCAwXSwgWzAsIDFdLCBbMSwgMF0sIFsxLCAxXV0pCmxhYmVscyA9IG5wLmFycmF5KFswLCAxLCAxLCAxXSkgICMgT1IgbG9naWMgZ2F0ZQoKcGVyY2VwdHJvbiA9IFBlcmNlcHRyb24oaW5wdXRfc2l6ZT0yKQpwZXJjZXB0cm9uLnRyYWluKHRyYWluaW5nX2lucHV0cywgbGFiZWxzLCBlcG9jaHM9MTApCgojIFRlc3QgdGhlIHBlcmNlcHRyb24KcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFsxLCAxXSkpKSAgIyBPdXRwdXQ6IDEKcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFswLCAwXSkpKSAgIyBPdXRwdXQ6IDA="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 1, 1, 1])  # OR logic gate

perceptron = Perceptron(input_size=2)
perceptron.train(training_inputs, labels, epochs=10)

# Test the perceptron
print(perceptron.predict(np.array([1, 1])))  # Output: 1
print(perceptron.predict(np.array([0, 0])))  # Output: 0</pre></div><div class='content'></div><h2><p>Exercise 2: Train an MLP for XOR Logic Gate</p>
</h2>
<div class='content'><p><strong>Task:</strong> Train an MLP to classify the XOR logic gate.</p>
<p><strong>Solution:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("dHJhaW5pbmdfaW5wdXRzID0gbnAuYXJyYXkoW1swLCAwXSwgWzAsIDFdLCBbMSwgMF0sIFsxLCAxXV0pCnRyYWluaW5nX291dHB1dHMgPSBucC5hcnJheShbWzBdLCBbMV0sIFsxXSwgWzBdXSkgICMgWE9SIGxvZ2ljIGdhdGUKCm1scCA9IE1MUChpbnB1dF9zaXplPTIsIGhpZGRlbl9zaXplPTIsIG91dHB1dF9zaXplPTEpCm1scC50cmFpbih0cmFpbmluZ19pbnB1dHMsIHRyYWluaW5nX291dHB1dHMsIGVwb2Nocz0xMDAwMCwgbGVhcm5pbmdfcmF0ZT0wLjEpCgojIFRlc3QgdGhlIE1MUApwcmludChtbHAuZm9yd2FyZChucC5hcnJheShbMSwgMV0pKSkgICMgT3V0cHV0OiB+MCAoY2xvc2UgdG8gMCkKcHJpbnQobWxwLmZvcndhcmQobnAuYXJyYXkoWzAsIDFdKSkpICAjIE91dHB1dDogfjEgKGNsb3NlIHRvIDEp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
training_outputs = np.array([[0], [1], [1], [0]])  # XOR logic gate

mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_inputs, training_outputs, epochs=10000, learning_rate=0.1)

# Test the MLP
print(mlp.forward(np.array([1, 1])))  # Output: ~0 (close to 0)
print(mlp.forward(np.array([0, 1])))  # Output: ~1 (close to 1)</pre></div><div class='content'></div><h1><p>Common Mistakes and Tips</p>
</h1>
<div class='content'><ul>
<li><strong>Learning Rate:</strong> Choosing an appropriate learning rate is crucial. Too high can cause the model to converge too quickly to a suboptimal solution, while too low can make the training process very slow.</li>
<li><strong>Epochs:</strong> Ensure you train for enough epochs to allow the model to learn, but not too many to avoid overfitting.</li>
<li><strong>Activation Functions:</strong> Use appropriate activation functions for hidden layers to introduce non-linearity.</li>
</ul>
</div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we covered the basics of perceptrons and multilayer perceptrons. We explored their structures, mathematical representations, and provided practical examples and exercises. Understanding these foundational concepts is essential as we move on to more complex neural network architectures in the subsequent modules.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='01-04-basic-concepts-neural-networks' title="Basic Concepts of Neural Networks">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-02-activation-function' title="Activation Function">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Advertising</h1>
<p>This space is reserved for advertising.</p>
<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>Thank you for collaborating!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
