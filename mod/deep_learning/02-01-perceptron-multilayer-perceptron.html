<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow, noarchive">
    <title>Perceptron and Multilayer Perceptron</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-01-perceptron-perceptron-multicapa" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-01-perceptron-perceptron-multicapa" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-01-perceptron-multilayer-perceptron" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.ea63f62b9e.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "en";
  		var CATEGORY = "foundations";
  		var MOD_NAME = "deep_learning";
  		var TEMA_NAME = "2-1";
  		var TYPE = "mod";
  		var PATH = "mod/deep_learning/02-01-perceptron-multilayer-perceptron";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.86da6c742a.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/deep_learning/02-01-perceptron-perceptron-multicapa" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/deep_learning/02-01-perceptron-perceptron-multicapa" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/my_modules" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>My courses</b></i>
</a>
<a href="/end_modules" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Completed             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="deep_learning">
		<a  href="#" class="text-secondary d-none" data-read-mod="deep_learning" data-read-unit="2-1" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Mark as read
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="deep_learning" data-unread-unit="2-1" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Mark as unread
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Course Content
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='01-04-basic-concepts-neural-networks' title="Basic Concepts of Neural Networks" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='01-04-basic-concepts-neural-networks' title="Basic Concepts of Neural Networks" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h2 style="text-decoration:underline">Perceptron and Multilayer Perceptron</h2>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='02-02-activation-function' title="Activation Function" class="py-2 px-3 btn btn-primary"
				data-read-mod="deep_learning" data-read-unit="2-1">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='02-02-activation-function' title="Activation Function" class="py-2 px-3 btn btn-primary" 
				data-read-mod="deep_learning" data-read-unit="2-1">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>In this section, we will explore the foundational elements of neural networks: the perceptron and the multilayer perceptron (MLP). Understanding these concepts is crucial as they form the building blocks for more complex neural network architectures.</p>
</div><h1>Perceptron</h1>
<div class='content'></div><h2>What is a Perceptron?</h2>
<div class='content'><p>A perceptron is the simplest type of artificial neural network and serves as a linear binary classifier. It consists of a single neuron with adjustable weights and a bias term.</p>
</div><h2>Structure of a Perceptron</h2>
<div class='content'><ul>
<li><strong>Inputs (x1, x2, ..., xn):</strong> These are the features of the input data.</li>
<li><strong>Weights (w1, w2, ..., wn):</strong> Each input has an associated weight that adjusts during training.</li>
<li><strong>Bias (b):</strong> An additional parameter that helps the model fit the data better.</li>
<li><strong>Activation Function:</strong> Typically a step function that determines the output based on the weighted sum of inputs.</li>
</ul>
</div><h2>Mathematical Representation</h2>
<div class='content'><p>The output of a perceptron can be represented mathematically as:</p>
<p>\[ y = f\left(\sum_{i=1}^{n} w_i x_i + b\right) \]</p>
<p>Where:</p>
<ul>
<li>\( y \) is the output.</li>
<li>\( f \) is the activation function (e.g., step function).</li>
<li>\( w_i \) are the weights.</li>
<li>\( x_i \) are the inputs.</li>
<li>\( b \) is the bias.</li>
</ul>
</div><h2>Example Code</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIFN0ZXAgYWN0aXZhdGlvbiBmdW5jdGlvbgpkZWYgc3RlcF9mdW5jdGlvbih4KToKICAgIHJldHVybiAxIGlmIHggPj0gMCBlbHNlIDAKCiMgUGVyY2VwdHJvbiBjbGFzcwpjbGFzcyBQZXJjZXB0cm9uOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGxlYXJuaW5nX3JhdGU9MC4wMSk6CiAgICAgICAgc2VsZi53ZWlnaHRzID0gbnAuemVyb3MoaW5wdXRfc2l6ZSkKICAgICAgICBzZWxmLmJpYXMgPSAwCiAgICAgICAgc2VsZi5sZWFybmluZ19yYXRlID0gbGVhcm5pbmdfcmF0ZQoKICAgIGRlZiBwcmVkaWN0KHNlbGYsIGlucHV0cyk6CiAgICAgICAgdG90YWxfc3VtID0gbnAuZG90KGlucHV0cywgc2VsZi53ZWlnaHRzKSArIHNlbGYuYmlhcwogICAgICAgIHJldHVybiBzdGVwX2Z1bmN0aW9uKHRvdGFsX3N1bSkKCiAgICBkZWYgdHJhaW4oc2VsZiwgdHJhaW5pbmdfaW5wdXRzLCBsYWJlbHMsIGVwb2Nocyk6CiAgICAgICAgZm9yIF8gaW4gcmFuZ2UoZXBvY2hzKToKICAgICAgICAgICAgZm9yIGlucHV0cywgbGFiZWwgaW4gemlwKHRyYWluaW5nX2lucHV0cywgbGFiZWxzKToKICAgICAgICAgICAgICAgIHByZWRpY3Rpb24gPSBzZWxmLnByZWRpY3QoaW5wdXRzKQogICAgICAgICAgICAgICAgc2VsZi53ZWlnaHRzICs9IHNlbGYubGVhcm5pbmdfcmF0ZSAqIChsYWJlbCAtIHByZWRpY3Rpb24pICogaW5wdXRzCiAgICAgICAgICAgICAgICBzZWxmLmJpYXMgKz0gc2VsZi5sZWFybmluZ19yYXRlICogKGxhYmVsIC0gcHJlZGljdGlvbikKCiMgRXhhbXBsZSB1c2FnZQp0cmFpbmluZ19pbnB1dHMgPSBucC5hcnJheShbWzAsIDBdLCBbMCwgMV0sIFsxLCAwXSwgWzEsIDFdXSkKbGFiZWxzID0gbnAuYXJyYXkoWzAsIDAsIDAsIDFdKSAgIyBBTkQgbG9naWMgZ2F0ZQoKcGVyY2VwdHJvbiA9IFBlcmNlcHRyb24oaW5wdXRfc2l6ZT0yKQpwZXJjZXB0cm9uLnRyYWluKHRyYWluaW5nX2lucHV0cywgbGFiZWxzLCBlcG9jaHM9MTApCgojIFRlc3QgdGhlIHBlcmNlcHRyb24KcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFsxLCAxXSkpKSAgIyBPdXRwdXQ6IDEKcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFswLCAwXSkpKSAgIyBPdXRwdXQ6IDA="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Step activation function
def step_function(x):
    return 1 if x &gt;= 0 else 0

# Perceptron class
class Perceptron:
    def __init__(self, input_size, learning_rate=0.01):
        self.weights = np.zeros(input_size)
        self.bias = 0
        self.learning_rate = learning_rate

    def predict(self, inputs):
        total_sum = np.dot(inputs, self.weights) + self.bias
        return step_function(total_sum)

    def train(self, training_inputs, labels, epochs):
        for _ in range(epochs):
            for inputs, label in zip(training_inputs, labels):
                prediction = self.predict(inputs)
                self.weights += self.learning_rate * (label - prediction) * inputs
                self.bias += self.learning_rate * (label - prediction)

# Example usage
training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 0, 0, 1])  # AND logic gate

perceptron = Perceptron(input_size=2)
perceptron.train(training_inputs, labels, epochs=10)

# Test the perceptron
print(perceptron.predict(np.array([1, 1])))  # Output: 1
print(perceptron.predict(np.array([0, 0])))  # Output: 0</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Initialization:</strong> The perceptron is initialized with zero weights and a bias of zero.</li>
<li><strong>Training:</strong> The perceptron adjusts its weights and bias based on the error between the predicted and actual labels.</li>
<li><strong>Prediction:</strong> The perceptron uses the step function to classify inputs.</li>
</ul>
</div><h1>Multilayer Perceptron (MLP)</h1>
<div class='content'></div><h2>What is a Multilayer Perceptron?</h2>
<div class='content'><p>An MLP is a class of feedforward artificial neural network that consists of multiple layers of neurons, including one or more hidden layers. Unlike a single-layer perceptron, an MLP can model non-linear relationships.</p>
</div><h2>Structure of an MLP</h2>
<div class='content'><ul>
<li><strong>Input Layer:</strong> Receives the input features.</li>
<li><strong>Hidden Layers:</strong> One or more layers where each neuron applies a non-linear activation function.</li>
<li><strong>Output Layer:</strong> Produces the final output.</li>
</ul>
</div><h2>Activation Functions</h2>
<div class='content'><p>Common activation functions used in MLPs include:</p>
<ul>
<li><strong>Sigmoid:</strong> \( f(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>ReLU (Rectified Linear Unit):</strong> \( f(x) = \max(0, x) \)</li>
<li><strong>Tanh:</strong> \( f(x) = \tanh(x) \)</li>
</ul>
</div><h2>Example Code</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Multilayer Perceptron class
class MLP:
    def __init__(self, input_size, hidden_size, output_size):
        self.weights_input_hidden = np.random.rand(input_size, hidden_size)
        self.weights_hidden_output = np.random.rand(hidden_size, output_size)
        self.bias_hidden = np.random.rand(hidden_size)
        self.bias_output = np.random.rand(output_size)

    def forward(self, inputs):
        self.hidden_layer_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden
        self.hidden_layer_output = sigmoid(self.hidden_layer_input)
        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output
        self.output = sigmoid(self.output_layer_input)
        return self.output

    def backward(self, inputs, expected_output, learning_rate):
        output_error = expected_output - self.output
        output_delta = output_error * sigmoid_derivative(self.output)

        hidden_error = output_delta.dot(self.weights_hidden_output.T)
        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_layer_output)

        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_delta) * learning_rate
        self.bias_output += np.sum(output_delta, axis=0) * learning_rate
        self.weights_input_hidden += inputs.T.dot(hidden_delta) * learning_rate
        self.bias_hidden += np.sum(hidden_delta, axis=0) * learning_rate

    def train(self, training_inputs, training_outputs, epochs, learning_rate):
        for _ in range(epochs):
            self.forward(training_inputs)
            self.backward(training_inputs, training_outputs, learning_rate)

# Example usage
training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
training_outputs = np.array([[0], [1], [1], [0]])  # XOR logic gate

mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_inputs, training_outputs, epochs=10000, learning_rate=0.1)

# Test the MLP
print(mlp.forward(np.array([1, 1])))  # Output: ~0 (close to 0)
print(mlp.forward(np.array([0, 1])))  # Output: ~1 (close to 1)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Initialization:</strong> The MLP is initialized with random weights and biases.</li>
<li><strong>Forward Propagation:</strong> The inputs are passed through the network, and activations are computed at each layer.</li>
<li><strong>Backward Propagation:</strong> The error is propagated backward, and weights and biases are updated to minimize the error.</li>
<li><strong>Training:</strong> The MLP is trained over multiple epochs to adjust weights and biases for better performance.</li>
</ul>
</div><h1>Practical Exercises</h1>
<div class='content'></div><h2>Exercise 1: Implement a Perceptron</h2>
<div class='content'><p><strong>Task:</strong> Implement a perceptron to classify the OR logic gate.</p>
<p><strong>Solution:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("dHJhaW5pbmdfaW5wdXRzID0gbnAuYXJyYXkoW1swLCAwXSwgWzAsIDFdLCBbMSwgMF0sIFsxLCAxXV0pCmxhYmVscyA9IG5wLmFycmF5KFswLCAxLCAxLCAxXSkgICMgT1IgbG9naWMgZ2F0ZQoKcGVyY2VwdHJvbiA9IFBlcmNlcHRyb24oaW5wdXRfc2l6ZT0yKQpwZXJjZXB0cm9uLnRyYWluKHRyYWluaW5nX2lucHV0cywgbGFiZWxzLCBlcG9jaHM9MTApCgojIFRlc3QgdGhlIHBlcmNlcHRyb24KcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFsxLCAxXSkpKSAgIyBPdXRwdXQ6IDEKcHJpbnQocGVyY2VwdHJvbi5wcmVkaWN0KG5wLmFycmF5KFswLCAwXSkpKSAgIyBPdXRwdXQ6IDA="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
labels = np.array([0, 1, 1, 1])  # OR logic gate

perceptron = Perceptron(input_size=2)
perceptron.train(training_inputs, labels, epochs=10)

# Test the perceptron
print(perceptron.predict(np.array([1, 1])))  # Output: 1
print(perceptron.predict(np.array([0, 0])))  # Output: 0</pre></div><div class='content'></div><h2>Exercise 2: Train an MLP for XOR Logic Gate</h2>
<div class='content'><p><strong>Task:</strong> Train an MLP to classify the XOR logic gate.</p>
<p><strong>Solution:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("dHJhaW5pbmdfaW5wdXRzID0gbnAuYXJyYXkoW1swLCAwXSwgWzAsIDFdLCBbMSwgMF0sIFsxLCAxXV0pCnRyYWluaW5nX291dHB1dHMgPSBucC5hcnJheShbWzBdLCBbMV0sIFsxXSwgWzBdXSkgICMgWE9SIGxvZ2ljIGdhdGUKCm1scCA9IE1MUChpbnB1dF9zaXplPTIsIGhpZGRlbl9zaXplPTIsIG91dHB1dF9zaXplPTEpCm1scC50cmFpbih0cmFpbmluZ19pbnB1dHMsIHRyYWluaW5nX291dHB1dHMsIGVwb2Nocz0xMDAwMCwgbGVhcm5pbmdfcmF0ZT0wLjEpCgojIFRlc3QgdGhlIE1MUApwcmludChtbHAuZm9yd2FyZChucC5hcnJheShbMSwgMV0pKSkgICMgT3V0cHV0OiB+MCAoY2xvc2UgdG8gMCkKcHJpbnQobWxwLmZvcndhcmQobnAuYXJyYXkoWzAsIDFdKSkpICAjIE91dHB1dDogfjEgKGNsb3NlIHRvIDEp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
training_outputs = np.array([[0], [1], [1], [0]])  # XOR logic gate

mlp = MLP(input_size=2, hidden_size=2, output_size=1)
mlp.train(training_inputs, training_outputs, epochs=10000, learning_rate=0.1)

# Test the MLP
print(mlp.forward(np.array([1, 1])))  # Output: ~0 (close to 0)
print(mlp.forward(np.array([0, 1])))  # Output: ~1 (close to 1)</pre></div><div class='content'></div><h1>Common Mistakes and Tips</h1>
<div class='content'><ul>
<li><strong>Learning Rate:</strong> Choosing an appropriate learning rate is crucial. Too high can cause the model to converge too quickly to a suboptimal solution, while too low can make the training process very slow.</li>
<li><strong>Epochs:</strong> Ensure you train for enough epochs to allow the model to learn, but not too many to avoid overfitting.</li>
<li><strong>Activation Functions:</strong> Use appropriate activation functions for hidden layers to introduce non-linearity.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we covered the basics of perceptrons and multilayer perceptrons. We explored their structures, mathematical representations, and provided practical examples and exercises. Understanding these foundational concepts is essential as we move on to more complex neural network architectures in the subsequent modules.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='01-04-basic-concepts-neural-networks' title="Basic Concepts of Neural Networks" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='01-04-basic-concepts-neural-networks' title="Basic Concepts of Neural Networks" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='02-02-activation-function' title="Activation Function" class="py-2 px-3 btn btn-primary"
				data-read-mod="deep_learning" data-read-unit="2-1">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='02-02-activation-function' title="Activation Function" class="py-2 px-3 btn btn-primary" 
				data-read-mod="deep_learning" data-read-unit="2-1">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>Deep Learning Course</h1>
<h2>Module 1: Introduction to Deep Learning</h2>
<ul>
<li><a href="01-01-what-is-deep-learning">What is Deep Learning?</a></li>
<li><a href="01-02-history-evolution-deep-learning">History and Evolution of Deep Learning</a></li>
<li><a href="01-03-applications-deep-learning">Applications of Deep Learning</a></li>
<li><a href="01-04-basic-concepts-neural-networks">Basic Concepts of Neural Networks</a></li>
</ul>
<h2>Module 2: Fundamentals of Neural Networks</h2>
<ul>
<li><a href="02-01-perceptron-multilayer-perceptron">Perceptron and Multilayer Perceptron</a></li>
<li><a href="02-02-activation-function">Activation Function</a></li>
<li><a href="02-03-forward-backward-propagation">Forward and Backward Propagation</a></li>
<li><a href="02-04-optimization-loss-function">Optimization and Loss Function</a></li>
</ul>
<h2>Module 3: Convolutional Neural Networks (CNN)</h2>
<ul>
<li><a href="03-01-introduction-cnn">Introduction to CNN</a></li>
<li><a href="03-02-convolutional-pooling-layers">Convolutional and Pooling Layers</a></li>
<li><a href="03-03-popular-cnn-architectures">Popular CNN Architectures</a></li>
<li><a href="03-04-cnn-applications-image-recognition">CNN Applications in Image Recognition</a></li>
</ul>
<h2>Module 4: Recurrent Neural Networks (RNN)</h2>
<ul>
<li><a href="04-01-introduction-rnn">Introduction to RNN</a></li>
<li><a href="04-02-lstm-gru">LSTM and GRU</a></li>
<li><a href="04-03-rnn-applications-nlp">RNN Applications in Natural Language Processing</a></li>
<li><a href="04-04-sequences-time-series">Sequences and Time Series</a></li>
</ul>
<h2>Module 5: Advanced Techniques in Deep Learning</h2>
<ul>
<li><a href="05-01-generative-adversarial-networks">Generative Adversarial Networks (GAN)</a></li>
<li><a href="05-02-autoencoders">Autoencoders</a></li>
<li><a href="05-03-transfer-learning">Transfer Learning</a></li>
<li><a href="05-04-regularization-improvement-techniques">Regularization and Improvement Techniques</a></li>
</ul>
<h2>Module 6: Tools and Frameworks</h2>
<ul>
<li><a href="06-01-introduction-tensorflow">Introduction to TensorFlow</a></li>
<li><a href="06-02-introduction-pytorch">Introduction to PyTorch</a></li>
<li><a href="06-03-framework-comparison">Framework Comparison</a></li>
<li><a href="06-04-development-environments-resources">Development Environments and Additional Resources</a></li>
</ul>
<h2>Module 7: Practical Projects</h2>
<ul>
<li><a href="07-01-image-classification-cnn">Image Classification with CNN</a></li>
<li><a href="07-02-text-generation-rnn">Text Generation with RNN</a></li>
<li><a href="07-03-anomaly-detection-autoencoders">Anomaly Detection with Autoencoders</a></li>
<li><a href="07-04-creating-gan-image-generation">Creating a GAN for Image Generation</a></li>
</ul>
<h2>Module 8: Ethical Considerations and the Future of Deep Learning</h2>
<ul>
<li><a href="08-01-ethics-deep-learning">Ethics in Deep Learning</a></li>
<li><a href="08-02-social-economic-impact">Social and Economic Impact</a></li>
<li><a href="08-03-future-trends-deep-learning">Future Trends in Deep Learning</a></li>
<li><a href="08-04-challenges-opportunities">Challenges and Opportunities</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">User not authenticated</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<p id="loginModalEmail"></p>
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
