<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimization and Loss Function</title>

    <link rel="alternate" href="https://campusempresa.com/mod/deep_learning/02-04-optimizacion-funcion-de-perdida" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/deep_learning/02-04-optimizacion-funcion-de-perdida" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/deep_learning/02-04-optimization-loss-function" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/deep_learning/02-04-optimizacion-funcion-de-perdida" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/deep_learning/02-04-optimizacion-funcion-de-perdida" class="px-2">CA</a>
					</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<!-- <a href="/">Home</a>  -->
									<a href="./">Course Content</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Technical Skills</a>
				<a href="/all/conocimientos">Knowledge</a>
				<a href="/all/soft_skills">Social Skills</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Optimization and Loss Function</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='03-01-introduction-cnn' title="Introduction to CNN">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introduction</p>
</h1>
<div class='content'><p>In this section, we will delve into the core concepts of optimization and loss functions in neural networks. Understanding these concepts is crucial for training effective deep learning models. We will cover:</p>
<ol>
<li><strong>What is Optimization?</strong></li>
<li><strong>Types of Optimization Algorithms</strong></li>
<li><strong>What is a Loss Function?</strong></li>
<li><strong>Common Loss Functions</strong></li>
<li><strong>Practical Examples and Exercises</strong></li>
</ol>
</div><h1><p>What is Optimization?</p>
</h1>
<div class='content'><p>Optimization in the context of neural networks refers to the process of adjusting the model parameters (weights and biases) to minimize the loss function. The goal is to find the set of parameters that result in the best performance of the model on the given task.</p>
</div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>Objective Function</strong>: The function that needs to be minimized or maximized. In neural networks, this is typically the loss function.</li>
<li><strong>Gradient Descent</strong>: A popular optimization algorithm used to minimize the loss function by iteratively moving in the direction of the steepest descent as defined by the negative of the gradient.</li>
</ul>
</div><h1><p>Types of Optimization Algorithms</p>
</h1>
<div class='content'></div><h2><p>Gradient Descent Variants:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Stochastic Gradient Descent (SGD)</strong>:</p>
<ul>
<li>Updates the model parameters using one training example at a time.</li>
<li>Pros: Faster updates.</li>
<li>Cons: High variance in updates can lead to instability.</li>
</ul>
</li>
<li>
<p><strong>Mini-Batch Gradient Descent</strong>:</p>
<ul>
<li>Updates the model parameters using a small batch of training examples.</li>
<li>Pros: Balances the trade-off between the efficiency of SGD and the stability of Batch Gradient Descent.</li>
</ul>
</li>
<li>
<p><strong>Batch Gradient Descent</strong>:</p>
<ul>
<li>Updates the model parameters using the entire training dataset.</li>
<li>Pros: Stable updates.</li>
<li>Cons: Computationally expensive and slow for large datasets.</li>
</ul>
</li>
</ol>
</div><h2><p>Advanced Optimization Algorithms:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Momentum</strong>:</p>
<ul>
<li>Accelerates gradient descent by considering the past gradients to smooth out the updates.</li>
<li>Formula: \( v_t = \beta v_{t-1} + (1 - \beta) \nabla J(\theta) \)</li>
<li>Update: \( \theta = \theta - \alpha v_t \)</li>
</ul>
</li>
<li>
<p><strong>RMSprop</strong>:</p>
<ul>
<li>Adapts the learning rate for each parameter by dividing the gradient by a running average of recent gradients.</li>
<li>Formula: \( E[g^2]<em>t = \beta E[g^2]</em>{t-1} + (1 - \beta) g_t^2 \)</li>
<li>Update: \( \theta = \theta - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}} g_t \)</li>
</ul>
</li>
<li>
<p><strong>Adam (Adaptive Moment Estimation)</strong>:</p>
<ul>
<li>Combines the ideas of Momentum and RMSprop.</li>
<li>Formula: \( m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \)</li>
<li>\( v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \)</li>
<li>Update: \( \theta = \theta - \frac{\alpha \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} \)</li>
</ul>
</li>
</ol>
</div><h1><p>What is a Loss Function?</p>
</h1>
<div class='content'><p>A loss function, also known as a cost function or objective function, measures how well the neural network's predictions match the actual target values. The goal of training a neural network is to minimize this loss function.</p>
</div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>Prediction Error</strong>: The difference between the predicted value and the actual value.</li>
<li><strong>Minimization</strong>: The process of finding the set of parameters that result in the lowest possible loss.</li>
</ul>
</div><h1><p>Common Loss Functions</p>
</h1>
<div class='content'></div><h2><p>For Regression Tasks:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Mean Squared Error (MSE)</strong>:</p>
<ul>
<li>Formula: \( \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \)</li>
<li>Measures the average squared difference between the predicted and actual values.</li>
</ul>
</li>
<li>
<p><strong>Mean Absolute Error (MAE)</strong>:</p>
<ul>
<li>Formula: \( \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i| \)</li>
<li>Measures the average absolute difference between the predicted and actual values.</li>
</ul>
</li>
</ol>
</div><h2><p>For Classification Tasks:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Binary Cross-Entropy</strong>:</p>
<ul>
<li>Formula: \( \text{BCE} = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] \)</li>
<li>Used for binary classification problems.</li>
</ul>
</li>
<li>
<p><strong>Categorical Cross-Entropy</strong>:</p>
<ul>
<li>Formula: \( \text{CCE} = -\sum_{i=1}^{n} \sum_{j=1}^{k} y_{ij} \log(\hat{y}_{ij}) \)</li>
<li>Used for multi-class classification problems.</li>
</ul>
</li>
</ol>
</div><h1><p>Practical Examples and Exercises</p>
</h1>
<div class='content'></div><h2><p>Example: Implementing Gradient Descent</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEV4YW1wbGUgZGF0YQpYID0gbnAuYXJyYXkoWzEsIDIsIDMsIDRdKQp5ID0gbnAuYXJyYXkoWzIsIDQsIDYsIDhdKQoKIyBJbml0aWFsaXplIHBhcmFtZXRlcnMKdGhldGEgPSAwLjAKYWxwaGEgPSAwLjAxICAjIExlYXJuaW5nIHJhdGUKZXBvY2hzID0gMTAwMAoKIyBHcmFkaWVudCBEZXNjZW50CmZvciBlcG9jaCBpbiByYW5nZShlcG9jaHMpOgogICAgZ3JhZGllbnQgPSAtMiAqIG5wLnN1bSgoeSAtIHRoZXRhICogWCkgKiBYKSAvIGxlbihYKQogICAgdGhldGEgPSB0aGV0YSAtIGFscGhhICogZ3JhZGllbnQKCnByaW50KGYiT3B0aW1pemVkIHRoZXRhOiB7dGhldGF9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Example data
X = np.array([1, 2, 3, 4])
y = np.array([2, 4, 6, 8])

# Initialize parameters
theta = 0.0
alpha = 0.01  # Learning rate
epochs = 1000

# Gradient Descent
for epoch in range(epochs):
    gradient = -2 * np.sum((y - theta * X) * X) / len(X)
    theta = theta - alpha * gradient

print(f&quot;Optimized theta: {theta}&quot;)</pre></div><div class='content'></div><h2><p>Explanation:</p>
</h2>
<div class='content'><ul>
<li><strong>Data</strong>: Simple linear relationship \( y = 2x \).</li>
<li><strong>Parameters</strong>: Initialized to zero.</li>
<li><strong>Gradient Descent</strong>: Iteratively updates the parameter \( \theta \) to minimize the Mean Squared Error.</li>
</ul>
</div><h2><p>Exercise: Implementing Mean Squared Error</p>
</h2>
<div class='content'><p><strong>Task</strong>: Write a function to compute the Mean Squared Error for given predictions and actual values.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIG1lYW5fc3F1YXJlZF9lcnJvcih5X3RydWUsIHlfcHJlZCk6CiAgICAiIiIKICAgIENvbXB1dGUgdGhlIE1lYW4gU3F1YXJlZCBFcnJvciBiZXR3ZWVuIGFjdHVhbCBhbmQgcHJlZGljdGVkIHZhbHVlcy4KICAgIAogICAgUGFyYW1ldGVyczoKICAgIHlfdHJ1ZSAoYXJyYXkpOiBBY3R1YWwgdmFsdWVzCiAgICB5X3ByZWQgKGFycmF5KTogUHJlZGljdGVkIHZhbHVlcwogICAgCiAgICBSZXR1cm5zOgogICAgZmxvYXQ6IE1lYW4gU3F1YXJlZCBFcnJvcgogICAgIiIiCiAgICBtc2UgPSBucC5tZWFuKCh5X3RydWUgLSB5X3ByZWQpICoqIDIpCiAgICByZXR1cm4gbXNlCgojIFRlc3QgdGhlIGZ1bmN0aW9uCnlfdHJ1ZSA9IG5wLmFycmF5KFsyLCA0LCA2LCA4XSkKeV9wcmVkID0gbnAuYXJyYXkoWzIuMSwgMy45LCA2LjIsIDcuOF0pCnByaW50KGYiTWVhbiBTcXVhcmVkIEVycm9yOiB7bWVhbl9zcXVhcmVkX2Vycm9yKHlfdHJ1ZSwgeV9wcmVkKX0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def mean_squared_error(y_true, y_pred):
    &quot;&quot;&quot;
    Compute the Mean Squared Error between actual and predicted values.
    
    Parameters:
    y_true (array): Actual values
    y_pred (array): Predicted values
    
    Returns:
    float: Mean Squared Error
    &quot;&quot;&quot;
    mse = np.mean((y_true - y_pred) ** 2)
    return mse

# Test the function
y_true = np.array([2, 4, 6, 8])
y_pred = np.array([2.1, 3.9, 6.2, 7.8])
print(f&quot;Mean Squared Error: {mean_squared_error(y_true, y_pred)}&quot;)</pre></div><div class='content'></div><h2><p>Solution Explanation:</p>
</h2>
<div class='content'><ul>
<li><strong>Function</strong>: Computes the average of the squared differences between actual and predicted values.</li>
<li><strong>Test</strong>: Validates the function with example data.</li>
</ul>
</div><h1><p>Summary</p>
</h1>
<div class='content'><p>In this section, we covered the fundamental concepts of optimization and loss functions in neural networks. We explored various optimization algorithms, including Gradient Descent and its variants, as well as advanced algorithms like Adam. We also discussed common loss functions for regression and classification tasks and provided practical examples and exercises to reinforce the concepts.</p>
</div><h2><p>Next Steps:</p>
</h2>
<div class='content'><p>In the next module, we will dive into Convolutional Neural Networks (CNNs), exploring their architecture, layers, and applications in image recognition.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='03-01-introduction-cnn' title="Introduction to CNN">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
