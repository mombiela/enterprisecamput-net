<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activation Functions in TensorFlow</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/tensorflow/activation-functions" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/tensorflow/activation-functions" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-4'>
					<a href='creating-a-simple-neural-network'>&#x25C4;Creating a Simple Neural Network</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Activation Functions in TensorFlow</a>
	</div>
	<div class='col-4 text-end'>
					<a href='loss-functions-and-optimizers'>Loss Functions and Optimizers &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Activation functions play a crucial role in neural networks by introducing non-linearity into the model, enabling it to learn complex patterns. In this section, we will explore various activation functions, their properties, and how to implement them in TensorFlow.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ul>
<li><strong>Non-linearity</strong>: Activation functions introduce non-linear properties to the network, allowing it to learn from errors and adjust weights accordingly.</li>
<li><strong>Differentiability</strong>: Most activation functions are differentiable, which is essential for backpropagation.</li>
<li><strong>Range</strong>: The output range of an activation function can affect the stability and performance of the network.</li>
</ul>
</div><h1>Common Activation Functions</h1>
<div class='content'></div><h2>Sigmoid</h2>
<div class='content'><p>The sigmoid function maps any input to a value between 0 and 1. It is often used in the output layer for binary classification problems.</p>
<p><strong>Formula</strong>:
\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>TensorFlow Implementation</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgRXhhbXBsZSBvZiB1c2luZyBzaWdtb2lkIGFjdGl2YXRpb24gZnVuY3Rpb24KeCA9IHRmLmNvbnN0YW50KFstMS4wLCAwLjAsIDEuMF0sIGR0eXBlPXRmLmZsb2F0MzIpCnNpZ21vaWRfb3V0cHV0ID0gdGYubm4uc2lnbW9pZCh4KQpwcmludChzaWdtb2lkX291dHB1dC5udW1weSgpKSAgIyBPdXRwdXQ6IFswLjI2ODk0MTQzIDAuNSAwLjczMTA1ODZd"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Example of using sigmoid activation function
x = tf.constant([-1.0, 0.0, 1.0], dtype=tf.float32)
sigmoid_output = tf.nn.sigmoid(x)
print(sigmoid_output.numpy())  # Output: [0.26894143 0.5 0.7310586]</pre></div><div class='content'></div><h2>Tanh</h2>
<div class='content'><p>The tanh function maps any input to a value between -1 and 1. It is zero-centered, which can make optimization easier.</p>
<p><strong>Formula</strong>:
\[ \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>TensorFlow Implementation</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeGFtcGxlIG9mIHVzaW5nIHRhbmggYWN0aXZhdGlvbiBmdW5jdGlvbgp0YW5oX291dHB1dCA9IHRmLm5uLnRhbmgoeCkKcHJpbnQodGFuaF9vdXRwdXQubnVtcHkoKSkgICMgT3V0cHV0OiBbLTAuNzYxNTk0MiAgMC4gICAgICAgICAwLjc2MTU5NDJd"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Example of using tanh activation function
tanh_output = tf.nn.tanh(x)
print(tanh_output.numpy())  # Output: [-0.7615942  0.         0.7615942]</pre></div><div class='content'></div><h2>ReLU (Rectified Linear Unit)</h2>
<div class='content'><p>ReLU is one of the most popular activation functions due to its simplicity and effectiveness. It outputs the input directly if it is positive; otherwise, it outputs zero.</p>
<p><strong>Formula</strong>:
\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>TensorFlow Implementation</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeGFtcGxlIG9mIHVzaW5nIFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbgpyZWx1X291dHB1dCA9IHRmLm5uLnJlbHUoeCkKcHJpbnQocmVsdV9vdXRwdXQubnVtcHkoKSkgICMgT3V0cHV0OiBbMC4gMC4gMS5d"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Example of using ReLU activation function
relu_output = tf.nn.relu(x)
print(relu_output.numpy())  # Output: [0. 0. 1.]</pre></div><div class='content'></div><h2>Leaky ReLU</h2>
<div class='content'><p>Leaky ReLU is a variant of ReLU that allows a small, non-zero gradient when the input is negative, which helps to mitigate the &quot;dying ReLU&quot; problem.</p>
<p><strong>Formula</strong>:
\[ \text{Leaky ReLU}(x) = \begin{cases}
x &amp; \text{if } x &gt; 0 <br>\alpha x &amp; \text{if } x \leq 0
\end{cases} \]</p>
<p><strong>TensorFlow Implementation</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeGFtcGxlIG9mIHVzaW5nIExlYWt5IFJlTFUgYWN0aXZhdGlvbiBmdW5jdGlvbgpsZWFreV9yZWx1X291dHB1dCA9IHRmLm5uLmxlYWt5X3JlbHUoeCwgYWxwaGE9MC4xKQpwcmludChsZWFreV9yZWx1X291dHB1dC5udW1weSgpKSAgIyBPdXRwdXQ6IFstMC4xICAwLiAgIDEuIF0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Example of using Leaky ReLU activation function
leaky_relu_output = tf.nn.leaky_relu(x, alpha=0.1)
print(leaky_relu_output.numpy())  # Output: [-0.1  0.   1. ]</pre></div><div class='content'></div><h2>Softmax</h2>
<div class='content'><p>Softmax is typically used in the output layer of a neural network for multi-class classification problems. It converts logits into probabilities.</p>
<p><strong>Formula</strong>:
\[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>TensorFlow Implementation</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeGFtcGxlIG9mIHVzaW5nIFNvZnRtYXggYWN0aXZhdGlvbiBmdW5jdGlvbgpsb2dpdHMgPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0sIGR0eXBlPXRmLmZsb2F0MzIpCnNvZnRtYXhfb3V0cHV0ID0gdGYubm4uc29mdG1heChsb2dpdHMpCnByaW50KHNvZnRtYXhfb3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuMDkwMDMwNTcgMC4yNDQ3Mjg0OCAwLjY2NTI0MDk0XQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Example of using Softmax activation function
logits = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
softmax_output = tf.nn.softmax(logits)
print(softmax_output.numpy())  # Output: [0.09003057 0.24472848 0.66524094]</pre></div><div class='content'></div><h1>Comparison of Activation Functions</h1>
<div class='content'><p>| Activation Function | Output Range | Differentiable | Common Use Cases |
|---------------------|--------------|----------------|------------------|
| Sigmoid             | (0, 1)       | Yes            | Binary classification, hidden layers |
| Tanh                | (-1, 1)      | Yes            | Hidden layers |
| ReLU                | [0, ∞)       | Yes            | Hidden layers |
| Leaky ReLU          | (-∞, ∞)      | Yes            | Hidden layers, mitigating dying ReLU |
| Softmax             | (0, 1)       | Yes            | Output layer for multi-class classification |</p>
</div><h1>Conclusion</h1>
<div class='content'><p>Activation functions are essential components of neural networks, introducing non-linearity and enabling the model to learn complex patterns. In TensorFlow, implementing these functions is straightforward, and understanding their properties can help you choose the right one for your specific problem. Experiment with different activation functions to see how they affect your model's performance and stability.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='creating-a-simple-neural-network'>&#x25C4;Creating a Simple Neural Network</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Activation Functions in TensorFlow</a>
	</div>
	<div class='col-4 text-end'>
					<a href='loss-functions-and-optimizers'>Loss Functions and Optimizers &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
