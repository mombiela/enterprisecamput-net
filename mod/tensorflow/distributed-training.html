<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Distributed Training</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/distributed-training" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/distributed-training" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/distributed-training" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/tensorflow/distributed-training" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/tensorflow/distributed-training" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-4'>
					<a href='tensorflow-hub'>&#x25C4;TensorFlow Hub</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Distributed Training</a>
	</div>
	<div class='col-4 text-end'>
					<a href='tensorflow-serving'>TensorFlow Serving &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Distributed training is a technique used to train machine learning models across multiple devices or machines. This approach can significantly speed up the training process and handle larger datasets that wouldn't fit into the memory of a single device. TensorFlow provides robust support for distributed training, making it easier to scale your models.</p>
</div><h1>Introduction to Distributed Training</h1>
<div class='content'><ul>
<li><strong>Definition</strong>: Distributed training involves splitting the training workload across multiple devices or machines.</li>
<li><strong>Benefits</strong>:
<ul>
<li><strong>Speed</strong>: Faster training times by leveraging multiple processors.</li>
<li><strong>Scalability</strong>: Ability to train on larger datasets.</li>
<li><strong>Resource Utilization</strong>: Efficient use of available hardware.</li>
</ul>
</li>
</ul>
</div><h1>Types of Distributed Training</h1>
<div class='content'></div><h2>Data Parallelism</h2>
<div class='content'><ul>
<li><strong>Concept</strong>: Each device gets a different portion of the data but the same model.</li>
<li><strong>Mechanism</strong>: Gradients are averaged across devices, and model weights are updated synchronously.</li>
<li><strong>Example</strong>:
<pre><code class="language-python">import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])

# Load and preprocess data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Train the model
model.fit(x_train, y_train, epochs=5)
</code></pre>
</li>
</ul>
</div><h2>Model Parallelism</h2>
<div class='content'><ul>
<li><strong>Concept</strong>: Different parts of the model are distributed across multiple devices.</li>
<li><strong>Mechanism</strong>: Each device computes a portion of the model, and the results are combined.</li>
<li><strong>Use Case</strong>: Useful when the model is too large to fit into the memory of a single device.</li>
</ul>
</div><h1>TensorFlow Strategies for Distributed Training</h1>
<div class='content'></div><h2>MirroredStrategy</h2>
<div class='content'><ul>
<li><strong>Description</strong>: Synchronous training on multiple GPUs on one machine.</li>
<li><strong>Usage</strong>:
<pre><code class="language-python">strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    # Define and compile your model here
</code></pre>
</li>
</ul>
</div><h2>MultiWorkerMirroredStrategy</h2>
<div class='content'><ul>
<li><strong>Description</strong>: Synchronous training on multiple machines.</li>
<li><strong>Usage</strong>:
<pre><code class="language-python">strategy = tf.distribute.MultiWorkerMirroredStrategy()
with strategy.scope():
    # Define and compile your model here
</code></pre>
</li>
</ul>
</div><h2>TPUStrategy</h2>
<div class='content'><ul>
<li><strong>Description</strong>: Training on TPU (Tensor Processing Unit) devices.</li>
<li><strong>Usage</strong>:
<pre><code class="language-python">resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='your-tpu-address')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

strategy = tf.distribute.TPUStrategy(resolver)
with strategy.scope():
    # Define and compile your model here
</code></pre>
</li>
</ul>
</div><h1>Practical Example: Distributed Training with MirroredStrategy</h1>
<div class='content'></div><h2>Step-by-Step Guide</h2>
<div class='content'><ol>
<li>
<p><strong>Setup the Strategy</strong>:</p>
<pre><code class="language-python">import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()
</code></pre>
</li>
<li>
<p><strong>Define the Model within the Strategy Scope</strong>:</p>
<pre><code class="language-python">with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer=tf.keras.optimizers.Adam(),
                  metrics=['accuracy'])
</code></pre>
</li>
<li>
<p><strong>Load and Preprocess Data</strong>:</p>
<pre><code class="language-python">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
</code></pre>
</li>
<li>
<p><strong>Train the Model</strong>:</p>
<pre><code class="language-python">model.fit(x_train, y_train, epochs=5)
</code></pre>
</li>
</ol>
</div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Strategy Setup</strong>: <code>MirroredStrategy</code> is used for synchronous training on multiple GPUs.</li>
<li><strong>Model Definition</strong>: The model is defined within the strategy's scope to ensure that it is distributed across the available GPUs.</li>
<li><strong>Data Loading</strong>: MNIST dataset is loaded and normalized.</li>
<li><strong>Training</strong>: The model is trained using the <code>fit</code> method, which will distribute the training across the GPUs.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>Distributed training in TensorFlow allows you to scale your models and speed up the training process by leveraging multiple devices. By understanding and utilizing different strategies like <code>MirroredStrategy</code>, <code>MultiWorkerMirroredStrategy</code>, and <code>TPUStrategy</code>, you can efficiently train large models on extensive datasets. Start with simple strategies and gradually move to more complex setups as your requirements grow.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='tensorflow-hub'>&#x25C4;TensorFlow Hub</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Distributed Training</a>
	</div>
	<div class='col-4 text-end'>
					<a href='tensorflow-serving'>TensorFlow Serving &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
