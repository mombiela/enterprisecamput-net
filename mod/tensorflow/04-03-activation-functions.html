<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activation Functions</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/04-03-activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/04-03-activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/04-03-activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/tensorflow/04-03-activation-functions" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/tensorflow/04-03-activation-functions" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-creating-a-simple-neural-network' title="Creating a Simple Neural Network">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Activation Functions</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-loss-functions-and-optimizers' title="Loss Functions and Optimizers">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Activation functions play a crucial role in neural networks by introducing non-linearity into the model, allowing it to learn complex patterns. In this section, we will explore various activation functions, their properties, and their applications.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ol>
<li>
<p><strong>What is an Activation Function?</strong></p>
<ul>
<li>An activation function determines whether a neuron should be activated or not by calculating the weighted sum and adding bias to it.</li>
<li>It introduces non-linearity into the output of a neuron, enabling the network to learn complex patterns.</li>
</ul>
</li>
<li>
<p><strong>Types of Activation Functions</strong></p>
<ul>
<li><strong>Linear Activation Function</strong></li>
<li><strong>Non-Linear Activation Functions</strong>
<ul>
<li>Sigmoid</li>
<li>Tanh</li>
<li>ReLU (Rectified Linear Unit)</li>
<li>Leaky ReLU</li>
<li>Softmax</li>
</ul>
</li>
</ul>
</li>
</ol>
</div><h1>Linear Activation Function</h1>
<div class='content'></div><h2>Definition</h2>
<div class='content'><p>A linear activation function is simply the identity function, where the output is directly proportional to the input.</p>
</div><h2>Formula</h2>
<div class='content'><p>\[ f(x) = x \]</p>
</div><h2>Characteristics</h2>
<div class='content'><ul>
<li><strong>Pros:</strong> Simple and easy to implement.</li>
<li><strong>Cons:</strong> Cannot handle complex patterns due to its linear nature.</li>
</ul>
</div><h2>Code Example</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgTGluZWFyIGFjdGl2YXRpb24gZnVuY3Rpb24KZGVmIGxpbmVhcl9hY3RpdmF0aW9uKHgpOgogICAgcmV0dXJuIHgKCiMgRXhhbXBsZSB1c2FnZQp4ID0gdGYuY29uc3RhbnQoWzEuMCwgMi4wLCAzLjBdKQpvdXRwdXQgPSBsaW5lYXJfYWN0aXZhdGlvbih4KQpwcmludChvdXRwdXQubnVtcHkoKSkgICMgT3V0cHV0OiBbMS4gMi4gMy5d"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Linear activation function
def linear_activation(x):
    return x

# Example usage
x = tf.constant([1.0, 2.0, 3.0])
output = linear_activation(x)
print(output.numpy())  # Output: [1. 2. 3.]</pre></div><div class='content'></div><h1>Non-Linear Activation Functions</h1>
<div class='content'></div><h2>Sigmoid</h2>
<div class='content'><h4>Definition</h4>
<p>The sigmoid function maps any input to a value between 0 and 1.</p>
<h4>Formula</h4>
<p>\[ f(x) = \frac{1}{1 + e^{-x}} \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Smooth gradient, output range (0, 1), good for binary classification.</li>
<li><strong>Cons:</strong> Vanishing gradient problem, not zero-centered.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgU2lnbW9pZCBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0pCm91dHB1dCA9IHRmLm5uLnNpZ21vaWQoeCkKcHJpbnQob3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuNzMxMDU4NiAwLjg4MDc5NyAgMC45NTI1NzQxM10="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Sigmoid activation function
x = tf.constant([1.0, 2.0, 3.0])
output = tf.nn.sigmoid(x)
print(output.numpy())  # Output: [0.7310586 0.880797  0.95257413]</pre></div><div class='content'></div><h2>Tanh</h2>
<div class='content'><h4>Definition</h4>
<p>The tanh function maps any input to a value between -1 and 1.</p>
<h4>Formula</h4>
<p>\[ f(x) = \tanh(x) = \frac{2}{1 + e^{-2x}} - 1 \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Zero-centered, smooth gradient.</li>
<li><strong>Cons:</strong> Vanishing gradient problem.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgVGFuaCBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0pCm91dHB1dCA9IHRmLm5uLnRhbmgoeCkKcHJpbnQob3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuNzYxNTk0MiAwLjk2NDAyNzYgMC45OTUwNTQ3XQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Tanh activation function
x = tf.constant([1.0, 2.0, 3.0])
output = tf.nn.tanh(x)
print(output.numpy())  # Output: [0.7615942 0.9640276 0.9950547]</pre></div><div class='content'></div><h2>ReLU (Rectified Linear Unit)</h2>
<div class='content'><h4>Definition</h4>
<p>The ReLU function outputs the input directly if it is positive; otherwise, it outputs zero.</p>
<h4>Formula</h4>
<p>\[ f(x) = \max(0, x) \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Computationally efficient, mitigates the vanishing gradient problem.</li>
<li><strong>Cons:</strong> Can cause &quot;dead neurons&quot; if many neurons output zero.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgUmVMVSBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbLTEuMCwgMi4wLCAzLjBdKQpvdXRwdXQgPSB0Zi5ubi5yZWx1KHgpCnByaW50KG91dHB1dC5udW1weSgpKSAgIyBPdXRwdXQ6IFswLiAyLiAzLl0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# ReLU activation function
x = tf.constant([-1.0, 2.0, 3.0])
output = tf.nn.relu(x)
print(output.numpy())  # Output: [0. 2. 3.]</pre></div><div class='content'></div><h2>Leaky ReLU</h2>
<div class='content'><h4>Definition</h4>
<p>Leaky ReLU allows a small, non-zero gradient when the input is negative.</p>
<h4>Formula</h4>
<p>\[ f(x) = \max(0.01x, x) \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Prevents &quot;dead neurons&quot; by allowing a small gradient when the input is negative.</li>
<li><strong>Cons:</strong> The slope of the negative part is a hyperparameter that needs tuning.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgTGVha3kgUmVMVSBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbLTEuMCwgMi4wLCAzLjBdKQpvdXRwdXQgPSB0Zi5ubi5sZWFreV9yZWx1KHgsIGFscGhhPTAuMDEpCnByaW50KG91dHB1dC5udW1weSgpKSAgIyBPdXRwdXQ6IFstMC4wMSAgMi4gICAgMy4gIF0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Leaky ReLU activation function
x = tf.constant([-1.0, 2.0, 3.0])
output = tf.nn.leaky_relu(x, alpha=0.01)
print(output.numpy())  # Output: [-0.01  2.    3.  ]</pre></div><div class='content'></div><h2>Softmax</h2>
<div class='content'><h4>Definition</h4>
<p>The softmax function converts a vector of values into a probability distribution.</p>
<h4>Formula</h4>
<p>\[ f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Useful for multi-class classification problems.</li>
<li><strong>Cons:</strong> Computationally expensive for large number of classes.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgU29mdG1heCBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0pCm91dHB1dCA9IHRmLm5uLnNvZnRtYXgoeCkKcHJpbnQob3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuMDkwMDMwNTcgMC4yNDQ3Mjg0OCAwLjY2NTI0MDk0XQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Softmax activation function
x = tf.constant([1.0, 2.0, 3.0])
output = tf.nn.softmax(x)
print(output.numpy())  # Output: [0.09003057 0.24472848 0.66524094]</pre></div><div class='content'></div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Task</h2>
<div class='content'><p>Implement a simple neural network using TensorFlow that uses different activation functions and compare their performance on the MNIST dataset.</p>
</div><h2>Solution</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmRhdGFzZXRzIGltcG9ydCBtbmlzdApmcm9tIHRlbnNvcmZsb3cua2VyYXMubW9kZWxzIGltcG9ydCBTZXF1ZW50aWFsCmZyb20gdGVuc29yZmxvdy5rZXJhcy5sYXllcnMgaW1wb3J0IERlbnNlLCBGbGF0dGVuCgojIExvYWQgTU5JU1QgZGF0YXNldAooeF90cmFpbiwgeV90cmFpbiksICh4X3Rlc3QsIHlfdGVzdCkgPSBtbmlzdC5sb2FkX2RhdGEoKQp4X3RyYWluLCB4X3Rlc3QgPSB4X3RyYWluIC8gMjU1LjAsIHhfdGVzdCAvIDI1NS4wCgojIERlZmluZSBhIHNpbXBsZSBuZXVyYWwgbmV0d29yayBtb2RlbApkZWYgY3JlYXRlX21vZGVsKGFjdGl2YXRpb25fZnVuY3Rpb24pOgogICAgbW9kZWwgPSBTZXF1ZW50aWFsKFsKICAgICAgICBGbGF0dGVuKGlucHV0X3NoYXBlPSgyOCwgMjgpKSwKICAgICAgICBEZW5zZSgxMjgsIGFjdGl2YXRpb249YWN0aXZhdGlvbl9mdW5jdGlvbiksCiAgICAgICAgRGVuc2UoMTAsIGFjdGl2YXRpb249J3NvZnRtYXgnKQogICAgXSkKICAgIG1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywKICAgICAgICAgICAgICAgICAgbG9zcz0nc3BhcnNlX2NhdGVnb3JpY2FsX2Nyb3NzZW50cm9weScsCiAgICAgICAgICAgICAgICAgIG1ldHJpY3M9WydhY2N1cmFjeSddKQogICAgcmV0dXJuIG1vZGVsCgojIFRyYWluIGFuZCBldmFsdWF0ZSB0aGUgbW9kZWwgd2l0aCBkaWZmZXJlbnQgYWN0aXZhdGlvbiBmdW5jdGlvbnMKYWN0aXZhdGlvbl9mdW5jdGlvbnMgPSBbJ3NpZ21vaWQnLCAndGFuaCcsICdyZWx1JywgJ2xlYWt5X3JlbHUnXQpmb3IgYWN0aXZhdGlvbiBpbiBhY3RpdmF0aW9uX2Z1bmN0aW9uczoKICAgIHByaW50KGYiVHJhaW5pbmcgd2l0aCB7YWN0aXZhdGlvbn0gYWN0aXZhdGlvbiBmdW5jdGlvbiIpCiAgICBtb2RlbCA9IGNyZWF0ZV9tb2RlbChhY3RpdmF0aW9uKQogICAgbW9kZWwuZml0KHhfdHJhaW4sIHlfdHJhaW4sIGVwb2Nocz01LCB2YWxpZGF0aW9uX2RhdGE9KHhfdGVzdCwgeV90ZXN0KSkKICAgIHRlc3RfbG9zcywgdGVzdF9hY2MgPSBtb2RlbC5ldmFsdWF0ZSh4X3Rlc3QsIHlfdGVzdCkKICAgIHByaW50KGYiVGVzdCBhY2N1cmFjeSB3aXRoIHthY3RpdmF0aW9ufToge3Rlc3RfYWNjfVxuIik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Define a simple neural network model
def create_model(activation_function):
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation=activation_function),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train and evaluate the model with different activation functions
activation_functions = ['sigmoid', 'tanh', 'relu', 'leaky_relu']
for activation in activation_functions:
    print(f&quot;Training with {activation} activation function&quot;)
    model = create_model(activation)
    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    test_loss, test_acc = model.evaluate(x_test, y_test)
    print(f&quot;Test accuracy with {activation}: {test_acc}\n&quot;)</pre></div><div class='content'></div><h1>Summary</h1>
<div class='content'><p>In this section, we covered the importance of activation functions in neural networks and explored various types, including linear, sigmoid, tanh, ReLU, Leaky ReLU, and softmax. Each activation function has its own characteristics, advantages, and disadvantages. Understanding these will help you choose the right activation function for your specific neural network model.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-creating-a-simple-neural-network' title="Creating a Simple Neural Network">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-loss-functions-and-optimizers' title="Loss Functions and Optimizers">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
