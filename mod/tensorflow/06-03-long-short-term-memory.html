<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Long Short-Term Memory (LSTM)</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/06-03-long-short-term-memory" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/06-03-long-short-term-memory" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/06-03-long-short-term-memory" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 px-0 py-2 py-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 px-0 py-2 py-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/tensorflow/06-03-long-short-term-memory" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/tensorflow/06-03-long-short-term-memory" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='06-02-building-an-rnn' title="Building an RNN">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Long Short-Term Memory (LSTM)</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='06-04-gated-recurrent-units' title="Gated Recurrent Units (GRUs)">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) that are capable of learning long-term dependencies. They are designed to avoid the long-term dependency problem, which is a common issue with traditional RNNs. LSTMs are widely used in various applications such as language modeling, speech recognition, and time series prediction.</p>
</div><h1>Key Concepts</h1>
<div class='content'></div><h2>1. LSTM Cell Structure</h2>
<div class='content'><p>An LSTM cell consists of several components:</p>
<ul>
<li><strong>Cell State</strong>: The memory of the network.</li>
<li><strong>Hidden State</strong>: The output of the LSTM cell.</li>
<li><strong>Gates</strong>: Mechanisms to control the flow of information. There are three types of gates:
<ul>
<li><strong>Forget Gate</strong>: Decides what information to discard from the cell state.</li>
<li><strong>Input Gate</strong>: Decides what new information to add to the cell state.</li>
<li><strong>Output Gate</strong>: Decides what information to output from the cell.</li>
</ul>
</li>
</ul>
</div><h2>2. LSTM Equations</h2>
<div class='content'><p>The operations within an LSTM cell can be described by the following equations:</p>
<ul>
<li><strong>Forget Gate</strong>:
\[
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\]</li>
<li><strong>Input Gate</strong>:
\[
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\]
\[
\tilde{C}<em>t = \tanh(W_C \cdot [h</em>{t-1}, x_t] + b_C)
\]</li>
<li><strong>Cell State Update</strong>:
\[
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
\]</li>
<li><strong>Output Gate</strong>:
\[
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\]
\[
h_t = o_t * \tanh(C_t)
\]</li>
</ul>
</div><h2>3. TensorFlow Implementation</h2>
<div class='content'><p>TensorFlow provides a high-level API to create LSTM layers easily. The <code>tf.keras.layers.LSTM</code> class is used to create an LSTM layer.</p>
</div><h1>Practical Example</h1>
<div class='content'><p>Let's build a simple LSTM model using TensorFlow to predict a sequence of numbers.</p>
</div><h2>Step 1: Import Libraries</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKaW1wb3J0IG51bXB5IGFzIG5w"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
import numpy as np</pre></div><div class='content'></div><h2>Step 2: Prepare Data</h2>
<div class='content'><p>For simplicity, we'll use a sine wave as our dataset.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBHZW5lcmF0ZSBhIHNpbmUgd2F2ZQp0aW1lX3N0ZXBzID0gbnAubGluc3BhY2UoMCwgMTAwLCAxMDAwKQpkYXRhID0gbnAuc2luKHRpbWVfc3RlcHMpCgojIFByZXBhcmUgdGhlIGRhdGFzZXQKZGVmIGNyZWF0ZV9kYXRhc2V0KGRhdGEsIHRpbWVfc3RlcD0xMCk6CiAgICBYLCB5ID0gW10sIFtdCiAgICBmb3IgaSBpbiByYW5nZShsZW4oZGF0YSkgLSB0aW1lX3N0ZXAgLSAxKToKICAgICAgICBYLmFwcGVuZChkYXRhW2k6KGkgKyB0aW1lX3N0ZXApXSkKICAgICAgICB5LmFwcGVuZChkYXRhW2kgKyB0aW1lX3N0ZXBdKQogICAgcmV0dXJuIG5wLmFycmF5KFgpLCBucC5hcnJheSh5KQoKdGltZV9zdGVwID0gMTAKWCwgeSA9IGNyZWF0ZV9kYXRhc2V0KGRhdGEsIHRpbWVfc3RlcCkKCiMgUmVzaGFwZSBpbnB1dCB0byBiZSBbc2FtcGxlcywgdGltZSBzdGVwcywgZmVhdHVyZXNdClggPSBYLnJlc2hhcGUoWC5zaGFwZVswXSwgWC5zaGFwZVsxXSwgMSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Generate a sine wave
time_steps = np.linspace(0, 100, 1000)
data = np.sin(time_steps)

# Prepare the dataset
def create_dataset(data, time_step=10):
    X, y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step)])
        y.append(data[i + time_step])
    return np.array(X), np.array(y)

time_step = 10
X, y = create_dataset(data, time_step)

# Reshape input to be [samples, time steps, features]
X = X.reshape(X.shape[0], X.shape[1], 1)</pre></div><div class='content'></div><h2>Step 3: Build the LSTM Model</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("bW9kZWwgPSB0Zi5rZXJhcy5TZXF1ZW50aWFsKFsKICAgIHRmLmtlcmFzLmxheWVycy5MU1RNKDUwLCByZXR1cm5fc2VxdWVuY2VzPVRydWUsIGlucHV0X3NoYXBlPSh0aW1lX3N0ZXAsIDEpKSwKICAgIHRmLmtlcmFzLmxheWVycy5MU1RNKDUwLCByZXR1cm5fc2VxdWVuY2VzPUZhbHNlKSwKICAgIHRmLmtlcmFzLmxheWVycy5EZW5zZSgxKQpdKQoKbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9J2FkYW0nLCBsb3NzPSdtZWFuX3NxdWFyZWRfZXJyb3InKQptb2RlbC5zdW1tYXJ5KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>model = tf.keras.Sequential([
    tf.keras.layers.LSTM(50, return_sequences=True, input_shape=(time_step, 1)),
    tf.keras.layers.LSTM(50, return_sequences=False),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mean_squared_error')
model.summary()</pre></div><div class='content'></div><h2>Step 4: Train the Model</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("bW9kZWwuZml0KFgsIHksIGVwb2Nocz0yMCwgYmF0Y2hfc2l6ZT0zMiwgdmVyYm9zZT0xKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>model.fit(X, y, epochs=20, batch_size=32, verbose=1)</pre></div><div class='content'></div><h2>Step 5: Make Predictions</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBNYWtlIHByZWRpY3Rpb25zCnByZWRpY3Rpb25zID0gbW9kZWwucHJlZGljdChYKQoKIyBQbG90IHRoZSByZXN1bHRzCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCnBsdC5wbG90KHRpbWVfc3RlcHNbdGltZV9zdGVwOl0sIGRhdGFbdGltZV9zdGVwOl0sIGxhYmVsPSdUcnVlIERhdGEnKQpwbHQucGxvdCh0aW1lX3N0ZXBzW3RpbWVfc3RlcDpdLCBwcmVkaWN0aW9ucywgbGFiZWw9J1ByZWRpY3Rpb25zJykKcGx0LmxlZ2VuZCgpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Make predictions
predictions = model.predict(X)

# Plot the results
import matplotlib.pyplot as plt

plt.plot(time_steps[time_step:], data[time_step:], label='True Data')
plt.plot(time_steps[time_step:], predictions, label='Predictions')
plt.legend()
plt.show()</pre></div><div class='content'></div><h1>Practical Exercises</h1>
<div class='content'></div><h2>Exercise 1: Modify the Dataset</h2>
<div class='content'><p>Modify the dataset to use a cosine wave instead of a sine wave. Train the LSTM model on this new dataset and plot the results.</p>
</div><h2>Exercise 2: Hyperparameter Tuning</h2>
<div class='content'><p>Experiment with different hyperparameters such as the number of LSTM units, the number of layers, and the batch size. Observe how these changes affect the model's performance.</p>
</div><h2>Exercise 3: Sequence Prediction</h2>
<div class='content'><p>Create a dataset where each sequence is a series of increasing numbers (e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]). Train an LSTM model to predict the next number in the sequence.</p>
</div><h1>Common Mistakes and Tips</h1>
<div class='content'><ul>
<li><strong>Overfitting</strong>: LSTM models can easily overfit, especially with small datasets. Use techniques like dropout and regularization to mitigate this.</li>
<li><strong>Data Scaling</strong>: Ensure that your data is properly scaled. LSTMs perform better with normalized data.</li>
<li><strong>Sequence Length</strong>: The choice of sequence length (time steps) can significantly impact the model's performance. Experiment with different lengths to find the optimal value.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we explored Long Short-Term Memory (LSTM) networks, their structure, and how they work. We also implemented a simple LSTM model using TensorFlow and provided practical exercises to reinforce the concepts. In the next section, we will delve into Gated Recurrent Units (GRUs), another type of RNN that addresses some of the limitations of LSTMs.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='06-02-building-an-rnn' title="Building an RNN">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='06-04-gated-recurrent-units' title="Gated Recurrent Units (GRUs)">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
