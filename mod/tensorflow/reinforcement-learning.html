<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning with TensorFlow</title>

    <link rel="alternate" href="https://campusempresa.com/mod/tensorflow/reinforcement-learning" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/tensorflow/reinforcement-learning" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/tensorflow/reinforcement-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/tensorflow/reinforcement-learning" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/tensorflow/reinforcement-learning" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-4'>
					<a href='autoencoders'>&#x25C4;Autoencoders</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Reinforcement Learning with TensorFlow</a>
	</div>
	<div class='col-4 text-end'>
					<a href='image-classification-project'>Image Classification Project &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction to Reinforcement Learning</h1>
<div class='content'><p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward.</p>
<ul>
<li><strong>Agent</strong>: The learner or decision maker.</li>
<li><strong>Environment</strong>: The external system with which the agent interacts.</li>
<li><strong>Action</strong>: What the agent can do.</li>
<li><strong>State</strong>: The current situation of the agent.</li>
<li><strong>Reward</strong>: The feedback from the environment based on the action taken.</li>
</ul>
</div><h1>Key Concepts in Reinforcement Learning</h1>
<div class='content'><ul>
<li><strong>Policy</strong>: A strategy used by the agent to decide which actions to take.</li>
<li><strong>Value Function</strong>: A function that estimates the expected reward of states or state-action pairs.</li>
<li><strong>Model</strong>: The agent's representation of the environment.</li>
</ul>
</div><h1>Setting Up TensorFlow for Reinforcement Learning</h1>
<div class='content'><p>First, ensure you have TensorFlow installed. You can install it using pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdGVuc29yZmxvdw=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install tensorflow</pre></div><div class='content'></div><h1>Building a Simple RL Model with TensorFlow</h1>
<h2>Step 1: Define the Environment</h2>
<div class='content'><p>We'll use OpenAI's Gym library to create a simple environment. Install Gym using pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgZ3lt"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install gym</pre></div><div class='content'></div><h2>Step 2: Create the Environment</h2>
<div class='content'><p>Here's an example of creating a simple CartPole environment:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQoKZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKc3RhdGUgPSBlbnYucmVzZXQoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym

env = gym.make('CartPole-v1')
state = env.reset()</pre></div><div class='content'></div><h2>Step 3: Define the Neural Network</h2>
<div class='content'><p>We'll use TensorFlow to define a simple neural network that will act as our policy.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzIGltcG9ydCBsYXllcnMKCm1vZGVsID0gdGYua2VyYXMuU2VxdWVudGlhbChbCiAgICBsYXllcnMuRGVuc2UoMjQsIGFjdGl2YXRpb249J3JlbHUnLCBpbnB1dF9zaGFwZT0oZW52Lm9ic2VydmF0aW9uX3NwYWNlLnNoYXBlWzBdLCkpLAogICAgbGF5ZXJzLkRlbnNlKDI0LCBhY3RpdmF0aW9uPSdyZWx1JyksCiAgICBsYXllcnMuRGVuc2UoZW52LmFjdGlvbl9zcGFjZS5uLCBhY3RpdmF0aW9uPSdsaW5lYXInKQpdKQoKbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9dGYua2VyYXMub3B0aW1pemVycy5BZGFtKGxlYXJuaW5nX3JhdGU9MC4wMDEpLCBsb3NzPSdtc2UnKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential([
    layers.Dense(24, activation='relu', input_shape=(env.observation_space.shape[0],)),
    layers.Dense(24, activation='relu'),
    layers.Dense(env.action_space.n, activation='linear')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')</pre></div><div class='content'></div><h2>Step 4: Implement the Training Loop</h2>
<div class='content'><p>We'll implement a simple training loop where the agent interacts with the environment and learns from the rewards.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgdHJhaW5fbW9kZWwoZW52LCBtb2RlbCwgZXBpc29kZXM9MTAwMCwgZ2FtbWE9MC45OSk6CiAgICBmb3IgZXBpc29kZSBpbiByYW5nZShlcGlzb2Rlcyk6CiAgICAgICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgICAgIHRvdGFsX3Jld2FyZCA9IDAKICAgICAgICBkb25lID0gRmFsc2UKCiAgICAgICAgd2hpbGUgbm90IGRvbmU6CiAgICAgICAgICAgIHN0YXRlID0gc3RhdGUucmVzaGFwZShbMSwgc3RhdGUuc2hhcGVbMF1dKQogICAgICAgICAgICBxX3ZhbHVlcyA9IG1vZGVsLnByZWRpY3Qoc3RhdGUpCiAgICAgICAgICAgIGFjdGlvbiA9IG5wLmFyZ21heChxX3ZhbHVlc1swXSkKCiAgICAgICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICAgICAgdG90YWxfcmV3YXJkICs9IHJld2FyZAoKICAgICAgICAgICAgbmV4dF9zdGF0ZSA9IG5leHRfc3RhdGUucmVzaGFwZShbMSwgbmV4dF9zdGF0ZS5zaGFwZVswXV0pCiAgICAgICAgICAgIHFfdmFsdWVzX25leHQgPSBtb2RlbC5wcmVkaWN0KG5leHRfc3RhdGUpCiAgICAgICAgICAgIHFfdmFsdWVzWzBdW2FjdGlvbl0gPSByZXdhcmQgKyBnYW1tYSAqIG5wLm1heChxX3ZhbHVlc19uZXh0WzBdKQoKICAgICAgICAgICAgbW9kZWwuZml0KHN0YXRlLCBxX3ZhbHVlcywgdmVyYm9zZT0wKQoKICAgICAgICAgICAgc3RhdGUgPSBuZXh0X3N0YXRlCgogICAgICAgIHByaW50KGYiRXBpc29kZToge2VwaXNvZGUgKyAxfSwgVG90YWwgUmV3YXJkOiB7dG90YWxfcmV3YXJkfSIpCgplbnYgPSBneW0ubWFrZSgnQ2FydFBvbGUtdjEnKQp0cmFpbl9tb2RlbChlbnYsIG1vZGVsKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def train_model(env, model, episodes=1000, gamma=0.99):
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        done = False

        while not done:
            state = state.reshape([1, state.shape[0]])
            q_values = model.predict(state)
            action = np.argmax(q_values[0])

            next_state, reward, done, _ = env.step(action)
            total_reward += reward

            next_state = next_state.reshape([1, next_state.shape[0]])
            q_values_next = model.predict(next_state)
            q_values[0][action] = reward + gamma * np.max(q_values_next[0])

            model.fit(state, q_values, verbose=0)

            state = next_state

        print(f&quot;Episode: {episode + 1}, Total Reward: {total_reward}&quot;)

env = gym.make('CartPole-v1')
train_model(env, model)</pre></div><div class='content'></div><h1>Advanced Topics in Reinforcement Learning</h1>
<h2>Deep Q-Learning</h2>
<div class='content'><p>Deep Q-Learning (DQN) uses a neural network to approximate the Q-value function.</p>
<ul>
<li><strong>Experience Replay</strong>: Stores the agent's experiences to break the correlation between consecutive samples.</li>
<li><strong>Target Network</strong>: A separate network to stabilize training.</li>
</ul>
</div><h2>Policy Gradient Methods</h2>
<div class='content'><p>Policy gradient methods optimize the policy directly.</p>
<ul>
<li><strong>REINFORCE Algorithm</strong>: A Monte Carlo policy gradient method.</li>
<li><strong>Actor-Critic Methods</strong>: Combines value-based and policy-based methods.</li>
</ul>
</div><h2>Example of DQN with TensorFlow</h2>
<div class='content'><p>Here's a more advanced example using DQN with experience replay and a target network.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHJhbmRvbQpmcm9tIGNvbGxlY3Rpb25zIGltcG9ydCBkZXF1ZQoKY2xhc3MgRFFOQWdlbnQ6CiAgICBkZWYgX19pbml0X18oc2VsZiwgc3RhdGVfc2l6ZSwgYWN0aW9uX3NpemUpOgogICAgICAgIHNlbGYuc3RhdGVfc2l6ZSA9IHN0YXRlX3NpemUKICAgICAgICBzZWxmLmFjdGlvbl9zaXplID0gYWN0aW9uX3NpemUKICAgICAgICBzZWxmLm1lbW9yeSA9IGRlcXVlKG1heGxlbj0yMDAwKQogICAgICAgIHNlbGYuZ2FtbWEgPSAwLjk1CiAgICAgICAgc2VsZi5lcHNpbG9uID0gMS4wCiAgICAgICAgc2VsZi5lcHNpbG9uX2RlY2F5ID0gMC45OTUKICAgICAgICBzZWxmLmVwc2lsb25fbWluID0gMC4wMQogICAgICAgIHNlbGYubGVhcm5pbmdfcmF0ZSA9IDAuMDAxCiAgICAgICAgc2VsZi5tb2RlbCA9IHNlbGYuX2J1aWxkX21vZGVsKCkKICAgICAgICBzZWxmLnRhcmdldF9tb2RlbCA9IHNlbGYuX2J1aWxkX21vZGVsKCkKICAgICAgICBzZWxmLnVwZGF0ZV90YXJnZXRfbW9kZWwoKQoKICAgIGRlZiBfYnVpbGRfbW9kZWwoc2VsZik6CiAgICAgICAgbW9kZWwgPSB0Zi5rZXJhcy5TZXF1ZW50aWFsKFsKICAgICAgICAgICAgbGF5ZXJzLkRlbnNlKDI0LCBhY3RpdmF0aW9uPSdyZWx1JywgaW5wdXRfc2hhcGU9KHNlbGYuc3RhdGVfc2l6ZSwpKSwKICAgICAgICAgICAgbGF5ZXJzLkRlbnNlKDI0LCBhY3RpdmF0aW9uPSdyZWx1JyksCiAgICAgICAgICAgIGxheWVycy5EZW5zZShzZWxmLmFjdGlvbl9zaXplLCBhY3RpdmF0aW9uPSdsaW5lYXInKQogICAgICAgIF0pCiAgICAgICAgbW9kZWwuY29tcGlsZShvcHRpbWl6ZXI9dGYua2VyYXMub3B0aW1pemVycy5BZGFtKGxlYXJuaW5nX3JhdGU9c2VsZi5sZWFybmluZ19yYXRlKSwgbG9zcz0nbXNlJykKICAgICAgICByZXR1cm4gbW9kZWwKCiAgICBkZWYgdXBkYXRlX3RhcmdldF9tb2RlbChzZWxmKToKICAgICAgICBzZWxmLnRhcmdldF9tb2RlbC5zZXRfd2VpZ2h0cyhzZWxmLm1vZGVsLmdldF93ZWlnaHRzKCkpCgogICAgZGVmIHJlbWVtYmVyKHNlbGYsIHN0YXRlLCBhY3Rpb24sIHJld2FyZCwgbmV4dF9zdGF0ZSwgZG9uZSk6CiAgICAgICAgc2VsZi5tZW1vcnkuYXBwZW5kKChzdGF0ZSwgYWN0aW9uLCByZXdhcmQsIG5leHRfc3RhdGUsIGRvbmUpKQoKICAgIGRlZiBhY3Qoc2VsZiwgc3RhdGUpOgogICAgICAgIGlmIG5wLnJhbmRvbS5yYW5kKCkgPD0gc2VsZi5lcHNpbG9uOgogICAgICAgICAgICByZXR1cm4gcmFuZG9tLnJhbmRyYW5nZShzZWxmLmFjdGlvbl9zaXplKQogICAgICAgIHFfdmFsdWVzID0gc2VsZi5tb2RlbC5wcmVkaWN0KHN0YXRlKQogICAgICAgIHJldHVybiBucC5hcmdtYXgocV92YWx1ZXNbMF0pCgogICAgZGVmIHJlcGxheShzZWxmLCBiYXRjaF9zaXplKToKICAgICAgICBtaW5pYmF0Y2ggPSByYW5kb20uc2FtcGxlKHNlbGYubWVtb3J5LCBiYXRjaF9zaXplKQogICAgICAgIGZvciBzdGF0ZSwgYWN0aW9uLCByZXdhcmQsIG5leHRfc3RhdGUsIGRvbmUgaW4gbWluaWJhdGNoOgogICAgICAgICAgICB0YXJnZXQgPSByZXdhcmQKICAgICAgICAgICAgaWYgbm90IGRvbmU6CiAgICAgICAgICAgICAgICB0YXJnZXQgPSByZXdhcmQgKyBzZWxmLmdhbW1hICogbnAuYW1heChzZWxmLnRhcmdldF9tb2RlbC5wcmVkaWN0KG5leHRfc3RhdGUpWzBdKQogICAgICAgICAgICB0YXJnZXRfZiA9IHNlbGYubW9kZWwucHJlZGljdChzdGF0ZSkKICAgICAgICAgICAgdGFyZ2V0X2ZbMF1bYWN0aW9uXSA9IHRhcmdldAogICAgICAgICAgICBzZWxmLm1vZGVsLmZpdChzdGF0ZSwgdGFyZ2V0X2YsIGVwb2Nocz0xLCB2ZXJib3NlPTApCiAgICAgICAgaWYgc2VsZi5lcHNpbG9uID4gc2VsZi5lcHNpbG9uX21pbjoKICAgICAgICAgICAgc2VsZi5lcHNpbG9uICo9IHNlbGYuZXBzaWxvbl9kZWNheQoKICAgIGRlZiBsb2FkKHNlbGYsIG5hbWUpOgogICAgICAgIHNlbGYubW9kZWwubG9hZF93ZWlnaHRzKG5hbWUpCgogICAgZGVmIHNhdmUoc2VsZiwgbmFtZSk6CiAgICAgICAgc2VsZi5tb2RlbC5zYXZlX3dlaWdodHMobmFtZSkKCmVudiA9IGd5bS5tYWtlKCdDYXJ0UG9sZS12MScpCnN0YXRlX3NpemUgPSBlbnYub2JzZXJ2YXRpb25fc3BhY2Uuc2hhcGVbMF0KYWN0aW9uX3NpemUgPSBlbnYuYWN0aW9uX3NwYWNlLm4KYWdlbnQgPSBEUU5BZ2VudChzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSkKZXBpc29kZXMgPSAxMDAwCmJhdGNoX3NpemUgPSAzMgoKZm9yIGUgaW4gcmFuZ2UoZXBpc29kZXMpOgogICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgc3RhdGUgPSBucC5yZXNoYXBlKHN0YXRlLCBbMSwgc3RhdGVfc2l6ZV0pCiAgICBmb3IgdGltZSBpbiByYW5nZSg1MDApOgogICAgICAgIGFjdGlvbiA9IGFnZW50LmFjdChzdGF0ZSkKICAgICAgICBuZXh0X3N0YXRlLCByZXdhcmQsIGRvbmUsIF8gPSBlbnYuc3RlcChhY3Rpb24pCiAgICAgICAgcmV3YXJkID0gcmV3YXJkIGlmIG5vdCBkb25lIGVsc2UgLTEwCiAgICAgICAgbmV4dF9zdGF0ZSA9IG5wLnJlc2hhcGUobmV4dF9zdGF0ZSwgWzEsIHN0YXRlX3NpemVdKQogICAgICAgIGFnZW50LnJlbWVtYmVyKHN0YXRlLCBhY3Rpb24sIHJld2FyZCwgbmV4dF9zdGF0ZSwgZG9uZSkKICAgICAgICBzdGF0ZSA9IG5leHRfc3RhdGUKICAgICAgICBpZiBkb25lOgogICAgICAgICAgICBhZ2VudC51cGRhdGVfdGFyZ2V0X21vZGVsKCkKICAgICAgICAgICAgcHJpbnQoZiJFcGlzb2RlOiB7ZSArIDF9L3tlcGlzb2Rlc30sIFNjb3JlOiB7dGltZX0sIEVwc2lsb246IHthZ2VudC5lcHNpbG9uOi4yfSIpCiAgICAgICAgICAgIGJyZWFrCiAgICAgICAgaWYgbGVuKGFnZW50Lm1lbW9yeSkgPiBiYXRjaF9zaXplOgogICAgICAgICAgICBhZ2VudC5yZXBsYXkoYmF0Y2hfc2l6ZSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import random
from collections import deque

class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        self.gamma = 0.95
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()

    def _build_model(self):
        model = tf.keras.Sequential([
            layers.Dense(24, activation='relu', input_shape=(self.state_size,)),
            layers.Dense(24, activation='relu'),
            layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate), loss='mse')
        return model

    def update_target_model(self):
        self.target_model.set_weights(self.model.get_weights())

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        if np.random.rand() &lt;= self.epsilon:
            return random.randrange(self.action_size)
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def replay(self, batch_size):
        minibatch = random.sample(self.memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs=1, verbose=0)
        if self.epsilon &gt; self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def load(self, name):
        self.model.load_weights(name)

    def save(self, name):
        self.model.save_weights(name)

env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
agent = DQNAgent(state_size, action_size)
episodes = 1000
batch_size = 32

for e in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    for time in range(500):
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        reward = reward if not done else -10
        next_state = np.reshape(next_state, [1, state_size])
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        if done:
            agent.update_target_model()
            print(f&quot;Episode: {e + 1}/{episodes}, Score: {time}, Epsilon: {agent.epsilon:.2}&quot;)
            break
        if len(agent.memory) &gt; batch_size:
            agent.replay(batch_size)</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Reinforcement Learning is a powerful paradigm for training agents to make decisions. TensorFlow provides robust tools to implement both simple and advanced RL algorithms. By understanding the key concepts and following structured examples, you can build and train your own RL models. Continue exploring more complex environments and algorithms to deepen your understanding and skills in RL with TensorFlow.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='autoencoders'>&#x25C4;Autoencoders</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Reinforcement Learning with TensorFlow</a>
	</div>
	<div class='col-4 text-end'>
					<a href='image-classification-project'>Image Classification Project &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
