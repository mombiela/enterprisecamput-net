<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Optimizing for Inference</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/optimizing-for-inference" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/optimizing-for-inference" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/optimizing-for-inference" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/optimizing-for-inference" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/optimizing-for-inference" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='serving-pytorch-models'>&#x25C4;Serving PyTorch Models</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Optimizing for Inference</a>
	</div>
	<div class='col-4 text-end'>
					<a href='deploying-on-cloud-platforms'>Deploying on Cloud Platforms &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Inference is the process of using a trained model to make predictions on new data. Optimizing for inference involves making your model run faster and more efficiently during this phase. This is crucial for deploying models in production environments where performance and resource utilization are critical.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ul>
<li><strong>Model Quantization</strong>: Reducing the precision of the model's weights and activations to lower bit-widths (e.g., from 32-bit floating point to 8-bit integer).</li>
<li><strong>Pruning</strong>: Removing less important weights from the model to reduce its size and improve inference speed.</li>
<li><strong>Batching</strong>: Processing multiple inputs at once to take advantage of parallelism.</li>
<li><strong>Hardware Acceleration</strong>: Utilizing specialized hardware like GPUs, TPUs, or FPGAs to speed up inference.</li>
<li><strong>Model Export</strong>: Converting the model to a format that is optimized for inference, such as TorchScript or ONNX.</li>
</ul>
</div><h1>Model Quantization</h1>
<div class='content'><p>Quantization can significantly reduce the model size and improve inference speed with minimal loss in accuracy.</p>
</div><h2>Example: Post-Training Quantization</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaHZpc2lvbi5tb2RlbHMgYXMgbW9kZWxzCmZyb20gdG9yY2gucXVhbnRpemF0aW9uIGltcG9ydCBxdWFudGl6ZV9keW5hbWljCgojIExvYWQgYSBwcmUtdHJhaW5lZCBtb2RlbAptb2RlbCA9IG1vZGVscy5yZXNuZXQxOChwcmV0cmFpbmVkPVRydWUpCgojIEFwcGx5IGR5bmFtaWMgcXVhbnRpemF0aW9uCnF1YW50aXplZF9tb2RlbCA9IHF1YW50aXplX2R5bmFtaWMobW9kZWwsIHt0b3JjaC5ubi5MaW5lYXJ9LCBkdHlwZT10b3JjaC5xaW50OCkKCiMgU2F2ZSB0aGUgcXVhbnRpemVkIG1vZGVsCnRvcmNoLnNhdmUocXVhbnRpemVkX21vZGVsLnN0YXRlX2RpY3QoKSwgJ3F1YW50aXplZF9yZXNuZXQxOC5wdGgnKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torchvision.models as models
from torch.quantization import quantize_dynamic

# Load a pre-trained model
model = models.resnet18(pretrained=True)

# Apply dynamic quantization
quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# Save the quantized model
torch.save(quantized_model.state_dict(), 'quantized_resnet18.pth')</pre></div><div class='content'><ul>
<li><strong>Explanation</strong>: This example demonstrates how to apply dynamic quantization to a pre-trained ResNet-18 model. The <code>quantize_dynamic</code> function reduces the precision of the linear layers to 8-bit integers.</li>
</ul>
</div><h1>Pruning</h1>
<div class='content'><p>Pruning involves removing weights that contribute less to the model's predictions, thus reducing the model's complexity.</p>
</div><h2>Example: Basic Weight Pruning</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi51dGlscy5wcnVuZSBhcyBwcnVuZQoKIyBEZWZpbmUgYSBzaW1wbGUgbW9kZWwKbW9kZWwgPSB0b3JjaC5ubi5TZXF1ZW50aWFsKAogICAgdG9yY2gubm4uTGluZWFyKDEwLCAxMCksCiAgICB0b3JjaC5ubi5SZUxVKCksCiAgICB0b3JjaC5ubi5MaW5lYXIoMTAsIDEpCikKCiMgQXBwbHkgcHJ1bmluZyB0byB0aGUgZmlyc3QgbGluZWFyIGxheWVyCnBydW5lLmwxX3Vuc3RydWN0dXJlZChtb2RlbFswXSwgbmFtZT0nd2VpZ2h0JywgYW1vdW50PTAuNCkKCiMgQ2hlY2sgdGhlIHNwYXJzaXR5IG9mIHRoZSBwcnVuZWQgbGF5ZXIKcHJpbnQobW9kZWxbMF0ud2VpZ2h0KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn.utils.prune as prune

# Define a simple model
model = torch.nn.Sequential(
    torch.nn.Linear(10, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

# Apply pruning to the first linear layer
prune.l1_unstructured(model[0], name='weight', amount=0.4)

# Check the sparsity of the pruned layer
print(model[0].weight)</pre></div><div class='content'><ul>
<li><strong>Explanation</strong>: This example shows how to prune 40% of the weights in the first linear layer of a simple neural network using L1 unstructured pruning.</li>
</ul>
</div><h1>Batching</h1>
<div class='content'><p>Batching allows you to process multiple inputs simultaneously, which can significantly speed up inference, especially on GPUs.</p>
</div><h2>Example: Batching Inputs</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIER1bW15IGlucHV0IGRhdGEKaW5wdXRzID0gdG9yY2gucmFuZG4oMzIsIDMsIDIyNCwgMjI0KSAgIyBCYXRjaCBvZiAzMiBpbWFnZXMKCiMgTG9hZCBhIHByZS10cmFpbmVkIG1vZGVsCm1vZGVsID0gbW9kZWxzLnJlc25ldDE4KHByZXRyYWluZWQ9VHJ1ZSkKbW9kZWwuZXZhbCgpCgojIFBlcmZvcm0gaW5mZXJlbmNlCndpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgb3V0cHV0cyA9IG1vZGVsKGlucHV0cykKCnByaW50KG91dHB1dHMuc2hhcGUp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Dummy input data
inputs = torch.randn(32, 3, 224, 224)  # Batch of 32 images

# Load a pre-trained model
model = models.resnet18(pretrained=True)
model.eval()

# Perform inference
with torch.no_grad():
    outputs = model(inputs)

print(outputs.shape)</pre></div><div class='content'><ul>
<li><strong>Explanation</strong>: This example demonstrates how to perform inference on a batch of 32 images using a pre-trained ResNet-18 model. The <code>torch.no_grad()</code> context is used to disable gradient computation, which is unnecessary for inference and saves memory.</li>
</ul>
</div><h1>Hardware Acceleration</h1>
<div class='content'><p>Utilizing specialized hardware can greatly enhance inference performance. PyTorch supports running models on GPUs and other accelerators.</p>
</div><h2>Example: Using GPU for Inference</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENoZWNrIGlmIEdQVSBpcyBhdmFpbGFibGUKZGV2aWNlID0gdG9yY2guZGV2aWNlKCdjdWRhJyBpZiB0b3JjaC5jdWRhLmlzX2F2YWlsYWJsZSgpIGVsc2UgJ2NwdScpCgojIExvYWQgYSBwcmUtdHJhaW5lZCBtb2RlbCBhbmQgbW92ZSBpdCB0byB0aGUgR1BVCm1vZGVsID0gbW9kZWxzLnJlc25ldDE4KHByZXRyYWluZWQ9VHJ1ZSkudG8oZGV2aWNlKQptb2RlbC5ldmFsKCkKCiMgRHVtbXkgaW5wdXQgZGF0YQppbnB1dHMgPSB0b3JjaC5yYW5kbigzMiwgMywgMjI0LCAyMjQpLnRvKGRldmljZSkgICMgQmF0Y2ggb2YgMzIgaW1hZ2VzCgojIFBlcmZvcm0gaW5mZXJlbmNlCndpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgb3V0cHV0cyA9IG1vZGVsKGlucHV0cykKCnByaW50KG91dHB1dHMuc2hhcGUp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Check if GPU is available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load a pre-trained model and move it to the GPU
model = models.resnet18(pretrained=True).to(device)
model.eval()

# Dummy input data
inputs = torch.randn(32, 3, 224, 224).to(device)  # Batch of 32 images

# Perform inference
with torch.no_grad():
    outputs = model(inputs)

print(outputs.shape)</pre></div><div class='content'><ul>
<li><strong>Explanation</strong>: This example shows how to move a model and input data to the GPU for faster inference. The <code>torch.cuda.is_available()</code> function checks if a GPU is available.</li>
</ul>
</div><h1>Model Export</h1>
<div class='content'><p>Exporting your model to a format optimized for inference can improve performance and compatibility with various deployment environments.</p>
</div><h2>Example: Exporting to TorchScript</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIExvYWQgYSBwcmUtdHJhaW5lZCBtb2RlbAptb2RlbCA9IG1vZGVscy5yZXNuZXQxOChwcmV0cmFpbmVkPVRydWUpCm1vZGVsLmV2YWwoKQoKIyBDb252ZXJ0IHRoZSBtb2RlbCB0byBUb3JjaFNjcmlwdApzY3JpcHRlZF9tb2RlbCA9IHRvcmNoLmppdC5zY3JpcHQobW9kZWwpCgojIFNhdmUgdGhlIFRvcmNoU2NyaXB0IG1vZGVsCnNjcmlwdGVkX21vZGVsLnNhdmUoJ3Jlc25ldDE4X3NjcmlwdGVkLnB0Jyk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Load a pre-trained model
model = models.resnet18(pretrained=True)
model.eval()

# Convert the model to TorchScript
scripted_model = torch.jit.script(model)

# Save the TorchScript model
scripted_model.save('resnet18_scripted.pt')</pre></div><div class='content'><ul>
<li><strong>Explanation</strong>: This example demonstrates how to convert a pre-trained ResNet-18 model to TorchScript, which is a format optimized for inference. The <code>torch.jit.script</code> function is used for this conversion.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>Optimizing for inference is a critical step in deploying machine learning models in production. Techniques such as quantization, pruning, batching, hardware acceleration, and model export can significantly enhance the performance and efficiency of your models. By understanding and applying these techniques, you can ensure that your models run faster and more efficiently, making them more suitable for real-world applications.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='serving-pytorch-models'>&#x25C4;Serving PyTorch Models</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Optimizing for Inference</a>
	</div>
	<div class='col-4 text-end'>
					<a href='deploying-on-cloud-platforms'>Deploying on Cloud Platforms &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
