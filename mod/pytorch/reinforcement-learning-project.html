<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning Project</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/reinforcement-learning-project" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/reinforcement-learning-project" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/reinforcement-learning-project" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/reinforcement-learning-project" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/reinforcement-learning-project" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='time-series-forecasting-project'>&#x25C4;Time Series Forecasting Project</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Reinforcement Learning Project</a>
	</div>
	<div class='col-4 text-end'>
					<a href='distributed-training'>Distributed Training &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction to Reinforcement Learning</h1>
<div class='content'><p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Agent</strong>: The learner or decision maker.</li>
<li><strong>Environment</strong>: The external system with which the agent interacts.</li>
<li><strong>State</strong>: A representation of the current situation of the agent.</li>
<li><strong>Action</strong>: A set of all possible moves the agent can make.</li>
<li><strong>Reward</strong>: Feedback from the environment to evaluate the action.</li>
<li><strong>Policy</strong>: A strategy used by the agent to decide actions based on the state.</li>
<li><strong>Value Function</strong>: A prediction of future rewards.</li>
</ul>
</div><h1>Setting Up the Environment</h1>
<div class='content'><p>Before diving into RL, ensure you have PyTorch installed. You can install PyTorch using the following command:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdG9yY2ggdG9yY2h2aXNpb24="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install torch torchvision</pre></div><div class='content'></div><h1>Building a Simple RL Model with PyTorch</h1>
<div class='content'><p>In this section, we will build a simple RL model using PyTorch. We will use the OpenAI Gym environment for this purpose.</p>
</div><h2>Step 1: Installing OpenAI Gym</h2>
<div class='content'><p>First, install the OpenAI Gym library:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgZ3lt"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install gym</pre></div><div class='content'></div><h2>Step 2: Creating the Environment</h2>
<div class='content'><p>Let's create a simple environment using OpenAI Gym:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQoKZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKc3RhdGUgPSBlbnYucmVzZXQoKQpwcmludChzdGF0ZSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym

env = gym.make('CartPole-v1')
state = env.reset()
print(state)</pre></div><div class='content'></div><h2>Step 3: Defining the Neural Network</h2>
<div class='content'><p>We will define a simple neural network to approximate the Q-value function.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KCmNsYXNzIFFOZXR3b3JrKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgc3RhdGVfc2l6ZSwgYWN0aW9uX3NpemUpOgogICAgICAgIHN1cGVyKFFOZXR3b3JrLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5mYzEgPSBubi5MaW5lYXIoc3RhdGVfc2l6ZSwgMjQpCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoMjQsIDI0KQogICAgICAgIHNlbGYuZmMzID0gbm4uTGluZWFyKDI0LCBhY3Rpb25fc2l6ZSkKCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMih4KSkKICAgICAgICB4ID0gc2VsZi5mYzMoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim

class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 24)
        self.fc2 = nn.Linear(24, 24)
        self.fc3 = nn.Linear(24, action_size)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x</pre></div><div class='content'></div><h2>Step 4: Training the Agent</h2>
<div class='content'><p>We will now train the agent using the Q-learning algorithm.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def train(env, q_network, episodes=1000, gamma=0.99, epsilon=0.1, lr=0.001):
    optimizer = optim.Adam(q_network.parameters(), lr=lr)
    criterion = nn.MSELoss()

    for episode in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0

        while not done:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q_values = q_network(state_tensor)
            if np.random.rand() &lt; epsilon:
                action = env.action_space.sample()
            else:
                action = torch.argmax(q_values).item()

            next_state, reward, done, _ = env.step(action)
            total_reward += reward

            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)
            next_q_values = q_network(next_state_tensor)
            max_next_q_value = torch.max(next_q_values).item()

            target_q_value = reward + (gamma * max_next_q_value * (1 - done))
            target_q_values = q_values.clone()
            target_q_values[0][action] = target_q_value

            loss = criterion(q_values, target_q_values)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            state = next_state

        print(f&quot;Episode {episode + 1}: Total Reward: {total_reward}&quot;)

# Initialize and train the Q-network
state_size = env.observation_space.shape[0]
action_size = env.action_space.n
q_network = QNetwork(state_size, action_size)
train(env, q_network)</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>In this topic, we covered the basics of reinforcement learning and implemented a simple Q-learning agent using PyTorch. We started by setting up the environment, defining the neural network, and then training the agent. This foundational knowledge will help you understand more complex RL algorithms and projects in the future.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='time-series-forecasting-project'>&#x25C4;Time Series Forecasting Project</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Reinforcement Learning Project</a>
	</div>
	<div class='col-4 text-end'>
					<a href='distributed-training'>Distributed Training &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
