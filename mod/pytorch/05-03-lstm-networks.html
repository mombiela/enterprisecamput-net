<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Long Short-Term Memory (LSTM) Networks</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/05-03-lstm-networks" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/05-03-lstm-networks" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/05-03-lstm-networks" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/pytorch/05-03-lstm-networks" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/pytorch/05-03-lstm-networks" class="px-2">CA</a>
<br>
			<cite>Building today's and tomorrow's society</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Programming Languages</a>
				 					<a href="/categ/frameworks">Frameworks and Libraries</a>
				 					<a href="/categ/tech-tools">Technical Tools</a>
				 					<a href="/categ/foundations">Theoretical Foundations</a>
				 					<a href="/categ/soft-skills">Social Skills</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-02-building-rnn-from-scratch' title="Building an RNN from Scratch">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Long Short-Term Memory (LSTM) Networks</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='05-04-gru-networks' title="Gated Recurrent Units (GRUs)">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1><p>Introduction</p>
</h1>
<div class='content'><p>Long Short-Term Memory (LSTM) networks are a type of Recurrent Neural Network (RNN) designed to handle the vanishing gradient problem, which is common in traditional RNNs. LSTMs are particularly effective for tasks involving sequential data, such as time series forecasting, natural language processing, and speech recognition.</p>
</div><h2><p>Key Concepts</p>
</h2>
<div class='content'><ol>
<li><strong>Cell State</strong>: The cell state acts as a conveyor belt, running through the entire chain with minor linear interactions, allowing information to flow unchanged.</li>
<li><strong>Gates</strong>: LSTMs use gates to control the flow of information. There are three types of gates:
<ul>
<li><strong>Forget Gate</strong>: Decides what information to discard from the cell state.</li>
<li><strong>Input Gate</strong>: Decides which values from the input to update the cell state.</li>
<li><strong>Output Gate</strong>: Decides what part of the cell state to output.</li>
</ul>
</li>
</ol>
</div><h1><p>LSTM Architecture</p>
</h1>
<div class='content'></div><h2><p>Forget Gate</p>
</h2>
<div class='content'><p>The forget gate decides what information to throw away from the cell state. It uses a sigmoid function to output a number between 0 and 1 for each number in the cell state \(C_{t-1}\).</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBFeGFtcGxlIG9mIGZvcmdldCBnYXRlCmlucHV0X3NpemUgPSAxMApoaWRkZW5fc2l6ZSA9IDIwCgp4X3QgPSB0b3JjaC5yYW5kbigxLCBpbnB1dF9zaXplKSAgIyBJbnB1dCBhdCB0aW1lIHQKaF90X21pbnVzXzEgPSB0b3JjaC5yYW5kbigxLCBoaWRkZW5fc2l6ZSkgICMgSGlkZGVuIHN0YXRlIGF0IHRpbWUgdC0xCkNfdF9taW51c18xID0gdG9yY2gucmFuZG4oMSwgaGlkZGVuX3NpemUpICAjIENlbGwgc3RhdGUgYXQgdGltZSB0LTEKCmZvcmdldF9nYXRlID0gbm4uTGluZWFyKGlucHV0X3NpemUgKyBoaWRkZW5fc2l6ZSwgaGlkZGVuX3NpemUpCmNvbWJpbmVkID0gdG9yY2guY2F0KCh4X3QsIGhfdF9taW51c18xKSwgMSkKZl90ID0gdG9yY2guc2lnbW9pZChmb3JnZXRfZ2F0ZShjb21iaW5lZCkpCgpwcmludChmIkZvcmdldCBnYXRlIG91dHB1dDoge2ZfdH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

# Example of forget gate
input_size = 10
hidden_size = 20

x_t = torch.randn(1, input_size)  # Input at time t
h_t_minus_1 = torch.randn(1, hidden_size)  # Hidden state at time t-1
C_t_minus_1 = torch.randn(1, hidden_size)  # Cell state at time t-1

forget_gate = nn.Linear(input_size + hidden_size, hidden_size)
combined = torch.cat((x_t, h_t_minus_1), 1)
f_t = torch.sigmoid(forget_gate(combined))

print(f&quot;Forget gate output: {f_t}&quot;)</pre></div><div class='content'></div><h2><p>Input Gate</p>
</h2>
<div class='content'><p>The input gate updates the cell state with new information. It consists of two parts: a sigmoid layer and a tanh layer.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeGFtcGxlIG9mIGlucHV0IGdhdGUKaW5wdXRfZ2F0ZSA9IG5uLkxpbmVhcihpbnB1dF9zaXplICsgaGlkZGVuX3NpemUsIGhpZGRlbl9zaXplKQppbnB1dF90cmFuc2Zvcm0gPSBubi5MaW5lYXIoaW5wdXRfc2l6ZSArIGhpZGRlbl9zaXplLCBoaWRkZW5fc2l6ZSkKCmlfdCA9IHRvcmNoLnNpZ21vaWQoaW5wdXRfZ2F0ZShjb21iaW5lZCkpCkNfdGlsZGVfdCA9IHRvcmNoLnRhbmgoaW5wdXRfdHJhbnNmb3JtKGNvbWJpbmVkKSkKCnByaW50KGYiSW5wdXQgZ2F0ZSBvdXRwdXQ6IHtpX3R9IikKcHJpbnQoZiJJbnB1dCB0cmFuc2Zvcm0gb3V0cHV0OiB7Q190aWxkZV90fSIp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Example of input gate
input_gate = nn.Linear(input_size + hidden_size, hidden_size)
input_transform = nn.Linear(input_size + hidden_size, hidden_size)

i_t = torch.sigmoid(input_gate(combined))
C_tilde_t = torch.tanh(input_transform(combined))

print(f&quot;Input gate output: {i_t}&quot;)
print(f&quot;Input transform output: {C_tilde_t}&quot;)</pre></div><div class='content'></div><h2><p>Cell State Update</p>
</h2>
<div class='content'><p>The cell state is updated using the forget gate and the input gate.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBVcGRhdGUgY2VsbCBzdGF0ZQpDX3QgPSBmX3QgKiBDX3RfbWludXNfMSArIGlfdCAqIENfdGlsZGVfdAoKcHJpbnQoZiJVcGRhdGVkIGNlbGwgc3RhdGU6IHtDX3R9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Update cell state
C_t = f_t * C_t_minus_1 + i_t * C_tilde_t

print(f&quot;Updated cell state: {C_t}&quot;)</pre></div><div class='content'></div><h2><p>Output Gate</p>
</h2>
<div class='content'><p>The output gate decides what the next hidden state should be. It uses a sigmoid function and the updated cell state.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeGFtcGxlIG9mIG91dHB1dCBnYXRlCm91dHB1dF9nYXRlID0gbm4uTGluZWFyKGlucHV0X3NpemUgKyBoaWRkZW5fc2l6ZSwgaGlkZGVuX3NpemUpCm9fdCA9IHRvcmNoLnNpZ21vaWQob3V0cHV0X2dhdGUoY29tYmluZWQpKQpoX3QgPSBvX3QgKiB0b3JjaC50YW5oKENfdCkKCnByaW50KGYiT3V0cHV0IGdhdGUgb3V0cHV0OiB7b190fSIpCnByaW50KGYiTmV4dCBoaWRkZW4gc3RhdGU6IHtoX3R9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Example of output gate
output_gate = nn.Linear(input_size + hidden_size, hidden_size)
o_t = torch.sigmoid(output_gate(combined))
h_t = o_t * torch.tanh(C_t)

print(f&quot;Output gate output: {o_t}&quot;)
print(f&quot;Next hidden state: {h_t}&quot;)</pre></div><div class='content'></div><h1><p>Building an LSTM from Scratch</p>
</h1>
<div class='content'></div><h2><p>Step-by-Step Implementation</p>
</h2>
<div class='content'><ol>
<li><strong>Define the LSTM Class</strong>: Create a class that inherits from <code>nn.Module</code>.</li>
<li><strong>Initialize Layers</strong>: Define the forget gate, input gate, and output gate layers.</li>
<li><strong>Forward Pass</strong>: Implement the forward pass to compute the hidden state and cell state.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgTFNUTShubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGhpZGRlbl9zaXplKToKICAgICAgICBzdXBlcihMU1RNLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5oaWRkZW5fc2l6ZSA9IGhpZGRlbl9zaXplCiAgICAgICAgCiAgICAgICAgc2VsZi5mb3JnZXRfZ2F0ZSA9IG5uLkxpbmVhcihpbnB1dF9zaXplICsgaGlkZGVuX3NpemUsIGhpZGRlbl9zaXplKQogICAgICAgIHNlbGYuaW5wdXRfZ2F0ZSA9IG5uLkxpbmVhcihpbnB1dF9zaXplICsgaGlkZGVuX3NpemUsIGhpZGRlbl9zaXplKQogICAgICAgIHNlbGYuaW5wdXRfdHJhbnNmb3JtID0gbm4uTGluZWFyKGlucHV0X3NpemUgKyBoaWRkZW5fc2l6ZSwgaGlkZGVuX3NpemUpCiAgICAgICAgc2VsZi5vdXRwdXRfZ2F0ZSA9IG5uLkxpbmVhcihpbnB1dF9zaXplICsgaGlkZGVuX3NpemUsIGhpZGRlbl9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4LCBoX3ByZXYsIENfcHJldik6CiAgICAgICAgY29tYmluZWQgPSB0b3JjaC5jYXQoKHgsIGhfcHJldiksIDEpCiAgICAgICAgCiAgICAgICAgZl90ID0gdG9yY2guc2lnbW9pZChzZWxmLmZvcmdldF9nYXRlKGNvbWJpbmVkKSkKICAgICAgICBpX3QgPSB0b3JjaC5zaWdtb2lkKHNlbGYuaW5wdXRfZ2F0ZShjb21iaW5lZCkpCiAgICAgICAgQ190aWxkZV90ID0gdG9yY2gudGFuaChzZWxmLmlucHV0X3RyYW5zZm9ybShjb21iaW5lZCkpCiAgICAgICAgQ190ID0gZl90ICogQ19wcmV2ICsgaV90ICogQ190aWxkZV90CiAgICAgICAgCiAgICAgICAgb190ID0gdG9yY2guc2lnbW9pZChzZWxmLm91dHB1dF9nYXRlKGNvbWJpbmVkKSkKICAgICAgICBoX3QgPSBvX3QgKiB0b3JjaC50YW5oKENfdCkKICAgICAgICAKICAgICAgICByZXR1cm4gaF90LCBDX3QKCiMgRXhhbXBsZSB1c2FnZQppbnB1dF9zaXplID0gMTAKaGlkZGVuX3NpemUgPSAyMApsc3RtID0gTFNUTShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSkKCnhfdCA9IHRvcmNoLnJhbmRuKDEsIGlucHV0X3NpemUpCmhfdF9taW51c18xID0gdG9yY2gucmFuZG4oMSwgaGlkZGVuX3NpemUpCkNfdF9taW51c18xID0gdG9yY2gucmFuZG4oMSwgaGlkZGVuX3NpemUpCgpoX3QsIENfdCA9IGxzdG0oeF90LCBoX3RfbWludXNfMSwgQ190X21pbnVzXzEpCnByaW50KGYiTmV4dCBoaWRkZW4gc3RhdGU6IHtoX3R9IikKcHJpbnQoZiJOZXh0IGNlbGwgc3RhdGU6IHtDX3R9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        
        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)
        self.input_transform = nn.Linear(input_size + hidden_size, hidden_size)
        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)
    
    def forward(self, x, h_prev, C_prev):
        combined = torch.cat((x, h_prev), 1)
        
        f_t = torch.sigmoid(self.forget_gate(combined))
        i_t = torch.sigmoid(self.input_gate(combined))
        C_tilde_t = torch.tanh(self.input_transform(combined))
        C_t = f_t * C_prev + i_t * C_tilde_t
        
        o_t = torch.sigmoid(self.output_gate(combined))
        h_t = o_t * torch.tanh(C_t)
        
        return h_t, C_t

# Example usage
input_size = 10
hidden_size = 20
lstm = LSTM(input_size, hidden_size)

x_t = torch.randn(1, input_size)
h_t_minus_1 = torch.randn(1, hidden_size)
C_t_minus_1 = torch.randn(1, hidden_size)

h_t, C_t = lstm(x_t, h_t_minus_1, C_t_minus_1)
print(f&quot;Next hidden state: {h_t}&quot;)
print(f&quot;Next cell state: {C_t}&quot;)</pre></div><div class='content'></div><h1><p>Practical Exercise</p>
</h1>
<div class='content'></div><h2><p>Task</p>
</h2>
<div class='content'><p>Create an LSTM network to predict the next value in a simple time series.</p>
</div><h2><p>Steps</p>
</h2>
<div class='content'><ol>
<li><strong>Generate Data</strong>: Create a simple sine wave dataset.</li>
<li><strong>Define the LSTM Model</strong>: Use PyTorch's <code>nn.LSTM</code> module.</li>
<li><strong>Train the Model</strong>: Implement the training loop.</li>
<li><strong>Evaluate the Model</strong>: Predict and plot the results.</li>
</ol>
</div><h2><p>Solution</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCiMgR2VuZXJhdGUgc2luZSB3YXZlIGRhdGEKdGltZV9zdGVwcyA9IG5wLmxpbnNwYWNlKDAsIDEwMCwgMTAwMCkKZGF0YSA9IG5wLnNpbih0aW1lX3N0ZXBzKQoKIyBQcmVwYXJlIGRhdGEgZm9yIExTVE0KZGVmIGNyZWF0ZV9pbm91dF9zZXF1ZW5jZXMoaW5wdXRfZGF0YSwgdHcpOgogICAgaW5vdXRfc2VxID0gW10KICAgIEwgPSBsZW4oaW5wdXRfZGF0YSkKICAgIGZvciBpIGluIHJhbmdlKEwtdHcpOgogICAgICAgIHRyYWluX3NlcSA9IGlucHV0X2RhdGFbaTppK3R3XQogICAgICAgIHRyYWluX2xhYmVsID0gaW5wdXRfZGF0YVtpK3R3OmkrdHcrMV0KICAgICAgICBpbm91dF9zZXEuYXBwZW5kKCh0cmFpbl9zZXEsIHRyYWluX2xhYmVsKSkKICAgIHJldHVybiBpbm91dF9zZXEKCnRyYWluX3dpbmRvdyA9IDEwCnRyYWluX2lub3V0X3NlcSA9IGNyZWF0ZV9pbm91dF9zZXF1ZW5jZXMoZGF0YSwgdHJhaW5fd2luZG93KQoKIyBEZWZpbmUgTFNUTSBtb2RlbApjbGFzcyBMU1RNTW9kZWwobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBpbnB1dF9zaXplPTEsIGhpZGRlbl9sYXllcl9zaXplPTEwMCwgb3V0cHV0X3NpemU9MSk6CiAgICAgICAgc3VwZXIoTFNUTU1vZGVsLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5oaWRkZW5fbGF5ZXJfc2l6ZSA9IGhpZGRlbl9sYXllcl9zaXplCiAgICAgICAgc2VsZi5sc3RtID0gbm4uTFNUTShpbnB1dF9zaXplLCBoaWRkZW5fbGF5ZXJfc2l6ZSkKICAgICAgICBzZWxmLmxpbmVhciA9IG5uLkxpbmVhcihoaWRkZW5fbGF5ZXJfc2l6ZSwgb3V0cHV0X3NpemUpCiAgICAgICAgc2VsZi5oaWRkZW5fY2VsbCA9ICh0b3JjaC56ZXJvcygxLDEsc2VsZi5oaWRkZW5fbGF5ZXJfc2l6ZSksCiAgICAgICAgICAgICAgICAgICAgICAgICAgICB0b3JjaC56ZXJvcygxLDEsc2VsZi5oaWRkZW5fbGF5ZXJfc2l6ZSkpCgogICAgZGVmIGZvcndhcmQoc2VsZiwgaW5wdXRfc2VxKToKICAgICAgICBsc3RtX291dCwgc2VsZi5oaWRkZW5fY2VsbCA9IHNlbGYubHN0bShpbnB1dF9zZXEudmlldyhsZW4oaW5wdXRfc2VxKSAsMSwgLTEpLCBzZWxmLmhpZGRlbl9jZWxsKQogICAgICAgIHByZWRpY3Rpb25zID0gc2VsZi5saW5lYXIobHN0bV9vdXQudmlldyhsZW4oaW5wdXRfc2VxKSwgLTEpKQogICAgICAgIHJldHVybiBwcmVkaWN0aW9uc1stMV0KCm1vZGVsID0gTFNUTU1vZGVsKCkKbG9zc19mdW5jdGlvbiA9IG5uLk1TRUxvc3MoKQpvcHRpbWl6ZXIgPSB0b3JjaC5vcHRpbS5BZGFtKG1vZGVsLnBhcmFtZXRlcnMoKSwgbHI9MC4wMDEpCgojIFRyYWluaW5nIGxvb3AKZXBvY2hzID0gMTUwCmZvciBpIGluIHJhbmdlKGVwb2Nocyk6CiAgICBmb3Igc2VxLCBsYWJlbHMgaW4gdHJhaW5faW5vdXRfc2VxOgogICAgICAgIG9wdGltaXplci56ZXJvX2dyYWQoKQogICAgICAgIG1vZGVsLmhpZGRlbl9jZWxsID0gKHRvcmNoLnplcm9zKDEsIDEsIG1vZGVsLmhpZGRlbl9sYXllcl9zaXplKSwKICAgICAgICAgICAgICAgICAgICAgICAgdG9yY2guemVyb3MoMSwgMSwgbW9kZWwuaGlkZGVuX2xheWVyX3NpemUpKQoKICAgICAgICB5X3ByZWQgPSBtb2RlbChzZXEpCgogICAgICAgIHNpbmdsZV9sb3NzID0gbG9zc19mdW5jdGlvbih5X3ByZWQsIGxhYmVscykKICAgICAgICBzaW5nbGVfbG9zcy5iYWNrd2FyZCgpCiAgICAgICAgb3B0aW1pemVyLnN0ZXAoKQoKICAgIGlmIGklMjUgPT0gMToKICAgICAgICBwcmludChmJ2Vwb2NoOiB7aTozfSBsb3NzOiB7c2luZ2xlX2xvc3MuaXRlbSgpOjEwLjhmfScpCgojIFByZWRpY3QgYW5kIHBsb3QKZnV0X3ByZWQgPSAxMDAKdGVzdF9pbnB1dHMgPSBkYXRhWy10cmFpbl93aW5kb3c6XS50b2xpc3QoKQoKbW9kZWwuZXZhbCgpCgpmb3IgaSBpbiByYW5nZShmdXRfcHJlZCk6CiAgICBzZXEgPSB0b3JjaC5GbG9hdFRlbnNvcih0ZXN0X2lucHV0c1stdHJhaW5fd2luZG93Ol0pCiAgICB3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgICAgICBtb2RlbC5oaWRkZW5fY2VsbCA9ICh0b3JjaC56ZXJvcygxLCAxLCBtb2RlbC5oaWRkZW5fbGF5ZXJfc2l6ZSksCiAgICAgICAgICAgICAgICAgICAgICAgIHRvcmNoLnplcm9zKDEsIDEsIG1vZGVsLmhpZGRlbl9sYXllcl9zaXplKSkKICAgICAgICB0ZXN0X2lucHV0cy5hcHBlbmQobW9kZWwoc2VxKS5pdGVtKCkpCgp4ID0gbnAuYXJhbmdlKDEwMDAsIDEwMDArZnV0X3ByZWQsIDEpCnBsdC50aXRsZSgnU2luZSBXYXZlIFByZWRpY3Rpb24nKQpwbHQuZ3JpZChUcnVlKQpwbHQucGxvdCh0aW1lX3N0ZXBzLCBkYXRhLCBsYWJlbD0nVHJ1ZSBEYXRhJykKcGx0LnBsb3QoeCwgdGVzdF9pbnB1dHNbdHJhaW5fd2luZG93Ol0sIGxhYmVsPSdQcmVkaWN0aW9ucycpCnBsdC5sZWdlbmQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

# Generate sine wave data
time_steps = np.linspace(0, 100, 1000)
data = np.sin(time_steps)

# Prepare data for LSTM
def create_inout_sequences(input_data, tw):
    inout_seq = []
    L = len(input_data)
    for i in range(L-tw):
        train_seq = input_data[i:i+tw]
        train_label = input_data[i+tw:i+tw+1]
        inout_seq.append((train_seq, train_label))
    return inout_seq

train_window = 10
train_inout_seq = create_inout_sequences(data, train_window)

# Define LSTM model
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):
        super(LSTMModel, self).__init__()
        self.hidden_layer_size = hidden_layer_size
        self.lstm = nn.LSTM(input_size, hidden_layer_size)
        self.linear = nn.Linear(hidden_layer_size, output_size)
        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),
                            torch.zeros(1,1,self.hidden_layer_size))

    def forward(self, input_seq):
        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)
        predictions = self.linear(lstm_out.view(len(input_seq), -1))
        return predictions[-1]

model = LSTMModel()
loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
epochs = 150
for i in range(epochs):
    for seq, labels in train_inout_seq:
        optimizer.zero_grad()
        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))

        y_pred = model(seq)

        single_loss = loss_function(y_pred, labels)
        single_loss.backward()
        optimizer.step()

    if i%25 == 1:
        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')

# Predict and plot
fut_pred = 100
test_inputs = data[-train_window:].tolist()

model.eval()

for i in range(fut_pred):
    seq = torch.FloatTensor(test_inputs[-train_window:])
    with torch.no_grad():
        model.hidden_cell = (torch.zeros(1, 1, model.hidden_layer_size),
                        torch.zeros(1, 1, model.hidden_layer_size))
        test_inputs.append(model(seq).item())

x = np.arange(1000, 1000+fut_pred, 1)
plt.title('Sine Wave Prediction')
plt.grid(True)
plt.plot(time_steps, data, label='True Data')
plt.plot(x, test_inputs[train_window:], label='Predictions')
plt.legend()
plt.show()</pre></div><div class='content'></div><h1><p>Summary</p>
</h1>
<div class='content'><p>In this section, we covered the architecture and functionality of Long Short-Term Memory (LSTM) networks. We explored the different gates that control the flow of information and implemented an LSTM from scratch. Finally, we applied an LSTM to a practical time series prediction task. This knowledge prepares you for more complex sequential data tasks and sets the foundation for understanding advanced RNN architectures.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-02-building-rnn-from-scratch' title="Building an RNN from Scratch">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='05-04-gru-networks' title="Gated Recurrent Units (GRUs)">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
