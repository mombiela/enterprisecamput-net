<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deploying PyTorch Models</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/06-03-deploying-models" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/06-03-deploying-models" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/06-03-deploying-models" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=2" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/pytorch/06-03-deploying-models" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/pytorch/06-03-deploying-models" class="px-2">CA</a>
<br>
			<cite>Building today's and tomorrow's society</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Programming Languages</a>
				 					<a href="/categ/frameworks">Frameworks and Libraries</a>
				 					<a href="/categ/tech-tools">Technical Tools</a>
				 					<a href="/categ/foundations">Theoretical Foundations</a>
				 					<a href="/categ/soft-skills">Social Skills</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-02-reinforcement-learning' title="Reinforcement Learning with PyTorch">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Deploying PyTorch Models</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='06-04-optimizing-performance' title="Optimizing Performance">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Deploying a PyTorch model involves taking a trained model and making it available for inference in a production environment. This process can include exporting the model, optimizing it for performance, and integrating it into a web service or application. In this section, we will cover the following topics:</p>
<ol>
<li><strong>Exporting a PyTorch Model</strong></li>
<li><strong>Optimizing the Model for Inference</strong></li>
<li><strong>Deploying with Flask</strong></li>
<li><strong>Deploying with TorchServe</strong></li>
<li><strong>Deploying on Cloud Platforms</strong></li>
</ol>
</div><h1><ol>
<li>Exporting a PyTorch Model</li>
</ol>
</h1>
<div class='content'></div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>Serialization</strong>: Saving the model's architecture and learned parameters.</li>
<li><strong>ONNX (Open Neural Network Exchange)</strong>: A format for representing deep learning models that allows interoperability between different frameworks.</li>
</ul>
</div><h2><p>Steps to Export a Model:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Save the Model State Dict</strong>:</p>
<pre><code class="language-python">import torch

# Assuming `model` is your trained PyTorch model
torch.save(model.state_dict(), 'model.pth')
</code></pre>
</li>
<li>
<p><strong>Load the Model State Dict</strong>:</p>
<pre><code class="language-python">model = TheModelClass(*args, **kwargs)  # Initialize the model class
model.load_state_dict(torch.load('model.pth'))
model.eval()  # Set the model to evaluation mode
</code></pre>
</li>
<li>
<p><strong>Export to ONNX</strong>:</p>
<pre><code class="language-python">dummy_input = torch.randn(1, 3, 224, 224)  # Example input tensor
torch.onnx.export(model, dummy_input, &quot;model.onnx&quot;, verbose=True)
</code></pre>
</li>
</ol>
</div><h2><p>Practical Example:</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBEZWZpbmUgYSBzaW1wbGUgbW9kZWwKY2xhc3MgU2ltcGxlTW9kZWwobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmKToKICAgICAgICBzdXBlcihTaW1wbGVNb2RlbCwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuZmMgPSBubi5MaW5lYXIoMTAsIDIpCgogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgcmV0dXJuIHNlbGYuZmMoeCkKCiMgSW5pdGlhbGl6ZSBhbmQgc2F2ZSB0aGUgbW9kZWwKbW9kZWwgPSBTaW1wbGVNb2RlbCgpCnRvcmNoLnNhdmUobW9kZWwuc3RhdGVfZGljdCgpLCAnc2ltcGxlX21vZGVsLnB0aCcpCgojIExvYWQgdGhlIG1vZGVsCm1vZGVsID0gU2ltcGxlTW9kZWwoKQptb2RlbC5sb2FkX3N0YXRlX2RpY3QodG9yY2gubG9hZCgnc2ltcGxlX21vZGVsLnB0aCcpKQptb2RlbC5ldmFsKCkKCiMgRXhwb3J0IHRvIE9OTlgKZHVtbXlfaW5wdXQgPSB0b3JjaC5yYW5kbigxLCAxMCkKdG9yY2gub25ueC5leHBvcnQobW9kZWwsIGR1bW15X2lucHV0LCAic2ltcGxlX21vZGVsLm9ubngiLCB2ZXJib3NlPVRydWUp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# Initialize and save the model
model = SimpleModel()
torch.save(model.state_dict(), 'simple_model.pth')

# Load the model
model = SimpleModel()
model.load_state_dict(torch.load('simple_model.pth'))
model.eval()

# Export to ONNX
dummy_input = torch.randn(1, 10)
torch.onnx.export(model, dummy_input, &quot;simple_model.onnx&quot;, verbose=True)</pre></div><div class='content'></div><h1><ol start="2">
<li>Optimizing the Model for Inference</li>
</ol>
</h1>
<div class='content'></div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>TorchScript</strong>: A way to create serializable and optimizable models from PyTorch code.</li>
<li><strong>Quantization</strong>: Reducing the precision of the model's weights and activations to improve performance.</li>
</ul>
</div><h2><p>Steps to Optimize:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Convert to TorchScript</strong>:</p>
<pre><code class="language-python">scripted_model = torch.jit.script(model)
scripted_model.save(&quot;scripted_model.pt&quot;)
</code></pre>
</li>
<li>
<p><strong>Apply Quantization</strong>:</p>
<pre><code class="language-python">model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)
torch.quantization.convert(model, inplace=True)
</code></pre>
</li>
</ol>
</div><h2><p>Practical Example:</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gucXVhbnRpemF0aW9uCgojIERlZmluZSBhIHNpbXBsZSBtb2RlbApjbGFzcyBTaW1wbGVNb2RlbChubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKFNpbXBsZU1vZGVsLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5mYyA9IG5uLkxpbmVhcigxMCwgMikKCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICByZXR1cm4gc2VsZi5mYyh4KQoKIyBJbml0aWFsaXplIGFuZCBwcmVwYXJlIHRoZSBtb2RlbCBmb3IgcXVhbnRpemF0aW9uCm1vZGVsID0gU2ltcGxlTW9kZWwoKQptb2RlbC5xY29uZmlnID0gdG9yY2gucXVhbnRpemF0aW9uLmdldF9kZWZhdWx0X3Fjb25maWcoJ2ZiZ2VtbScpCnRvcmNoLnF1YW50aXphdGlvbi5wcmVwYXJlKG1vZGVsLCBpbnBsYWNlPVRydWUpCgojIENhbGlicmF0ZSB0aGUgbW9kZWwgd2l0aCBzb21lIGRhdGEgKGR1bW15IGRhdGEgaW4gdGhpcyBjYXNlKQptb2RlbCh0b3JjaC5yYW5kbigxMDAsIDEwKSkKCiMgQ29udmVydCB0aGUgbW9kZWwgdG8gYSBxdWFudGl6ZWQgdmVyc2lvbgp0b3JjaC5xdWFudGl6YXRpb24uY29udmVydChtb2RlbCwgaW5wbGFjZT1UcnVlKQoKIyBTYXZlIHRoZSBxdWFudGl6ZWQgbW9kZWwKc2NyaXB0ZWRfbW9kZWwgPSB0b3JjaC5qaXQuc2NyaXB0KG1vZGVsKQpzY3JpcHRlZF9tb2RlbC5zYXZlKCJxdWFudGl6ZWRfbW9kZWwucHQiKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.quantization

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# Initialize and prepare the model for quantization
model = SimpleModel()
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# Calibrate the model with some data (dummy data in this case)
model(torch.randn(100, 10))

# Convert the model to a quantized version
torch.quantization.convert(model, inplace=True)

# Save the quantized model
scripted_model = torch.jit.script(model)
scripted_model.save(&quot;quantized_model.pt&quot;)</pre></div><div class='content'></div><h1><ol start="3">
<li>Deploying with Flask</li>
</ol>
</h1>
<div class='content'></div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>Flask</strong>: A lightweight WSGI web application framework in Python.</li>
</ul>
</div><h2><p>Steps to Deploy:</p>
</h2>
<div class='content'><ol>
<li><strong>Create a Flask App</strong>:
<pre><code class="language-python">from flask import Flask, request, jsonify
import torch

app = Flask(__name__)

# Load the model
model = torch.jit.load('scripted_model.pt')
model.eval()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    input_tensor = torch.tensor(data['input'])
    with torch.no_grad():
        output = model(input_tensor)
    return jsonify(output.tolist())

if __name__ == '__main__':
    app.run()
</code></pre>
</li>
</ol>
</div><h2><p>Practical Example:</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBmbGFzayBpbXBvcnQgRmxhc2ssIHJlcXVlc3QsIGpzb25pZnkKaW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBEZWZpbmUgYSBzaW1wbGUgbW9kZWwKY2xhc3MgU2ltcGxlTW9kZWwobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmKToKICAgICAgICBzdXBlcihTaW1wbGVNb2RlbCwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuZmMgPSBubi5MaW5lYXIoMTAsIDIpCgogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgcmV0dXJuIHNlbGYuZmMoeCkKCiMgSW5pdGlhbGl6ZSBhbmQgc2F2ZSB0aGUgbW9kZWwKbW9kZWwgPSBTaW1wbGVNb2RlbCgpCnNjcmlwdGVkX21vZGVsID0gdG9yY2guaml0LnNjcmlwdChtb2RlbCkKc2NyaXB0ZWRfbW9kZWwuc2F2ZSgic2NyaXB0ZWRfbW9kZWwucHQiKQoKIyBDcmVhdGUgRmxhc2sgYXBwCmFwcCA9IEZsYXNrKF9fbmFtZV9fKQoKIyBMb2FkIHRoZSBtb2RlbAptb2RlbCA9IHRvcmNoLmppdC5sb2FkKCdzY3JpcHRlZF9tb2RlbC5wdCcpCm1vZGVsLmV2YWwoKQoKQGFwcC5yb3V0ZSgnL3ByZWRpY3QnLCBtZXRob2RzPVsnUE9TVCddKQpkZWYgcHJlZGljdCgpOgogICAgZGF0YSA9IHJlcXVlc3QuanNvbgogICAgaW5wdXRfdGVuc29yID0gdG9yY2gudGVuc29yKGRhdGFbJ2lucHV0J10pCiAgICB3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgICAgICBvdXRwdXQgPSBtb2RlbChpbnB1dF90ZW5zb3IpCiAgICByZXR1cm4ganNvbmlmeShvdXRwdXQudG9saXN0KCkpCgppZiBfX25hbWVfXyA9PSAnX19tYWluX18nOgogICAgYXBwLnJ1bigp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from flask import Flask, request, jsonify
import torch
import torch.nn as nn

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc = nn.Linear(10, 2)

    def forward(self, x):
        return self.fc(x)

# Initialize and save the model
model = SimpleModel()
scripted_model = torch.jit.script(model)
scripted_model.save(&quot;scripted_model.pt&quot;)

# Create Flask app
app = Flask(__name__)

# Load the model
model = torch.jit.load('scripted_model.pt')
model.eval()

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    input_tensor = torch.tensor(data['input'])
    with torch.no_grad():
        output = model(input_tensor)
    return jsonify(output.tolist())

if __name__ == '__main__':
    app.run()</pre></div><div class='content'></div><h1><ol start="4">
<li>Deploying with TorchServe</li>
</ol>
</h1>
<div class='content'></div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>TorchServe</strong>: A flexible and easy-to-use tool for serving PyTorch models.</li>
</ul>
</div><h2><p>Steps to Deploy:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Install TorchServe</strong>:</p>
<pre><code class="language-bash">pip install torchserve torch-model-archiver
</code></pre>
</li>
<li>
<p><strong>Archive the Model</strong>:</p>
<pre><code class="language-bash">torch-model-archiver --model-name simple_model --version 1.0 --serialized-file scripted_model.pt --handler torchserve_handler.py --export-path model_store
</code></pre>
</li>
<li>
<p><strong>Start TorchServe</strong>:</p>
<pre><code class="language-bash">torchserve --start --model-store model_store --models simple_model=simple_model.mar
</code></pre>
</li>
<li>
<p><strong>Send Inference Requests</strong>:</p>
<pre><code class="language-bash">curl -X POST http://127.0.0.1:8080/predictions/simple_model -T input.json
</code></pre>
</li>
</ol>
</div><h2><p>Practical Example:</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBBc3N1bWluZyB5b3UgaGF2ZSBhIG1vZGVsIHNhdmVkIGFzIGBzY3JpcHRlZF9tb2RlbC5wdGAgYW5kIGEgaGFuZGxlciBzY3JpcHQgYHRvcmNoc2VydmVfaGFuZGxlci5weWAKCiMgQXJjaGl2ZSB0aGUgbW9kZWwKdG9yY2gtbW9kZWwtYXJjaGl2ZXIgLS1tb2RlbC1uYW1lIHNpbXBsZV9tb2RlbCAtLXZlcnNpb24gMS4wIC0tc2VyaWFsaXplZC1maWxlIHNjcmlwdGVkX21vZGVsLnB0IC0taGFuZGxlciB0b3JjaHNlcnZlX2hhbmRsZXIucHkgLS1leHBvcnQtcGF0aCBtb2RlbF9zdG9yZQoKIyBTdGFydCBUb3JjaFNlcnZlCnRvcmNoc2VydmUgLS1zdGFydCAtLW1vZGVsLXN0b3JlIG1vZGVsX3N0b3JlIC0tbW9kZWxzIHNpbXBsZV9tb2RlbD1zaW1wbGVfbW9kZWwubWFyCgojIFNlbmQgYW4gaW5mZXJlbmNlIHJlcXVlc3QKY3VybCAtWCBQT1NUIGh0dHA6Ly8xMjcuMC4wLjE6ODA4MC9wcmVkaWN0aW9ucy9zaW1wbGVfbW9kZWwgLVQgaW5wdXQuanNvbg=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Assuming you have a model saved as `scripted_model.pt` and a handler script `torchserve_handler.py`

# Archive the model
torch-model-archiver --model-name simple_model --version 1.0 --serialized-file scripted_model.pt --handler torchserve_handler.py --export-path model_store

# Start TorchServe
torchserve --start --model-store model_store --models simple_model=simple_model.mar

# Send an inference request
curl -X POST http://127.0.0.1:8080/predictions/simple_model -T input.json</pre></div><div class='content'></div><h1><ol start="5">
<li>Deploying on Cloud Platforms</li>
</ol>
</h1>
<div class='content'></div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>AWS SageMaker</strong>: A fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.</li>
<li><strong>Google Cloud AI Platform</strong>: A managed service that enables you to easily build, deploy, and manage machine learning models.</li>
</ul>
</div><h2><p>Steps to Deploy on AWS SageMaker:</p>
</h2>
<div class='content'><ol>
<li>
<p><strong>Create a SageMaker Model</strong>:</p>
<pre><code class="language-python">import sagemaker
from sagemaker.pytorch import PyTorchModel

sagemaker_session = sagemaker.Session()
role = 'your-aws-role'

model = PyTorchModel(model_data='s3://path-to-your-model/model.tar.gz',
                     role=role,
                     entry_point='inference.py',
                     framework_version='1.6.0',
                     py_version='py3')

predictor = model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)
</code></pre>
</li>
<li>
<p><strong>Send Inference Requests</strong>:</p>
<pre><code class="language-python">response = predictor.predict(data)
</code></pre>
</li>
</ol>
</div><h2><p>Practical Example:</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHNhZ2VtYWtlcgpmcm9tIHNhZ2VtYWtlci5weXRvcmNoIGltcG9ydCBQeVRvcmNoTW9kZWwKCnNhZ2VtYWtlcl9zZXNzaW9uID0gc2FnZW1ha2VyLlNlc3Npb24oKQpyb2xlID0gJ3lvdXItYXdzLXJvbGUnCgptb2RlbCA9IFB5VG9yY2hNb2RlbChtb2RlbF9kYXRhPSdzMzovL3BhdGgtdG8teW91ci1tb2RlbC9tb2RlbC50YXIuZ3onLAogICAgICAgICAgICAgICAgICAgICByb2xlPXJvbGUsCiAgICAgICAgICAgICAgICAgICAgIGVudHJ5X3BvaW50PSdpbmZlcmVuY2UucHknLAogICAgICAgICAgICAgICAgICAgICBmcmFtZXdvcmtfdmVyc2lvbj0nMS42LjAnLAogICAgICAgICAgICAgICAgICAgICBweV92ZXJzaW9uPSdweTMnKQoKcHJlZGljdG9yID0gbW9kZWwuZGVwbG95KGluc3RhbmNlX3R5cGU9J21sLm00LnhsYXJnZScsIGluaXRpYWxfaW5zdGFuY2VfY291bnQ9MSkKCiMgU2VuZCBhbiBpbmZlcmVuY2UgcmVxdWVzdApkYXRhID0geydpbnB1dCc6IFsxLjAsIDIuMCwgMy4wLCA0LjAsIDUuMCwgNi4wLCA3LjAsIDguMCwgOS4wLCAxMC4wXX0KcmVzcG9uc2UgPSBwcmVkaWN0b3IucHJlZGljdChkYXRhKQpwcmludChyZXNwb25zZSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import sagemaker
from sagemaker.pytorch import PyTorchModel

sagemaker_session = sagemaker.Session()
role = 'your-aws-role'

model = PyTorchModel(model_data='s3://path-to-your-model/model.tar.gz',
                     role=role,
                     entry_point='inference.py',
                     framework_version='1.6.0',
                     py_version='py3')

predictor = model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)

# Send an inference request
data = {'input': [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]}
response = predictor.predict(data)
print(response)</pre></div><div class='content'></div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we covered the essential steps for deploying PyTorch models. We started with exporting and optimizing the model, then moved on to deploying using Flask and TorchServe, and finally discussed deploying on cloud platforms like AWS SageMaker. By following these steps, you can make your PyTorch models available for inference in various production environments, ensuring they are ready to deliver real-world value.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-02-reinforcement-learning' title="Reinforcement Learning with PyTorch">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='06-04-optimizing-performance' title="Optimizing Performance">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Advertising</h1>
<p>This space is reserved for advertising.</p>
<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>Thank you for collaborating!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
