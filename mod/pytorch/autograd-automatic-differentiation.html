<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autograd: Automatic Differentiation</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/autograd-automatic-differentiation" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/autograd-automatic-differentiation" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/autograd-automatic-differentiation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/autograd-automatic-differentiation" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/autograd-automatic-differentiation" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='basic-tensor-operations'>&#x25C4;Basic Tensor Operations</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Autograd: Automatic Differentiation</a>
	</div>
	<div class='col-4 text-end'>
					<a href='introduction-to-neural-networks'>Introduction to Neural Networks &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction to Autograd</h1>
<div class='content'><p>Autograd is PyTorch's automatic differentiation engine that powers neural network training. It provides the ability to automatically compute gradients, which are essential for optimizing neural networks.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Tensors</strong>: The fundamental building blocks in PyTorch, similar to arrays in NumPy.</li>
<li><strong>Gradients</strong>: Derivatives of tensors that are used to update model parameters during training.</li>
<li><strong>Computational Graph</strong>: A dynamic graph that records the operations performed on tensors to compute gradients.</li>
</ul>
</div><h1>Setting Up Autograd</h1>
<div class='content'><p>To use Autograd, you need to understand how to create tensors and enable gradient computation.</p>
</div><h2>Creating Tensors with Gradients</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENyZWF0ZSBhIHRlbnNvciBhbmQgZW5hYmxlIGdyYWRpZW50IGNvbXB1dGF0aW9uCnggPSB0b3JjaC50ZW5zb3IoWzEuMCwgMi4wLCAzLjBdLCByZXF1aXJlc19ncmFkPVRydWUpCnByaW50KHgp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Create a tensor and enable gradient computation
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
print(x)</pre></div><div class='content'><ul>
<li><code>requires_grad=True</code>: This flag tells PyTorch to track all operations on this tensor.</li>
</ul>
</div><h2>Basic Gradient Computation</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEZWZpbmUgYSBzaW1wbGUgZnVuY3Rpb24KeSA9IHggKyAyCnogPSB5ICogeSAqIDIKCiMgQ29tcHV0ZSB0aGUgZ3JhZGllbnRzCnouYmFja3dhcmQodG9yY2gudGVuc29yKFsxLjAsIDEuMCwgMS4wXSkpCnByaW50KHguZ3JhZCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Define a simple function
y = x + 2
z = y * y * 2

# Compute the gradients
z.backward(torch.tensor([1.0, 1.0, 1.0]))
print(x.grad)</pre></div><div class='content'><ul>
<li><code>backward()</code>: This function computes the gradient of <code>z</code> with respect to <code>x</code>.</li>
<li><code>x.grad</code>: This attribute holds the computed gradients.</li>
</ul>
</div><h1>Understanding the Computational Graph</h1>
<div class='content'><p>The computational graph is a directed acyclic graph where nodes represent operations and edges represent tensors.</p>
</div><h2>Example of a Computational Graph</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("YSA9IHRvcmNoLnRlbnNvcihbMi4wLCAzLjBdLCByZXF1aXJlc19ncmFkPVRydWUpCmIgPSBhICogMgpjID0gYiArIDMKZCA9IGMubWVhbigpCgpkLmJhY2t3YXJkKCkKcHJpbnQoYS5ncmFkKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>a = torch.tensor([2.0, 3.0], requires_grad=True)
b = a * 2
c = b + 3
d = c.mean()

d.backward()
print(a.grad)</pre></div><div class='content'><ul>
<li>Here, <code>a</code> is the input tensor, <code>b</code> and <code>c</code> are intermediate tensors, and <code>d</code> is the final output.</li>
<li><code>d.backward()</code>: Computes the gradient of <code>d</code> with respect to <code>a</code>.</li>
</ul>
</div><h1>Advanced Autograd Techniques</h1>
<h2>Gradient Accumulation</h2>
<div class='content'><p>Gradients are accumulated into the <code>.grad</code> attribute. This is useful for mini-batch gradient descent.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBaZXJvIHRoZSBncmFkaWVudHMgYmVmb3JlIHJ1bm5pbmcgdGhlIGJhY2t3YXJkIHBhc3MKeC5ncmFkLnplcm9fKCkKCiMgUGVyZm9ybSBhbm90aGVyIGJhY2t3YXJkIHBhc3MKei5iYWNrd2FyZCh0b3JjaC50ZW5zb3IoWzEuMCwgMS4wLCAxLjBdKSkKcHJpbnQoeC5ncmFkKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Zero the gradients before running the backward pass
x.grad.zero_()

# Perform another backward pass
z.backward(torch.tensor([1.0, 1.0, 1.0]))
print(x.grad)</pre></div><div class='content'></div><h2>Stopping Gradient Tracking</h2>
<div class='content'><p>Sometimes you need to stop tracking gradients, for example, during inference.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("d2l0aCB0b3JjaC5ub19ncmFkKCk6CiAgICB5ID0geCAqIDIKcHJpbnQoeS5yZXF1aXJlc19ncmFkKSAgIyBPdXRwdXQ6IEZhbHNl"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>with torch.no_grad():
    y = x * 2
print(y.requires_grad)  # Output: False</pre></div><div class='content'></div><h2>Custom Gradients</h2>
<div class='content'><p>You can define custom gradients using the <code>torch.autograd.Function</code> class.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgTXlSZUxVKHRvcmNoLmF1dG9ncmFkLkZ1bmN0aW9uKToKICAgIEBzdGF0aWNtZXRob2QKICAgIGRlZiBmb3J3YXJkKGN0eCwgaW5wdXQpOgogICAgICAgIGN0eC5zYXZlX2Zvcl9iYWNrd2FyZChpbnB1dCkKICAgICAgICByZXR1cm4gaW5wdXQuY2xhbXAobWluPTApCgogICAgQHN0YXRpY21ldGhvZAogICAgZGVmIGJhY2t3YXJkKGN0eCwgZ3JhZF9vdXRwdXQpOgogICAgICAgIGlucHV0LCA9IGN0eC5zYXZlZF90ZW5zb3JzCiAgICAgICAgZ3JhZF9pbnB1dCA9IGdyYWRfb3V0cHV0LmNsb25lKCkKICAgICAgICBncmFkX2lucHV0W2lucHV0IDwgMF0gPSAwCiAgICAgICAgcmV0dXJuIGdyYWRfaW5wdXQKCiMgVXNlIHRoZSBjdXN0b20gUmVMVQpyZWx1ID0gTXlSZUxVLmFwcGx5CnggPSB0b3JjaC50ZW5zb3IoWy0xLjAsIDIuMCwgMy4wXSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQp5ID0gcmVsdSh4KQp5LmJhY2t3YXJkKHRvcmNoLnRlbnNvcihbMS4wLCAxLjAsIDEuMF0pKQpwcmludCh4LmdyYWQp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input &lt; 0] = 0
        return grad_input

# Use the custom ReLU
relu = MyReLU.apply
x = torch.tensor([-1.0, 2.0, 3.0], requires_grad=True)
y = relu(x)
y.backward(torch.tensor([1.0, 1.0, 1.0]))
print(x.grad)</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Autograd is a powerful feature in PyTorch that simplifies the process of computing gradients, which are essential for training neural networks. By understanding how to create tensors with gradients, manipulate the computational graph, and use advanced techniques like gradient accumulation and custom gradients, you can effectively leverage Autograd in your deep learning projects.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='basic-tensor-operations'>&#x25C4;Basic Tensor Operations</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Autograd: Automatic Differentiation</a>
	</div>
	<div class='col-4 text-end'>
					<a href='introduction-to-neural-networks'>Introduction to Neural Networks &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
