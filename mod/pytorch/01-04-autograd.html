<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autograd: Automatic Differentiation</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/01-04-autograd" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/01-04-autograd" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/01-04-autograd" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 px-0 py-2 py-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 px-0 py-2 py-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/01-04-autograd" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/01-04-autograd" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='01-03-basic-tensor-operations' title="Basic Tensor Operations">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Autograd: Automatic Differentiation</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='02-01-introduction-to-neural-networks' title="Introduction to Neural Networks">Next &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Autograd is PyTorch's automatic differentiation engine that powers neural network training. It provides automatic computation of gradients, which are essential for optimizing neural networks. This module will cover the basics of autograd, how to use it, and practical examples to solidify your understanding.</p>
</div><h1>Key Concepts</h1>
<div class='content'></div><h2>1. Tensors and Gradients</h2>
<div class='content'><ul>
<li><strong>Tensors</strong>: The fundamental building blocks in PyTorch, similar to NumPy arrays but with additional capabilities for GPU acceleration.</li>
<li><strong>Gradients</strong>: Derivatives of tensors with respect to some scalar value, typically a loss function.</li>
</ul>
</div><h2>2. Computational Graph</h2>
<div class='content'><ul>
<li><strong>Computational Graph</strong>: A directed acyclic graph where nodes represent operations and edges represent tensors. PyTorch dynamically builds this graph as operations are performed.</li>
</ul>
</div><h2>3. Backpropagation</h2>
<div class='content'><ul>
<li><strong>Backpropagation</strong>: The process of computing gradients by traversing the computational graph in reverse order.</li>
</ul>
</div><h1>Practical Examples</h1>
<div class='content'></div><h2>Example 1: Basic Tensor Operations with Autograd</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENyZWF0ZSB0ZW5zb3JzCnggPSB0b3JjaC50ZW5zb3IoMi4wLCByZXF1aXJlc19ncmFkPVRydWUpCnkgPSB0b3JjaC50ZW5zb3IoMy4wLCByZXF1aXJlc19ncmFkPVRydWUpCgojIFBlcmZvcm0gb3BlcmF0aW9ucwp6ID0geCAqIHkgKyB5KioyCgojIENvbXB1dGUgZ3JhZGllbnRzCnouYmFja3dhcmQoKQoKIyBQcmludCBncmFkaWVudHMKcHJpbnQoZiJHcmFkaWVudCBvZiB4OiB7eC5ncmFkfSIpCnByaW50KGYiR3JhZGllbnQgb2YgeToge3kuZ3JhZH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Create tensors
x = torch.tensor(2.0, requires_grad=True)
y = torch.tensor(3.0, requires_grad=True)

# Perform operations
z = x * y + y**2

# Compute gradients
z.backward()

# Print gradients
print(f&quot;Gradient of x: {x.grad}&quot;)
print(f&quot;Gradient of y: {y.grad}&quot;)</pre></div><div class='content'><p><strong>Explanation:</strong></p>
<ol>
<li><code>requires_grad=True</code> tells PyTorch to track operations on these tensors.</li>
<li><code>z.backward()</code> computes the gradients of <code>z</code> with respect to <code>x</code> and <code>y</code>.</li>
<li><code>x.grad</code> and <code>y.grad</code> hold the computed gradients.</li>
</ol>
</div><h2>Example 2: Using Autograd with a Simple Neural Network</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBEZWZpbmUgYSBzaW1wbGUgbGluZWFyIG1vZGVsCm1vZGVsID0gbm4uTGluZWFyKDEsIDEpCgojIElucHV0IHRlbnNvcgppbnB1dF90ZW5zb3IgPSB0b3JjaC50ZW5zb3IoW1sxLjBdXSwgcmVxdWlyZXNfZ3JhZD1UcnVlKQoKIyBGb3J3YXJkIHBhc3MKb3V0cHV0ID0gbW9kZWwoaW5wdXRfdGVuc29yKQoKIyBEZWZpbmUgYSBzaW1wbGUgbG9zcyBmdW5jdGlvbgpsb3NzID0gKG91dHB1dCAtIDIuMCkqKjIKCiMgQmFja3dhcmQgcGFzcwpsb3NzLmJhY2t3YXJkKCkKCiMgUHJpbnQgZ3JhZGllbnRzCnByaW50KGYiR3JhZGllbnQgb2YgaW5wdXRfdGVuc29yOiB7aW5wdXRfdGVuc29yLmdyYWR9IikKcHJpbnQoZiJHcmFkaWVudCBvZiBtb2RlbCB3ZWlnaHQ6IHttb2RlbC53ZWlnaHQuZ3JhZH0iKQpwcmludChmIkdyYWRpZW50IG9mIG1vZGVsIGJpYXM6IHttb2RlbC5iaWFzLmdyYWR9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

# Define a simple linear model
model = nn.Linear(1, 1)

# Input tensor
input_tensor = torch.tensor([[1.0]], requires_grad=True)

# Forward pass
output = model(input_tensor)

# Define a simple loss function
loss = (output - 2.0)**2

# Backward pass
loss.backward()

# Print gradients
print(f&quot;Gradient of input_tensor: {input_tensor.grad}&quot;)
print(f&quot;Gradient of model weight: {model.weight.grad}&quot;)
print(f&quot;Gradient of model bias: {model.bias.grad}&quot;)</pre></div><div class='content'><p><strong>Explanation:</strong></p>
<ol>
<li>A simple linear model is defined using <code>nn.Linear</code>.</li>
<li>The forward pass computes the output.</li>
<li>A loss function is defined as the squared difference between the output and a target value.</li>
<li><code>loss.backward()</code> computes the gradients of the loss with respect to the input tensor and model parameters.</li>
</ol>
</div><h1>Exercises</h1>
<div class='content'></div><h2>Exercise 1: Compute Gradients for a Polynomial Function</h2>
<div class='content'><p><strong>Task:</strong></p>
<ol>
<li>Create a tensor <code>x</code> with value <code>3.0</code> and set <code>requires_grad=True</code>.</li>
<li>Define a polynomial function <code>y = 3x^3 + 2x^2 + x</code>.</li>
<li>Compute the gradient of <code>y</code> with respect to <code>x</code>.</li>
</ol>
<p><strong>Solution:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENyZWF0ZSB0ZW5zb3IKeCA9IHRvcmNoLnRlbnNvcigzLjAsIHJlcXVpcmVzX2dyYWQ9VHJ1ZSkKCiMgRGVmaW5lIHBvbHlub21pYWwgZnVuY3Rpb24KeSA9IDMgKiB4KiozICsgMiAqIHgqKjIgKyB4CgojIENvbXB1dGUgZ3JhZGllbnQKeS5iYWNrd2FyZCgpCgojIFByaW50IGdyYWRpZW50CnByaW50KGYiR3JhZGllbnQgb2YgeDoge3guZ3JhZH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Create tensor
x = torch.tensor(3.0, requires_grad=True)

# Define polynomial function
y = 3 * x**3 + 2 * x**2 + x

# Compute gradient
y.backward()

# Print gradient
print(f&quot;Gradient of x: {x.grad}&quot;)</pre></div><div class='content'><p><strong>Explanation:</strong></p>
<ol>
<li>The tensor <code>x</code> is created with <code>requires_grad=True</code>.</li>
<li>The polynomial function <code>y</code> is defined.</li>
<li><code>y.backward()</code> computes the gradient of <code>y</code> with respect to <code>x</code>.</li>
</ol>
</div><h2>Exercise 2: Gradient Descent Step</h2>
<div class='content'><p><strong>Task:</strong></p>
<ol>
<li>Create a tensor <code>w</code> with value <code>1.0</code> and set <code>requires_grad=True</code>.</li>
<li>Define a simple quadratic function <code>loss = (w - 5)**2</code>.</li>
<li>Perform a gradient descent step to update <code>w</code>.</li>
</ol>
<p><strong>Solution:</strong></p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCgojIENyZWF0ZSB0ZW5zb3IKdyA9IHRvcmNoLnRlbnNvcigxLjAsIHJlcXVpcmVzX2dyYWQ9VHJ1ZSkKCiMgRGVmaW5lIGxvc3MgZnVuY3Rpb24KbG9zcyA9ICh3IC0gNSkqKjIKCiMgQ29tcHV0ZSBncmFkaWVudApsb3NzLmJhY2t3YXJkKCkKCiMgUGVyZm9ybSBncmFkaWVudCBkZXNjZW50IHN0ZXAKbGVhcm5pbmdfcmF0ZSA9IDAuMQp3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgIHcgLT0gbGVhcm5pbmdfcmF0ZSAqIHcuZ3JhZAoKIyBQcmludCB1cGRhdGVkIHZhbHVlIG9mIHcKcHJpbnQoZiJVcGRhdGVkIHZhbHVlIG9mIHc6IHt3fSIp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch

# Create tensor
w = torch.tensor(1.0, requires_grad=True)

# Define loss function
loss = (w - 5)**2

# Compute gradient
loss.backward()

# Perform gradient descent step
learning_rate = 0.1
with torch.no_grad():
    w -= learning_rate * w.grad

# Print updated value of w
print(f&quot;Updated value of w: {w}&quot;)</pre></div><div class='content'><p><strong>Explanation:</strong></p>
<ol>
<li>The tensor <code>w</code> is created with <code>requires_grad=True</code>.</li>
<li>The loss function is defined.</li>
<li><code>loss.backward()</code> computes the gradient of the loss with respect to <code>w</code>.</li>
<li>A gradient descent step is performed to update <code>w</code>.</li>
</ol>
</div><h1>Common Mistakes and Tips</h1>
<div class='content'><ul>
<li><strong>Forgetting <code>requires_grad=True</code></strong>: Ensure that tensors for which you need gradients have <code>requires_grad=True</code>.</li>
<li><strong>Clearing Gradients</strong>: Gradients accumulate by default. Use <code>optimizer.zero_grad()</code> or <code>tensor.grad.zero_()</code> to clear gradients before the next backward pass.</li>
<li><strong>Using <code>torch.no_grad()</code></strong>: Use this context manager to perform operations that should not track gradients, such as during model evaluation or updating parameters manually.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, you learned about PyTorch's autograd functionality, which is crucial for training neural networks. You explored basic tensor operations, computational graphs, and backpropagation. Practical examples and exercises helped reinforce these concepts. In the next module, you will dive into building neural networks using PyTorch.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='01-03-basic-tensor-operations' title="Basic Tensor Operations">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='02-01-introduction-to-neural-networks' title="Introduction to Neural Networks">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
