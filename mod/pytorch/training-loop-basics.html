<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training Loop Basics</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/training-loop-basics" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/training-loop-basics" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/training-loop-basics" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/training-loop-basics" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/training-loop-basics" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='data-loading-and-preprocessing'>&#x25C4;Data Loading and Preprocessing</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Training Loop Basics</a>
	</div>
	<div class='col-4 text-end'>
					<a href='evaluating-model-performance'>Evaluating Model Performance &#x25BA;</a>
			</div>
</div>
<div class='content'><p>In this section, we will cover the fundamentals of training loops in PyTorch. Understanding training loops is crucial for building and optimizing neural networks. We will start with the basic concepts and gradually move to more advanced topics.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ul>
<li><strong>Epoch</strong>: One complete pass through the entire training dataset.</li>
<li><strong>Batch</strong>: A subset of the training dataset used to update the model parameters.</li>
<li><strong>Forward Pass</strong>: Calculating the output of the neural network.</li>
<li><strong>Loss Function</strong>: A function that measures the difference between the predicted output and the actual target.</li>
<li><strong>Backward Pass</strong>: Calculating the gradients of the loss with respect to the model parameters.</li>
<li><strong>Optimizer</strong>: An algorithm that updates the model parameters based on the gradients.</li>
</ul>
</div><h1>Basic Training Loop Structure</h1>
<div class='content'><p>A typical training loop in PyTorch involves the following steps:</p>
<ol>
<li><strong>Initialize the model, loss function, and optimizer</strong>.</li>
<li><strong>Iterate over the dataset</strong> for a number of epochs.</li>
<li><strong>For each batch</strong> in the dataset:
<ul>
<li>Perform a forward pass.</li>
<li>Compute the loss.</li>
<li>Perform a backward pass.</li>
<li>Update the model parameters.</li>
</ul>
</li>
</ol>
</div><h2>Example Code</h2>
<div class='content'><p>Here is a simple example of a training loop in PyTorch:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KZnJvbSB0b3JjaC51dGlscy5kYXRhIGltcG9ydCBEYXRhTG9hZGVyLCBUZW5zb3JEYXRhc2V0CgojIER1bW15IGRhdGFzZXQKeCA9IHRvcmNoLnJhbmRuKDEwMCwgMTApCnkgPSB0b3JjaC5yYW5kbigxMDAsIDEpCgojIERhdGFMb2FkZXIKZGF0YXNldCA9IFRlbnNvckRhdGFzZXQoeCwgeSkKZGF0YWxvYWRlciA9IERhdGFMb2FkZXIoZGF0YXNldCwgYmF0Y2hfc2l6ZT0xMCwgc2h1ZmZsZT1UcnVlKQoKIyBTaW1wbGUgbW9kZWwKbW9kZWwgPSBubi5TZXF1ZW50aWFsKAogICAgbm4uTGluZWFyKDEwLCA1MCksCiAgICBubi5SZUxVKCksCiAgICBubi5MaW5lYXIoNTAsIDEpCikKCiMgTG9zcyBmdW5jdGlvbiBhbmQgb3B0aW1pemVyCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQpvcHRpbWl6ZXIgPSBvcHRpbS5TR0QobW9kZWwucGFyYW1ldGVycygpLCBscj0wLjAxKQoKIyBUcmFpbmluZyBsb29wCm51bV9lcG9jaHMgPSA1CmZvciBlcG9jaCBpbiByYW5nZShudW1fZXBvY2hzKToKICAgIGZvciBiYXRjaF94LCBiYXRjaF95IGluIGRhdGFsb2FkZXI6CiAgICAgICAgIyBGb3J3YXJkIHBhc3MKICAgICAgICBvdXRwdXRzID0gbW9kZWwoYmF0Y2hfeCkKICAgICAgICBsb3NzID0gY3JpdGVyaW9uKG91dHB1dHMsIGJhdGNoX3kpCiAgICAgICAgCiAgICAgICAgIyBCYWNrd2FyZCBwYXNzIGFuZCBvcHRpbWl6YXRpb24KICAgICAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgICAgICBsb3NzLmJhY2t3YXJkKCkKICAgICAgICBvcHRpbWl6ZXIuc3RlcCgpCiAgICAKICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS97bnVtX2Vwb2Noc31dLCBMb3NzOiB7bG9zcy5pdGVtKCk6LjRmfScp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Dummy dataset
x = torch.randn(100, 10)
y = torch.randn(100, 1)

# DataLoader
dataset = TensorDataset(x, y)
dataloader = DataLoader(dataset, batch_size=10, shuffle=True)

# Simple model
model = nn.Sequential(
    nn.Linear(10, 50),
    nn.ReLU(),
    nn.Linear(50, 1)
)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Training loop
num_epochs = 5
for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        # Forward pass
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>DataLoader</strong>: Used to load the dataset in batches.</li>
<li><strong>Model</strong>: A simple neural network with one hidden layer.</li>
<li><strong>Criterion</strong>: Mean Squared Error (MSE) loss function.</li>
<li><strong>Optimizer</strong>: Stochastic Gradient Descent (SGD) optimizer.</li>
<li><strong>Training Loop</strong>: Iterates over the dataset for a specified number of epochs, performing forward and backward passes, and updating the model parameters.</li>
</ul>
</div><h1>Advanced Topics</h1>
<div class='content'></div><h2>Learning Rate Scheduling</h2>
<div class='content'><p>Adjusting the learning rate during training can help improve model performance. PyTorch provides several learning rate schedulers.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c2NoZWR1bGVyID0gb3B0aW0ubHJfc2NoZWR1bGVyLlN0ZXBMUihvcHRpbWl6ZXIsIHN0ZXBfc2l6ZT0yLCBnYW1tYT0wLjEpCgpmb3IgZXBvY2ggaW4gcmFuZ2UobnVtX2Vwb2Nocyk6CiAgICBmb3IgYmF0Y2hfeCwgYmF0Y2hfeSBpbiBkYXRhbG9hZGVyOgogICAgICAgIG91dHB1dHMgPSBtb2RlbChiYXRjaF94KQogICAgICAgIGxvc3MgPSBjcml0ZXJpb24ob3V0cHV0cywgYmF0Y2hfeSkKICAgICAgICAKICAgICAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgICAgICBsb3NzLmJhY2t3YXJkKCkKICAgICAgICBvcHRpbWl6ZXIuc3RlcCgpCiAgICAKICAgIHNjaGVkdWxlci5zdGVwKCkKICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS97bnVtX2Vwb2Noc31dLCBMb3NzOiB7bG9zcy5pdGVtKCk6LjRmfSwgTFI6IHtzY2hlZHVsZXIuZ2V0X2xhc3RfbHIoKX0nKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)

for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    scheduler.step()
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, LR: {scheduler.get_last_lr()}')</pre></div><div class='content'></div><h2>Gradient Clipping</h2>
<div class='content'><p>Gradient clipping is used to prevent the gradients from exploding during training.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Zm9yIGVwb2NoIGluIHJhbmdlKG51bV9lcG9jaHMpOgogICAgZm9yIGJhdGNoX3gsIGJhdGNoX3kgaW4gZGF0YWxvYWRlcjoKICAgICAgICBvdXRwdXRzID0gbW9kZWwoYmF0Y2hfeCkKICAgICAgICBsb3NzID0gY3JpdGVyaW9uKG91dHB1dHMsIGJhdGNoX3kpCiAgICAgICAgCiAgICAgICAgb3B0aW1pemVyLnplcm9fZ3JhZCgpCiAgICAgICAgbG9zcy5iYWNrd2FyZCgpCiAgICAgICAgCiAgICAgICAgIyBHcmFkaWVudCBjbGlwcGluZwogICAgICAgIHRvcmNoLm5uLnV0aWxzLmNsaXBfZ3JhZF9ub3JtXyhtb2RlbC5wYXJhbWV0ZXJzKCksIG1heF9ub3JtPTEuMCkKICAgICAgICAKICAgICAgICBvcHRpbWl6ZXIuc3RlcCgpCiAgICAKICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS97bnVtX2Vwb2Noc31dLCBMb3NzOiB7bG9zcy5pdGVtKCk6LjRmfScp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        optimizer.zero_grad()
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h1>Summary</h1>
<div class='content'><p>In this section, we covered the basics of training loops in PyTorch, including key concepts, a basic training loop structure, and some advanced topics like learning rate scheduling and gradient clipping. Understanding these fundamentals is essential for building and optimizing neural networks effectively.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='data-loading-and-preprocessing'>&#x25C4;Data Loading and Preprocessing</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Training Loop Basics</a>
	</div>
	<div class='col-4 text-end'>
					<a href='evaluating-model-performance'>Evaluating Model Performance &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
