<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recurrent Neural Networks (RNNs)</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/recurrent-neural-networks-rnns" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/recurrent-neural-networks-rnns" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/recurrent-neural-networks-rnns" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/recurrent-neural-networks-rnns" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/recurrent-neural-networks-rnns" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='convolutional-neural-networks-cnns'>&#x25C4;Convolutional Neural Networks (CNNs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Recurrent Neural Networks (RNNs)</a>
	</div>
	<div class='col-4 text-end'>
					<a href='long-short-term-memory-networks-lstms'>Long Short-Term Memory Networks (LSTMs) &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Recurrent Neural Networks (RNNs) are a class of neural networks that are particularly effective for sequential data. In this section, we will explore the fundamentals of RNNs, how to implement them using PyTorch, and advanced techniques to improve their performance.</p>
</div><h1>Introduction to RNNs</h1>
<div class='content'><ul>
<li><strong>Sequential Data</strong>: Data where the order of elements is important, such as time series, text, and audio.</li>
<li><strong>Recurrent Connections</strong>: Unlike traditional neural networks, RNNs have connections that form cycles, allowing information to persist.</li>
</ul>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Hidden State</strong>: A memory that captures information about previous elements in the sequence.</li>
<li><strong>Vanishing/Exploding Gradients</strong>: Challenges in training RNNs due to long-term dependencies.</li>
</ul>
</div><h1>Implementing RNNs in PyTorch</h1>
<div class='content'></div><h2>Basic RNN Structure</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKY2xhc3MgU2ltcGxlUk5OKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKToKICAgICAgICBzdXBlcihTaW1wbGVSTk4sIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmhpZGRlbl9zaXplID0gaGlkZGVuX3NpemUKICAgICAgICBzZWxmLnJubiA9IG5uLlJOTihpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgYmF0Y2hfZmlyc3Q9VHJ1ZSkKICAgICAgICBzZWxmLmZjID0gbm4uTGluZWFyKGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgaDAgPSB0b3JjaC56ZXJvcygxLCB4LnNpemUoMCksIHNlbGYuaGlkZGVuX3NpemUpLnRvKHguZGV2aWNlKSAgIyBJbml0aWFsIGhpZGRlbiBzdGF0ZQogICAgICAgIG91dCwgaG4gPSBzZWxmLnJubih4LCBoMCkKICAgICAgICBvdXQgPSBzZWxmLmZjKG91dFs6LCAtMSwgOl0pICAjIFRha2luZyB0aGUgb3V0cHV0IG9mIHRoZSBsYXN0IHRpbWUgc3RlcAogICAgICAgIHJldHVybiBvdXQKCiMgRXhhbXBsZSB1c2FnZQppbnB1dF9zaXplID0gMTAKaGlkZGVuX3NpemUgPSAyMApvdXRwdXRfc2l6ZSA9IDEKbW9kZWwgPSBTaW1wbGVSTk4oaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)  # Initial hidden state
        out, hn = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])  # Taking the output of the last time step
        return out

# Example usage
input_size = 10
hidden_size = 20
output_size = 1
model = SimpleRNN(input_size, hidden_size, output_size)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Input Size</strong>: Number of features in the input.</li>
<li><strong>Hidden Size</strong>: Number of features in the hidden state.</li>
<li><strong>Output Size</strong>: Number of features in the output.</li>
<li><strong>Batch First</strong>: Indicates that the input tensor has the batch size as the first dimension.</li>
</ul>
</div><h2>Training the RNN</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoLm9wdGltIGFzIG9wdGltCgojIER1bW15IGRhdGEKaW5wdXRzID0gdG9yY2gucmFuZG4oMzIsIDEwLCBpbnB1dF9zaXplKSAgIyBCYXRjaCBzaXplIG9mIDMyLCBzZXF1ZW5jZSBsZW5ndGggb2YgMTAKdGFyZ2V0cyA9IHRvcmNoLnJhbmRuKDMyLCBvdXRwdXRfc2l6ZSkKCiMgTG9zcyBhbmQgb3B0aW1pemVyCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQpvcHRpbWl6ZXIgPSBvcHRpbS5BZGFtKG1vZGVsLnBhcmFtZXRlcnMoKSwgbHI9MC4wMDEpCgojIFRyYWluaW5nIGxvb3AKZm9yIGVwb2NoIGluIHJhbmdlKDEwMCk6CiAgICBtb2RlbC50cmFpbigpCiAgICBvdXRwdXRzID0gbW9kZWwoaW5wdXRzKQogICAgbG9zcyA9IGNyaXRlcmlvbihvdXRwdXRzLCB0YXJnZXRzKQogICAgCiAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgIGxvc3MuYmFja3dhcmQoKQogICAgb3B0aW1pemVyLnN0ZXAoKQogICAgCiAgICBpZiAoZXBvY2grMSkgJSAxMCA9PSAwOgogICAgICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS8xMDBdLCBMb3NzOiB7bG9zcy5pdGVtKCk6LjRmfScp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch.optim as optim

# Dummy data
inputs = torch.randn(32, 10, input_size)  # Batch size of 32, sequence length of 10
targets = torch.randn(32, output_size)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
for epoch in range(100):
    model.train()
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Loss Function</strong>: Mean Squared Error (MSE) for regression tasks.</li>
<li><strong>Optimizer</strong>: Adam optimizer for efficient training.</li>
<li><strong>Training Loop</strong>: Iterates over epochs, computes loss, performs backpropagation, and updates weights.</li>
</ul>
</div><h1>Advanced RNN Techniques</h1>
<div class='content'></div><h2>Long Short-Term Memory (LSTM)</h2>
<div class='content'><p>LSTMs are a type of RNN designed to handle the vanishing gradient problem by introducing gates that control the flow of information.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlTFNUTShubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X3NpemUsIGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSk6CiAgICAgICAgc3VwZXIoU2ltcGxlTFNUTSwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuaGlkZGVuX3NpemUgPSBoaWRkZW5fc2l6ZQogICAgICAgIHNlbGYubHN0bSA9IG5uLkxTVE0oaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIGJhdGNoX2ZpcnN0PVRydWUpCiAgICAgICAgc2VsZi5mYyA9IG5uLkxpbmVhcihoaWRkZW5fc2l6ZSwgb3V0cHV0X3NpemUpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIGgwID0gdG9yY2guemVyb3MoMSwgeC5zaXplKDApLCBzZWxmLmhpZGRlbl9zaXplKS50byh4LmRldmljZSkKICAgICAgICBjMCA9IHRvcmNoLnplcm9zKDEsIHguc2l6ZSgwKSwgc2VsZi5oaWRkZW5fc2l6ZSkudG8oeC5kZXZpY2UpCiAgICAgICAgb3V0LCAoaG4sIGNuKSA9IHNlbGYubHN0bSh4LCAoaDAsIGMwKSkKICAgICAgICBvdXQgPSBzZWxmLmZjKG91dFs6LCAtMSwgOl0pCiAgICAgICAgcmV0dXJuIG91dAoKIyBFeGFtcGxlIHVzYWdlCm1vZGVsID0gU2ltcGxlTFNUTShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgb3V0cHV0X3NpemUp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, (hn, cn) = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Example usage
model = SimpleLSTM(input_size, hidden_size, output_size)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>LSTM</strong>: Similar to RNN but includes cell state and gates (input, forget, output) to manage long-term dependencies.</li>
</ul>
</div><h2>Gated Recurrent Unit (GRU)</h2>
<div class='content'><p>GRUs are a simpler alternative to LSTMs, using fewer gates and parameters.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlR1JVKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemUsIG91dHB1dF9zaXplKToKICAgICAgICBzdXBlcihTaW1wbGVHUlUsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmhpZGRlbl9zaXplID0gaGlkZGVuX3NpemUKICAgICAgICBzZWxmLmdydSA9IG5uLkdSVShpbnB1dF9zaXplLCBoaWRkZW5fc2l6ZSwgYmF0Y2hfZmlyc3Q9VHJ1ZSkKICAgICAgICBzZWxmLmZjID0gbm4uTGluZWFyKGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgeCk6CiAgICAgICAgaDAgPSB0b3JjaC56ZXJvcygxLCB4LnNpemUoMCksIHNlbGYuaGlkZGVuX3NpemUpLnRvKHguZGV2aWNlKQogICAgICAgIG91dCwgaG4gPSBzZWxmLmdydSh4LCBoMCkKICAgICAgICBvdXQgPSBzZWxmLmZjKG91dFs6LCAtMSwgOl0pCiAgICAgICAgcmV0dXJuIG91dAoKIyBFeGFtcGxlIHVzYWdlCm1vZGVsID0gU2ltcGxlR1JVKGlucHV0X3NpemUsIGhpZGRlbl9zaXplLCBvdXRwdXRfc2l6ZSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleGRU(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleGRU, self).__init__()
        self.hidden_size = hidden_size
        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)
        out, hn = self.gru(x, h0)
        out = self.fc(out[:, -1, :])
        return out

# Example usage
model = SimpleGRU(input_size, hidden_size, output_size)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>GRU</strong>: Combines the forget and input gates into a single update gate, simplifying the architecture.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>Recurrent Neural Networks are powerful tools for handling sequential data. By understanding the basics of RNNs and implementing them in PyTorch, you can tackle a wide range of problems involving time series, text, and more. Advanced variants like LSTMs and GRUs help mitigate common issues like vanishing gradients, making them suitable for more complex tasks. With practice and experimentation, you can leverage these models to build sophisticated applications.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='convolutional-neural-networks-cnns'>&#x25C4;Convolutional Neural Networks (CNNs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Recurrent Neural Networks (RNNs)</a>
	</div>
	<div class='col-4 text-end'>
					<a href='long-short-term-memory-networks-lstms'>Long Short-Term Memory Networks (LSTMs) &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
