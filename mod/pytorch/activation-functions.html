<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activation Functions in PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/activation-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/activation-functions" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/activation-functions" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='creating-a-simple-neural-network'>&#x25C4;Creating a Simple Neural Network</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Activation Functions in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='loss-functions-and-optimization'>Loss Functions and Optimization &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Activation functions play a crucial role in neural networks by introducing non-linearity into the model, enabling it to learn complex patterns. In this section, we will explore various activation functions available in PyTorch, understand their properties, and see how to implement them in code.</p>
</div><h1>Introduction to Activation Functions</h1>
<div class='content'><ul>
<li><strong>Definition</strong>: Activation functions determine the output of a neural network node given an input or set of inputs.</li>
<li><strong>Purpose</strong>: They introduce non-linear properties to the network, allowing it to learn from data and perform complex tasks.</li>
</ul>
</div><h1>Common Activation Functions</h1>
<div class='content'></div><h2>Sigmoid</h2>
<div class='content'><ul>
<li><strong>Formula</strong>: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>Range</strong>: (0, 1)</li>
<li><strong>Properties</strong>:
<ul>
<li>Smooth gradient</li>
<li>Output values bound between 0 and 1</li>
<li>Can cause vanishing gradient problem</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBTaWdtb2lkIGFjdGl2YXRpb24gZnVuY3Rpb24Kc2lnbW9pZCA9IG5uLlNpZ21vaWQoKQppbnB1dF90ZW5zb3IgPSB0b3JjaC50ZW5zb3IoWzEuMCwgMi4wLCAzLjBdKQpvdXRwdXRfdGVuc29yID0gc2lnbW9pZChpbnB1dF90ZW5zb3IpCnByaW50KG91dHB1dF90ZW5zb3Ip"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

# Sigmoid activation function
sigmoid = nn.Sigmoid()
input_tensor = torch.tensor([1.0, 2.0, 3.0])
output_tensor = sigmoid(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>Tanh</h2>
<div class='content'><ul>
<li><strong>Formula</strong>: \( \tanh(x) = \frac{2}{1 + e^{-2x}} - 1 \)</li>
<li><strong>Range</strong>: (-1, 1)</li>
<li><strong>Properties</strong>:
<ul>
<li>Zero-centered</li>
<li>Smooth gradient</li>
<li>Can also cause vanishing gradient problem</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBUYW5oIGFjdGl2YXRpb24gZnVuY3Rpb24KdGFuaCA9IG5uLlRhbmgoKQpvdXRwdXRfdGVuc29yID0gdGFuaChpbnB1dF90ZW5zb3IpCnByaW50KG91dHB1dF90ZW5zb3Ip"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Tanh activation function
tanh = nn.Tanh()
output_tensor = tanh(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>ReLU (Rectified Linear Unit)</h2>
<div class='content'><ul>
<li><strong>Formula</strong>: \( \text{ReLU}(x) = \max(0, x) \)</li>
<li><strong>Range</strong>: [0, ∞)</li>
<li><strong>Properties</strong>:
<ul>
<li>Computationally efficient</li>
<li>Helps mitigate the vanishing gradient problem</li>
<li>Can cause dying ReLU problem</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBSZUxVIGFjdGl2YXRpb24gZnVuY3Rpb24KcmVsdSA9IG5uLlJlTFUoKQpvdXRwdXRfdGVuc29yID0gcmVsdShpbnB1dF90ZW5zb3IpCnByaW50KG91dHB1dF90ZW5zb3Ip"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># ReLU activation function
relu = nn.ReLU()
output_tensor = relu(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>Leaky ReLU</h2>
<div class='content'><ul>
<li><strong>Formula</strong>: \( \text{Leaky ReLU}(x) = \max(0.01x, x) \)</li>
<li><strong>Range</strong>: (-∞, ∞)</li>
<li><strong>Properties</strong>:
<ul>
<li>Allows a small gradient when the unit is not active</li>
<li>Helps mitigate the dying ReLU problem</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBMZWFreSBSZUxVIGFjdGl2YXRpb24gZnVuY3Rpb24KbGVha3lfcmVsdSA9IG5uLkxlYWt5UmVMVSgwLjAxKQpvdXRwdXRfdGVuc29yID0gbGVha3lfcmVsdShpbnB1dF90ZW5zb3IpCnByaW50KG91dHB1dF90ZW5zb3Ip"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Leaky ReLU activation function
leaky_relu = nn.LeakyReLU(0.01)
output_tensor = leaky_relu(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h2>Softmax</h2>
<div class='content'><ul>
<li><strong>Formula</strong>: \( \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \)</li>
<li><strong>Range</strong>: (0, 1)</li>
<li><strong>Properties</strong>:
<ul>
<li>Converts logits to probabilities</li>
<li>Often used in the output layer for classification tasks</li>
</ul>
</li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBTb2Z0bWF4IGFjdGl2YXRpb24gZnVuY3Rpb24Kc29mdG1heCA9IG5uLlNvZnRtYXgoZGltPTApCm91dHB1dF90ZW5zb3IgPSBzb2Z0bWF4KGlucHV0X3RlbnNvcikKcHJpbnQob3V0cHV0X3RlbnNvcik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Softmax activation function
softmax = nn.Softmax(dim=0)
output_tensor = softmax(input_tensor)
print(output_tensor)</pre></div><div class='content'></div><h1>Comparison of Activation Functions</h1>
<div class='content'><p>| Activation Function | Formula | Range | Key Properties |
|---------------------|---------|-------|----------------|
| Sigmoid             | \( \frac{1}{1 + e^{-x}} \) | (0, 1) | Smooth gradient, vanishing gradient problem |
| Tanh                | \( \frac{2}{1 + e^{-2x}} - 1 \) | (-1, 1) | Zero-centered, vanishing gradient problem |
| ReLU                | \( \max(0, x) \) | [0, ∞) | Computationally efficient, dying ReLU problem |
| Leaky ReLU          | \( \max(0.01x, x) \) | (-∞, ∞) | Small gradient when inactive, mitigates dying ReLU |
| Softmax             | \( \frac{e^{x_i}}{\sum_{j} e^{x_j}} \) | (0, 1) | Converts logits to probabilities, used in classification |</p>
</div><h1>Conclusion</h1>
<div class='content'><p>Activation functions are a fundamental component of neural networks, enabling them to learn and model complex data. In PyTorch, various activation functions are readily available and can be easily integrated into your models. Understanding the properties and appropriate use cases for each activation function is essential for building effective neural networks.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='creating-a-simple-neural-network'>&#x25C4;Creating a Simple Neural Network</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Activation Functions in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='loss-functions-and-optimization'>Loss Functions and Optimization &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
