<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Introduction to Neural Networks</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/02-01-introduction-to-neural-networks" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/02-01-introduction-to-neural-networks" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/02-01-introduction-to-neural-networks" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=4" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>var DEVELOP = window.location.href.indexOf("localhost")!=-1 && true;</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/pytorch/02-01-introduction-to-neural-networks" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/pytorch/02-01-introduction-to-neural-networks" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>
		<!-- <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
					<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Course Content
	</a>

			</div>
		</div>
	</div>
</div>
 -->
 
<div class="top-bar container-fluid">
	<div class="container-xxl">
		<nav class="navbar navbar-expand-md p-0">
			<div class="container-fluid">
				<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
				</button>
				
				<div class="collapse navbar-collapse" id="navbarNav">
					<div class="navbar-nav me-auto">
							<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Course Content
	</a>

					</div>
				</div>			
							</div>
		</nav>
	</div>
</div>				<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='01-04-autograd' title="Autograd: Automatic Differentiation">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<h2 style="text-decoration:underline">Introduction to Neural Networks</h2>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-02-creating-simple-neural-network' title="Creating a Simple Neural Network">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'></div><h1>What are Neural Networks?</h1>
<div class='content'><p>Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling, or clustering of raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text, or time series, must be translated.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ol>
<li><strong>Neurons</strong>: The basic unit of a neural network, analogous to a biological neuron. Each neuron receives input, processes it, and passes the output to the next layer.</li>
<li><strong>Layers</strong>: Neural networks are composed of layers of neurons. The most common types are:
<ul>
<li><strong>Input Layer</strong>: The first layer that receives the input data.</li>
<li><strong>Hidden Layers</strong>: Intermediate layers that process inputs received from the input layer.</li>
<li><strong>Output Layer</strong>: The final layer that produces the output.</li>
</ul>
</li>
<li><strong>Weights and Biases</strong>: Parameters that are adjusted during training to minimize the error in predictions.</li>
<li><strong>Activation Functions</strong>: Functions applied to the output of each neuron to introduce non-linearity into the model.</li>
</ol>
</div><h2>Structure of a Neural Network</h2>
<div class='content'><p>A typical neural network consists of an input layer, one or more hidden layers, and an output layer. Each layer is made up of neurons, and each neuron in one layer is connected to every neuron in the next layer.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("SW5wdXQgTGF5ZXIgLT4gSGlkZGVuIExheWVyKHMpIC0+IE91dHB1dCBMYXllcg=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>Input Layer -&gt; Hidden Layer(s) -&gt; Output Layer</pre></div><div class='content'></div><h2>Example: Simple Neural Network</h2>
<div class='content'><p>Consider a simple neural network with one input layer, one hidden layer, and one output layer.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("SW5wdXQgTGF5ZXIgKDMgbmV1cm9ucykgLT4gSGlkZGVuIExheWVyICg0IG5ldXJvbnMpIC0+IE91dHB1dCBMYXllciAoMSBuZXVyb24p"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>Input Layer (3 neurons) -&gt; Hidden Layer (4 neurons) -&gt; Output Layer (1 neuron)</pre></div><div class='content'></div><h1>Practical Example: Building a Simple Neural Network with PyTorch</h1>
<div class='content'><p>Let's build a simple neural network using PyTorch to understand these concepts better.</p>
</div><h2>Step 1: Import Libraries</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim</pre></div><div class='content'></div><h2>Step 2: Define the Neural Network</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgU2ltcGxlTk4obm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmKToKICAgICAgICBzdXBlcihTaW1wbGVOTiwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuZmMxID0gbm4uTGluZWFyKDMsIDQpICAjIElucHV0IGxheWVyIHRvIGhpZGRlbiBsYXllcgogICAgICAgIHNlbGYuZmMyID0gbm4uTGluZWFyKDQsIDEpICAjIEhpZGRlbiBsYXllciB0byBvdXRwdXQgbGF5ZXIKCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkgICMgQXBwbHkgUmVMVSBhY3RpdmF0aW9uIGZ1bmN0aW9uCiAgICAgICAgeCA9IHNlbGYuZmMyKHgpCiAgICAgICAgcmV0dXJuIHg="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(3, 4)  # Input layer to hidden layer
        self.fc2 = nn.Linear(4, 1)  # Hidden layer to output layer

    def forward(self, x):
        x = torch.relu(self.fc1(x))  # Apply ReLU activation function
        x = self.fc2(x)
        return x</pre></div><div class='content'></div><h2>Step 3: Initialize the Network, Define Loss Function and Optimizer</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBJbml0aWFsaXplIHRoZSBuZXR3b3JrCm5ldCA9IFNpbXBsZU5OKCkKCiMgRGVmaW5lIGxvc3MgZnVuY3Rpb24gYW5kIG9wdGltaXplcgpjcml0ZXJpb24gPSBubi5NU0VMb3NzKCkgICMgTWVhbiBTcXVhcmVkIEVycm9yIExvc3MKb3B0aW1pemVyID0gb3B0aW0uU0dEKG5ldC5wYXJhbWV0ZXJzKCksIGxyPTAuMDEpICAjIFN0b2NoYXN0aWMgR3JhZGllbnQgRGVzY2VudA=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Initialize the network
net = SimpleNN()

# Define loss function and optimizer
criterion = nn.MSELoss()  # Mean Squared Error Loss
optimizer = optim.SGD(net.parameters(), lr=0.01)  # Stochastic Gradient Descent</pre></div><div class='content'></div><h2>Step 4: Training the Network</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEdW1teSBkYXRhCmlucHV0cyA9IHRvcmNoLnRlbnNvcihbWzEuMCwgMi4wLCAzLjBdLCBbNC4wLCA1LjAsIDYuMF1dLCBkdHlwZT10b3JjaC5mbG9hdDMyKQp0YXJnZXRzID0gdG9yY2gudGVuc29yKFtbMC41XSwgWzEuNV1dLCBkdHlwZT10b3JjaC5mbG9hdDMyKQoKIyBUcmFpbmluZyBsb29wCmZvciBlcG9jaCBpbiByYW5nZSgxMDAwKToKICAgIG9wdGltaXplci56ZXJvX2dyYWQoKSAgIyBaZXJvIHRoZSBncmFkaWVudHMKICAgIG91dHB1dHMgPSBuZXQoaW5wdXRzKSAgIyBGb3J3YXJkIHBhc3MKICAgIGxvc3MgPSBjcml0ZXJpb24ob3V0cHV0cywgdGFyZ2V0cykgICMgQ29tcHV0ZSBsb3NzCiAgICBsb3NzLmJhY2t3YXJkKCkgICMgQmFja3dhcmQgcGFzcwogICAgb3B0aW1pemVyLnN0ZXAoKSAgIyBVcGRhdGUgd2VpZ2h0cwoKICAgIGlmIChlcG9jaCsxKSAlIDEwMCA9PSAwOgogICAgICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS8xMDAwXSwgTG9zczoge2xvc3MuaXRlbSgpOi40Zn0nKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Dummy data
inputs = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=torch.float32)
targets = torch.tensor([[0.5], [1.5]], dtype=torch.float32)

# Training loop
for epoch in range(1000):
    optimizer.zero_grad()  # Zero the gradients
    outputs = net(inputs)  # Forward pass
    loss = criterion(outputs, targets)  # Compute loss
    loss.backward()  # Backward pass
    optimizer.step()  # Update weights

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Initialization</strong>: We define a simple neural network with one hidden layer.</li>
<li><strong>Forward Pass</strong>: The input data is passed through the network, and the ReLU activation function is applied to the hidden layer.</li>
<li><strong>Loss Function</strong>: Mean Squared Error (MSE) is used to measure the difference between the predicted and actual values.</li>
<li><strong>Optimizer</strong>: Stochastic Gradient Descent (SGD) is used to update the weights of the network.</li>
<li><strong>Training Loop</strong>: The network is trained for 1000 epochs, and the loss is printed every 100 epochs.</li>
</ul>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Task</h2>
<div class='content'><p>Create a neural network with the following structure:</p>
<ul>
<li>Input Layer: 2 neurons</li>
<li>Hidden Layer: 3 neurons</li>
<li>Output Layer: 1 neuron</li>
</ul>
<p>Train the network on the following dummy data:</p>
<ul>
<li>Inputs: <code>[[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]</code></li>
<li>Targets: <code>[[0.5], [1.0], [1.5]]</code></li>
</ul>
</div><h2>Solution</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KCiMgRGVmaW5lIHRoZSBuZXVyYWwgbmV0d29yawpjbGFzcyBDdXN0b21OTihubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKEN1c3RvbU5OLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi5mYzEgPSBubi5MaW5lYXIoMiwgMykgICMgSW5wdXQgbGF5ZXIgdG8gaGlkZGVuIGxheWVyCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoMywgMSkgICMgSGlkZGVuIGxheWVyIHRvIG91dHB1dCBsYXllcgoKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIHggPSB0b3JjaC5yZWx1KHNlbGYuZmMxKHgpKSAgIyBBcHBseSBSZUxVIGFjdGl2YXRpb24gZnVuY3Rpb24KICAgICAgICB4ID0gc2VsZi5mYzIoeCkKICAgICAgICByZXR1cm4geAoKIyBJbml0aWFsaXplIHRoZSBuZXR3b3JrCm5ldCA9IEN1c3RvbU5OKCkKCiMgRGVmaW5lIGxvc3MgZnVuY3Rpb24gYW5kIG9wdGltaXplcgpjcml0ZXJpb24gPSBubi5NU0VMb3NzKCkgICMgTWVhbiBTcXVhcmVkIEVycm9yIExvc3MKb3B0aW1pemVyID0gb3B0aW0uU0dEKG5ldC5wYXJhbWV0ZXJzKCksIGxyPTAuMDEpICAjIFN0b2NoYXN0aWMgR3JhZGllbnQgRGVzY2VudAoKIyBEdW1teSBkYXRhCmlucHV0cyA9IHRvcmNoLnRlbnNvcihbWzEuMCwgMi4wXSwgWzMuMCwgNC4wXSwgWzUuMCwgNi4wXV0sIGR0eXBlPXRvcmNoLmZsb2F0MzIpCnRhcmdldHMgPSB0b3JjaC50ZW5zb3IoW1swLjVdLCBbMS4wXSwgWzEuNV1dLCBkdHlwZT10b3JjaC5mbG9hdDMyKQoKIyBUcmFpbmluZyBsb29wCmZvciBlcG9jaCBpbiByYW5nZSgxMDAwKToKICAgIG9wdGltaXplci56ZXJvX2dyYWQoKSAgIyBaZXJvIHRoZSBncmFkaWVudHMKICAgIG91dHB1dHMgPSBuZXQoaW5wdXRzKSAgIyBGb3J3YXJkIHBhc3MKICAgIGxvc3MgPSBjcml0ZXJpb24ob3V0cHV0cywgdGFyZ2V0cykgICMgQ29tcHV0ZSBsb3NzCiAgICBsb3NzLmJhY2t3YXJkKCkgICMgQmFja3dhcmQgcGFzcwogICAgb3B0aW1pemVyLnN0ZXAoKSAgIyBVcGRhdGUgd2VpZ2h0cwoKICAgIGlmIChlcG9jaCsxKSAlIDEwMCA9PSAwOgogICAgICAgIHByaW50KGYnRXBvY2ggW3tlcG9jaCsxfS8xMDAwXSwgTG9zczoge2xvc3MuaXRlbSgpOi40Zn0nKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim

# Define the neural network
class CustomNN(nn.Module):
    def __init__(self):
        super(CustomNN, self).__init__()
        self.fc1 = nn.Linear(2, 3)  # Input layer to hidden layer
        self.fc2 = nn.Linear(3, 1)  # Hidden layer to output layer

    def forward(self, x):
        x = torch.relu(self.fc1(x))  # Apply ReLU activation function
        x = self.fc2(x)
        return x

# Initialize the network
net = CustomNN()

# Define loss function and optimizer
criterion = nn.MSELoss()  # Mean Squared Error Loss
optimizer = optim.SGD(net.parameters(), lr=0.01)  # Stochastic Gradient Descent

# Dummy data
inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=torch.float32)
targets = torch.tensor([[0.5], [1.0], [1.5]], dtype=torch.float32)

# Training loop
for epoch in range(1000):
    optimizer.zero_grad()  # Zero the gradients
    outputs = net(inputs)  # Forward pass
    loss = criterion(outputs, targets)  # Compute loss
    loss.backward()  # Backward pass
    optimizer.step()  # Update weights

    if (epoch+1) % 100 == 0:
        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h2>Common Mistakes and Tips</h2>
<div class='content'><ul>
<li><strong>Learning Rate</strong>: If the learning rate is too high, the network may not converge. If it's too low, training will be very slow.</li>
<li><strong>Overfitting</strong>: Ensure you have enough data to train the network to avoid overfitting.</li>
<li><strong>Data Normalization</strong>: Normalize your input data to improve the training process.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we introduced the basic concepts of neural networks and demonstrated how to build a simple neural network using PyTorch. We covered the structure of a neural network, key components like neurons, layers, weights, biases, and activation functions. We also provided a practical example and an exercise to reinforce the concepts learned. In the next section, we will delve deeper into creating more complex neural networks and explore different activation functions.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='01-04-autograd' title="Autograd: Automatic Differentiation">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='02-02-creating-simple-neural-network' title="Creating a Simple Neural Network">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
				<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
	     crossorigin="anonymous"></script>
	<!-- enterprise_campus -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-0611338592562725"
	     data-ad-slot="6914733106"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
	     (adsbygoogle = window.adsbygoogle || []).push({});
	</script>
			
	<div class="container mt-2 d-none d-md-block index">
		<h1>PyTorch: From Beginner to Advanced</h1>
<h2>Module 1: Introduction to PyTorch</h2>
<ul>
<li><a href="01-01-what-is-pytorch">What is PyTorch?</a></li>
<li><a href="01-02-setting-up-environment">Setting Up the Environment</a></li>
<li><a href="01-03-basic-tensor-operations">Basic Tensor Operations</a></li>
<li><a href="01-04-autograd">Autograd: Automatic Differentiation</a></li>
</ul>
<h2>Module 2: Building Neural Networks</h2>
<ul>
<li><a href="02-01-introduction-to-neural-networks">Introduction to Neural Networks</a></li>
<li><a href="02-02-creating-simple-neural-network">Creating a Simple Neural Network</a></li>
<li><a href="02-03-activation-functions">Activation Functions</a></li>
<li><a href="02-04-loss-functions-optimization">Loss Functions and Optimization</a></li>
</ul>
<h2>Module 3: Training Neural Networks</h2>
<ul>
<li><a href="03-01-data-loading-preprocessing">Data Loading and Preprocessing</a></li>
<li><a href="03-02-training-loop">Training Loop</a></li>
<li><a href="03-03-validation-testing">Validation and Testing</a></li>
<li><a href="03-04-saving-loading-models">Saving and Loading Models</a></li>
</ul>
<h2>Module 4: Convolutional Neural Networks (CNNs)</h2>
<ul>
<li><a href="04-01-introduction-to-cnns">Introduction to CNNs</a></li>
<li><a href="04-02-building-cnn-from-scratch">Building a CNN from Scratch</a></li>
<li><a href="04-03-transfer-learning">Transfer Learning with Pre-trained Models</a></li>
<li><a href="04-04-fine-tuning-cnns">Fine-Tuning CNNs</a></li>
</ul>
<h2>Module 5: Recurrent Neural Networks (RNNs)</h2>
<ul>
<li><a href="05-01-introduction-to-rnns">Introduction to RNNs</a></li>
<li><a href="05-02-building-rnn-from-scratch">Building an RNN from Scratch</a></li>
<li><a href="05-03-lstm-networks">Long Short-Term Memory (LSTM) Networks</a></li>
<li><a href="05-04-gru-networks">Gated Recurrent Units (GRUs)</a></li>
</ul>
<h2>Module 6: Advanced Topics</h2>
<ul>
<li><a href="06-01-gans">Generative Adversarial Networks (GANs)</a></li>
<li><a href="06-02-reinforcement-learning">Reinforcement Learning with PyTorch</a></li>
<li><a href="06-03-deploying-models">Deploying PyTorch Models</a></li>
<li><a href="06-04-optimizing-performance">Optimizing Performance</a></li>
</ul>
<h2>Module 7: Case Studies and Projects</h2>
<ul>
<li><a href="07-01-image-classification-project">Image Classification Project</a></li>
<li><a href="07-02-nlp-project">Natural Language Processing Project</a></li>
<li><a href="07-03-time-series-forecasting-project">Time Series Forecasting Project</a></li>
<li><a href="07-04-custom-project">Custom Project</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>
</body>
</html>
