<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training Loop</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/03-02-training-loop" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/03-02-training-loop" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/03-02-training-loop" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/03-02-training-loop" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/03-02-training-loop" class="px-2">CA</a>
					</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<!-- <a href="/">Home</a>  -->
									<a href="./">Course Content</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Technical Skills</a>
				<a href="/all/conocimientos">Knowledge</a>
				<a href="/all/soft_skills">Social Skills</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='03-01-data-loading-preprocessing' title="Data Loading and Preprocessing">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Training Loop</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='03-03-validation-testing' title="Validation and Testing">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>In this section, we will delve into the core of training neural networks in PyTorch: the training loop. The training loop is where the model learns from the data by iteratively updating its parameters to minimize the loss function. This process involves several key steps, which we will break down and explain in detail.</p>
</div><h1><p>Key Concepts</p>
</h1>
<div class='content'><ol>
<li><strong>Epoch</strong>: One complete pass through the entire training dataset.</li>
<li><strong>Batch</strong>: A subset of the training data used to update the model's parameters.</li>
<li><strong>Forward Pass</strong>: Calculating the output of the neural network.</li>
<li><strong>Loss Calculation</strong>: Measuring the difference between the predicted output and the actual target.</li>
<li><strong>Backward Pass</strong>: Computing the gradients of the loss with respect to the model's parameters.</li>
<li><strong>Parameter Update</strong>: Adjusting the model's parameters using the computed gradients.</li>
</ol>
</div><h1><p>Steps in a Training Loop</p>
</h1>
<div class='content'><ol>
<li><strong>Initialize the Model, Loss Function, and Optimizer</strong></li>
<li><strong>Iterate Over the Dataset</strong></li>
<li><strong>Perform Forward Pass</strong></li>
<li><strong>Compute Loss</strong></li>
<li><strong>Perform Backward Pass</strong></li>
<li><strong>Update Parameters</strong></li>
<li><strong>Track and Print Metrics</strong></li>
</ol>
</div><h1><p>Example Code</p>
</h1>
<div class='content'><p>Let's look at a practical example of a training loop in PyTorch. We'll use a simple neural network to demonstrate the process.</p>
</div><h2><p>Step 1: Initialize the Model, Loss Function, and Optimizer</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KCiMgRGVmaW5lIGEgc2ltcGxlIG5ldXJhbCBuZXR3b3JrCmNsYXNzIFNpbXBsZU5OKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZik6CiAgICAgICAgc3VwZXIoU2ltcGxlTk4sIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcigxMCwgNTApCiAgICAgICAgc2VsZi5mYzIgPSBubi5MaW5lYXIoNTAsIDEpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHgpOgogICAgICAgIHggPSB0b3JjaC5yZWx1KHNlbGYuZmMxKHgpKQogICAgICAgIHggPSBzZWxmLmZjMih4KQogICAgICAgIHJldHVybiB4CgojIEluaXRpYWxpemUgdGhlIG1vZGVsCm1vZGVsID0gU2ltcGxlTk4oKQoKIyBEZWZpbmUgdGhlIGxvc3MgZnVuY3Rpb24KY3JpdGVyaW9uID0gbm4uTVNFTG9zcygpCgojIERlZmluZSB0aGUgb3B0aW1pemVyCm9wdGltaXplciA9IG9wdGltLlNHRChtb2RlbC5wYXJhbWV0ZXJzKCksIGxyPTAuMDEp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim

# Define a simple neural network
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(10, 50)
        self.fc2 = nn.Linear(50, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Initialize the model
model = SimpleNN()

# Define the loss function
criterion = nn.MSELoss()

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)</pre></div><div class='content'></div><h2><p>Step 2: Iterate Over the Dataset</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBEdW1teSBkYXRhc2V0CmRhdGEgPSB0b3JjaC5yYW5kbigxMDAsIDEwKSAgIyAxMDAgc2FtcGxlcywgMTAgZmVhdHVyZXMgZWFjaAp0YXJnZXRzID0gdG9yY2gucmFuZG4oMTAwLCAxKSAgIyAxMDAgdGFyZ2V0IHZhbHVlcwoKIyBOdW1iZXIgb2YgZXBvY2hzCm51bV9lcG9jaHMgPSAyMAoKIyBCYXRjaCBzaXplCmJhdGNoX3NpemUgPSAxMAoKIyBEYXRhIGxvYWRlcgpkYXRhX2xvYWRlciA9IHRvcmNoLnV0aWxzLmRhdGEuRGF0YUxvYWRlcigKICAgIGRhdGFzZXQ9bGlzdCh6aXAoZGF0YSwgdGFyZ2V0cykpLAogICAgYmF0Y2hfc2l6ZT1iYXRjaF9zaXplLAogICAgc2h1ZmZsZT1UcnVlCik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Dummy dataset
data = torch.randn(100, 10)  # 100 samples, 10 features each
targets = torch.randn(100, 1)  # 100 target values

# Number of epochs
num_epochs = 20

# Batch size
batch_size = 10

# Data loader
data_loader = torch.utils.data.DataLoader(
    dataset=list(zip(data, targets)),
    batch_size=batch_size,
    shuffle=True
)</pre></div><div class='content'></div><h2><p>Step 3: Perform Forward Pass</p>
</h2>
<div class='content'></div><h2><p>Step 4: Compute Loss</p>
</h2>
<div class='content'></div><h2><p>Step 5: Perform Backward Pass</p>
</h2>
<div class='content'></div><h2><p>Step 6: Update Parameters</p>
</h2>
<div class='content'></div><h2><p>Step 7: Track and Print Metrics</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBUcmFpbmluZyBsb29wCmZvciBlcG9jaCBpbiByYW5nZShudW1fZXBvY2hzKToKICAgIGZvciBiYXRjaF9kYXRhLCBiYXRjaF90YXJnZXRzIGluIGRhdGFfbG9hZGVyOgogICAgICAgICMgWmVybyB0aGUgZ3JhZGllbnRzCiAgICAgICAgb3B0aW1pemVyLnplcm9fZ3JhZCgpCiAgICAgICAgCiAgICAgICAgIyBGb3J3YXJkIHBhc3MKICAgICAgICBvdXRwdXRzID0gbW9kZWwoYmF0Y2hfZGF0YSkKICAgICAgICAKICAgICAgICAjIENvbXB1dGUgbG9zcwogICAgICAgIGxvc3MgPSBjcml0ZXJpb24ob3V0cHV0cywgYmF0Y2hfdGFyZ2V0cykKICAgICAgICAKICAgICAgICAjIEJhY2t3YXJkIHBhc3MKICAgICAgICBsb3NzLmJhY2t3YXJkKCkKICAgICAgICAKICAgICAgICAjIFVwZGF0ZSBwYXJhbWV0ZXJzCiAgICAgICAgb3B0aW1pemVyLnN0ZXAoKQogICAgCiAgICAjIFByaW50IGVwb2NoIGFuZCBsb3NzCiAgICBwcmludChmJ0Vwb2NoIFt7ZXBvY2grMX0ve251bV9lcG9jaHN9XSwgTG9zczoge2xvc3MuaXRlbSgpOi40Zn0nKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Training loop
for epoch in range(num_epochs):
    for batch_data, batch_targets in data_loader:
        # Zero the gradients
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(batch_data)
        
        # Compute loss
        loss = criterion(outputs, batch_targets)
        
        # Backward pass
        loss.backward()
        
        # Update parameters
        optimizer.step()
    
    # Print epoch and loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h1><p>Common Mistakes and Tips</p>
</h1>
<div class='content'><ol>
<li><strong>Forgetting to Zero Gradients</strong>: Always call <code>optimizer.zero_grad()</code> before the forward pass to clear the old gradients.</li>
<li><strong>Incorrect Loss Calculation</strong>: Ensure that the loss function matches the problem type (e.g., MSE for regression, CrossEntropy for classification).</li>
<li><strong>Learning Rate</strong>: Choosing an appropriate learning rate is crucial. Too high can cause divergence, too low can slow down training.</li>
<li><strong>Batch Size</strong>: Larger batch sizes can speed up training but require more memory. Smaller batch sizes can provide more accurate gradient estimates.</li>
</ol>
</div><h1><p>Practical Exercise</p>
</h1>
<div class='content'></div><h2><p>Exercise: Implement a Training Loop</p>
</h2>
<div class='content'><ol>
<li>Define a neural network with two hidden layers.</li>
<li>Use the MNIST dataset for training.</li>
<li>Implement the training loop with the following specifications:
<ul>
<li>Use CrossEntropyLoss as the loss function.</li>
<li>Use Adam optimizer.</li>
<li>Train for 10 epochs.</li>
<li>Print the loss every epoch.</li>
</ul>
</li>
</ol>
</div><h2><p>Solution</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# Define the neural network
class NeuralNet(nn.Module):
    def __init__(self):
        super(NeuralNet, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)
    
    def forward(self, x):
        x = x.view(-1, 28*28)  # Flatten the input
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Initialize the model
model = NeuralNet()

# Define the loss function
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Load the MNIST dataset
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    for batch_data, batch_targets in train_loader:
        # Zero the gradients
        optimizer.zero_grad()
        
        # Forward pass
        outputs = model(batch_data)
        
        # Compute loss
        loss = criterion(outputs, batch_targets)
        
        # Backward pass
        loss.backward()
        
        # Update parameters
        optimizer.step()
    
    # Print epoch and loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')</pre></div><div class='content'></div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we covered the essential steps involved in a training loop in PyTorch. We discussed the key concepts, provided a detailed example, and highlighted common mistakes and tips. By understanding and implementing these steps, you can effectively train neural networks using PyTorch. In the next section, we will explore validation and testing to evaluate the performance of your trained models.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='03-01-data-loading-preprocessing' title="Data Loading and Preprocessing">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='03-03-validation-testing' title="Validation and Testing">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
