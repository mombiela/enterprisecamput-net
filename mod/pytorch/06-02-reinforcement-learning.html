<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Reinforcement Learning with PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/06-02-reinforcement-learning" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/06-02-reinforcement-learning" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/06-02-reinforcement-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/06-02-reinforcement-learning" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/06-02-reinforcement-learning" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='06-01-gans' title="Generative Adversarial Networks (GANs)">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Reinforcement Learning with PyTorch</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='06-03-deploying-models' title="Deploying PyTorch Models">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize some notion of cumulative reward. In this section, we will explore the basics of RL and how to implement RL algorithms using PyTorch.</p>
</div><h1><p>Key Concepts in Reinforcement Learning</p>
</h1>
<div class='content'><ol>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: The external system with which the agent interacts.</li>
<li><strong>State (s)</strong>: A representation of the current situation of the agent.</li>
<li><strong>Action (a)</strong>: The set of all possible moves the agent can make.</li>
<li><strong>Reward (r)</strong>: The feedback from the environment based on the action taken.</li>
<li><strong>Policy (Ï€)</strong>: The strategy that the agent employs to determine the next action based on the current state.</li>
<li><strong>Value Function (V)</strong>: The expected cumulative reward from a given state.</li>
<li><strong>Q-Function (Q)</strong>: The expected cumulative reward from a given state-action pair.</li>
</ol>
</div><h1><p>Setting Up the Environment</p>
</h1>
<div class='content'><p>Before diving into the implementation, ensure you have the necessary libraries installed:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdG9yY2ggZ3lt"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install torch gym</pre></div><div class='content'></div><h1><p>Implementing a Simple RL Algorithm: Q-Learning</p>
</h1>
<div class='content'></div><h2><p>Step 1: Import Libraries</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQppbXBvcnQgdG9yY2gKaW1wb3J0IHRvcmNoLm5uIGFzIG5uCmltcG9ydCB0b3JjaC5vcHRpbSBhcyBvcHRpbQppbXBvcnQgbnVtcHkgYXMgbnA="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np</pre></div><div class='content'></div><h2><p>Step 2: Define the Q-Network</p>
</h2>
<div class='content'><p>A Q-Network approximates the Q-Function using a neural network.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUU5ldHdvcmsobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSk6CiAgICAgICAgc3VwZXIoUU5ldHdvcmssIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcihzdGF0ZV9zaXplLCA2NCkKICAgICAgICBzZWxmLmZjMiA9IG5uLkxpbmVhcig2NCwgNjQpCiAgICAgICAgc2VsZi5mYzMgPSBubi5MaW5lYXIoNjQsIGFjdGlvbl9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMih4KSkKICAgICAgICB4ID0gc2VsZi5mYzMoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x</pre></div><div class='content'></div><h2><p>Step 3: Initialize the Environment and Network</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKc3RhdGVfc2l6ZSA9IGVudi5vYnNlcnZhdGlvbl9zcGFjZS5zaGFwZVswXQphY3Rpb25fc2l6ZSA9IGVudi5hY3Rpb25fc3BhY2UubgoKcV9uZXR3b3JrID0gUU5ldHdvcmsoc3RhdGVfc2l6ZSwgYWN0aW9uX3NpemUpCm9wdGltaXplciA9IG9wdGltLkFkYW0ocV9uZXR3b3JrLnBhcmFtZXRlcnMoKSwgbHI9MC4wMDEpCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

q_network = QNetwork(state_size, action_size)
optimizer = optim.Adam(q_network.parameters(), lr=0.001)
criterion = nn.MSELoss()</pre></div><div class='content'></div><h2><p>Step 4: Define the Training Loop</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("bnVtX2VwaXNvZGVzID0gMTAwMApnYW1tYSA9IDAuOTkgICMgRGlzY291bnQgZmFjdG9yCmVwc2lsb24gPSAxLjAgICMgRXhwbG9yYXRpb24gcmF0ZQplcHNpbG9uX2RlY2F5ID0gMC45OTUKZXBzaWxvbl9taW4gPSAwLjAxCgpmb3IgZXBpc29kZSBpbiByYW5nZShudW1fZXBpc29kZXMpOgogICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgc3RhdGUgPSB0b3JjaC5GbG9hdFRlbnNvcihzdGF0ZSkudW5zcXVlZXplKDApCiAgICB0b3RhbF9yZXdhcmQgPSAwCiAgICAKICAgIGZvciB0IGluIHJhbmdlKDIwMCk6CiAgICAgICAgIyBFcHNpbG9uLWdyZWVkeSBhY3Rpb24gc2VsZWN0aW9uCiAgICAgICAgaWYgbnAucmFuZG9tLnJhbmQoKSA8PSBlcHNpbG9uOgogICAgICAgICAgICBhY3Rpb24gPSBucC5yYW5kb20uY2hvaWNlKGFjdGlvbl9zaXplKQogICAgICAgIGVsc2U6CiAgICAgICAgICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgICAgICAgICAgcV92YWx1ZXMgPSBxX25ldHdvcmsoc3RhdGUpCiAgICAgICAgICAgICAgICBhY3Rpb24gPSB0b3JjaC5hcmdtYXgocV92YWx1ZXMpLml0ZW0oKQogICAgICAgIAogICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICBuZXh0X3N0YXRlID0gdG9yY2guRmxvYXRUZW5zb3IobmV4dF9zdGF0ZSkudW5zcXVlZXplKDApCiAgICAgICAgdG90YWxfcmV3YXJkICs9IHJld2FyZAogICAgICAgIAogICAgICAgICMgQ29tcHV0ZSB0aGUgdGFyZ2V0IFEtdmFsdWUKICAgICAgICB3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgICAgICAgICAgdGFyZ2V0X3FfdmFsdWUgPSByZXdhcmQgKyBnYW1tYSAqIHRvcmNoLm1heChxX25ldHdvcmsobmV4dF9zdGF0ZSkpCiAgICAgICAgCiAgICAgICAgIyBDb21wdXRlIHRoZSBjdXJyZW50IFEtdmFsdWUKICAgICAgICBjdXJyZW50X3FfdmFsdWUgPSBxX25ldHdvcmsoc3RhdGUpWzAsIGFjdGlvbl0KICAgICAgICAKICAgICAgICAjIENvbXB1dGUgdGhlIGxvc3MKICAgICAgICBsb3NzID0gY3JpdGVyaW9uKGN1cnJlbnRfcV92YWx1ZSwgdGFyZ2V0X3FfdmFsdWUpCiAgICAgICAgCiAgICAgICAgIyBPcHRpbWl6ZSB0aGUgUS1uZXR3b3JrCiAgICAgICAgb3B0aW1pemVyLnplcm9fZ3JhZCgpCiAgICAgICAgbG9zcy5iYWNrd2FyZCgpCiAgICAgICAgb3B0aW1pemVyLnN0ZXAoKQogICAgICAgIAogICAgICAgIHN0YXRlID0gbmV4dF9zdGF0ZQogICAgICAgIAogICAgICAgIGlmIGRvbmU6CiAgICAgICAgICAgIGJyZWFrCiAgICAKICAgICMgRGVjYXkgZXBzaWxvbgogICAgaWYgZXBzaWxvbiA+IGVwc2lsb25fbWluOgogICAgICAgIGVwc2lsb24gKj0gZXBzaWxvbl9kZWNheQogICAgCiAgICBwcmludChmIkVwaXNvZGUge2VwaXNvZGUrMX0ve251bV9lcGlzb2Rlc30sIFRvdGFsIFJld2FyZDoge3RvdGFsX3Jld2FyZH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>num_episodes = 1000
gamma = 0.99  # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_decay = 0.995
epsilon_min = 0.01

for episode in range(num_episodes):
    state = env.reset()
    state = torch.FloatTensor(state).unsqueeze(0)
    total_reward = 0
    
    for t in range(200):
        # Epsilon-greedy action selection
        if np.random.rand() &lt;= epsilon:
            action = np.random.choice(action_size)
        else:
            with torch.no_grad():
                q_values = q_network(state)
                action = torch.argmax(q_values).item()
        
        next_state, reward, done, _ = env.step(action)
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        total_reward += reward
        
        # Compute the target Q-value
        with torch.no_grad():
            target_q_value = reward + gamma * torch.max(q_network(next_state))
        
        # Compute the current Q-value
        current_q_value = q_network(state)[0, action]
        
        # Compute the loss
        loss = criterion(current_q_value, target_q_value)
        
        # Optimize the Q-network
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        state = next_state
        
        if done:
            break
    
    # Decay epsilon
    if epsilon &gt; epsilon_min:
        epsilon *= epsilon_decay
    
    print(f&quot;Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h2><p>Step 5: Evaluate the Trained Agent</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c3RhdGUgPSBlbnYucmVzZXQoKQpzdGF0ZSA9IHRvcmNoLkZsb2F0VGVuc29yKHN0YXRlKS51bnNxdWVlemUoMCkKdG90YWxfcmV3YXJkID0gMAoKZm9yIHQgaW4gcmFuZ2UoMjAwKToKICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgIHFfdmFsdWVzID0gcV9uZXR3b3JrKHN0YXRlKQogICAgICAgIGFjdGlvbiA9IHRvcmNoLmFyZ21heChxX3ZhbHVlcykuaXRlbSgpCiAgICAKICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgIG5leHRfc3RhdGUgPSB0b3JjaC5GbG9hdFRlbnNvcihuZXh0X3N0YXRlKS51bnNxdWVlemUoMCkKICAgIHRvdGFsX3Jld2FyZCArPSByZXdhcmQKICAgIHN0YXRlID0gbmV4dF9zdGF0ZQogICAgCiAgICBpZiBkb25lOgogICAgICAgIGJyZWFrCgpwcmludChmIlRvdGFsIFJld2FyZDoge3RvdGFsX3Jld2FyZH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>state = env.reset()
state = torch.FloatTensor(state).unsqueeze(0)
total_reward = 0

for t in range(200):
    with torch.no_grad():
        q_values = q_network(state)
        action = torch.argmax(q_values).item()
    
    next_state, reward, done, _ = env.step(action)
    next_state = torch.FloatTensor(next_state).unsqueeze(0)
    total_reward += reward
    state = next_state
    
    if done:
        break

print(f&quot;Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h1><p>Practical Exercises</p>
</h1>
<div class='content'></div><h2><p>Exercise 1: Modify the Q-Network Architecture</p>
</h2>
<div class='content'><p><strong>Task</strong>: Modify the Q-Network to include an additional hidden layer with 128 neurons. Train the network and observe the performance.</p>
</div><h2><p>Solution:</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUU5ldHdvcmsobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSk6CiAgICAgICAgc3VwZXIoUU5ldHdvcmssIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcihzdGF0ZV9zaXplLCA2NCkKICAgICAgICBzZWxmLmZjMiA9IG5uLkxpbmVhcig2NCwgMTI4KQogICAgICAgIHNlbGYuZmMzID0gbm4uTGluZWFyKDEyOCwgNjQpCiAgICAgICAgc2VsZi5mYzQgPSBubi5MaW5lYXIoNjQsIGFjdGlvbl9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMih4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMyh4KSkKICAgICAgICB4ID0gc2VsZi5mYzQoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x</pre></div><div class='content'></div><h2><p>Exercise 2: Implement Experience Replay</p>
</h2>
<div class='content'><p><strong>Task</strong>: Implement experience replay to store and sample past experiences to break the correlation between consecutive experiences.</p>
</div><h2><p>Solution:</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from collections import deque
import random

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

# Initialize replay buffer
replay_buffer = ReplayBuffer(10000)

# Modify the training loop to use experience replay
batch_size = 64

for episode in range(num_episodes):
    state = env.reset()
    state = torch.FloatTensor(state).unsqueeze(0)
    total_reward = 0
    
    for t in range(200):
        if np.random.rand() &lt;= epsilon:
            action = np.random.choice(action_size)
        else:
            with torch.no_grad():
                q_values = q_network(state)
                action = torch.argmax(q_values).item()
        
        next_state, reward, done, _ = env.step(action)
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        total_reward += reward
        
        replay_buffer.push(state, action, reward, next_state, done)
        
        state = next_state
        
        if len(replay_buffer) &gt; batch_size:
            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
            states = torch.cat(states)
            next_states = torch.cat(next_states)
            actions = torch.tensor(actions).unsqueeze(1)
            rewards = torch.tensor(rewards).unsqueeze(1)
            dones = torch.tensor(dones).unsqueeze(1)
            
            with torch.no_grad():
                target_q_values = rewards + gamma * torch.max(q_network(next_states), dim=1, keepdim=True)[0] * (1 - dones)
            
            current_q_values = q_network(states).gather(1, actions)
            
            loss = criterion(current_q_values, target_q_values)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        if done:
            break
    
    if epsilon &gt; epsilon_min:
        epsilon *= epsilon_decay
    
    print(f&quot;Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h1><p>Summary</p>
</h1>
<div class='content'><p>In this section, we covered the basics of reinforcement learning and implemented a simple Q-Learning algorithm using PyTorch. We also explored practical exercises to modify the Q-Network architecture and implement experience replay. These exercises help reinforce the concepts and provide hands-on experience with RL in PyTorch.</p>
<p>In the next module, we will delve into more advanced topics and explore other RL algorithms and techniques.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='06-01-gans' title="Generative Adversarial Networks (GANs)">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='06-03-deploying-models' title="Deploying PyTorch Models">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiÃ¨ncia d'Ãºs i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
