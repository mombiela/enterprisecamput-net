<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformers in PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/mod/pytorch/transformers" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/pytorch/transformers" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/pytorch/transformers" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/pytorch/transformers" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/pytorch/transformers" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='long-short-term-memory-networks-lstms'>&#x25C4;Long Short-Term Memory Networks (LSTMs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Transformers in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='custom-datasets-and-dataloaders'>Custom Datasets and DataLoaders &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction to Transformers</h1>
<div class='content'><p>Transformers are a type of model architecture that has revolutionized the field of natural language processing (NLP). They are designed to handle sequential data and have been highly successful in tasks such as language translation, text summarization, and more.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Attention Mechanism</strong>: Allows the model to focus on different parts of the input sequence when making predictions.</li>
<li><strong>Self-Attention</strong>: A type of attention mechanism where the model attends to different positions of the same sequence.</li>
<li><strong>Encoder-Decoder Architecture</strong>: The original transformer model consists of an encoder to process the input and a decoder to generate the output.</li>
</ul>
</div><h1>Setting Up PyTorch Environment</h1>
<div class='content'><p>Before diving into transformers, ensure you have PyTorch installed. You can install it using pip:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdG9yY2ggdG9yY2h2aXNpb24="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install torch torchvision</pre></div><div class='content'></div><h1>Basic Transformer Model</h1>
<div class='content'><p>PyTorch provides a <code>nn.Transformer</code> module that can be used to build transformer models.</p>
</div><h2>Example: Simple Transformer Model</h2>
<div class='content'><p>Let's start with a simple example to understand the basic structure of a transformer model.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgppbXBvcnQgdG9yY2gub3B0aW0gYXMgb3B0aW0KCiMgRGVmaW5lIHRoZSB0cmFuc2Zvcm1lciBtb2RlbApjbGFzcyBTaW1wbGVUcmFuc2Zvcm1lcihubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGlucHV0X2RpbSwgbW9kZWxfZGltLCBudW1faGVhZHMsIG51bV9sYXllcnMsIG91dHB1dF9kaW0pOgogICAgICAgIHN1cGVyKFNpbXBsZVRyYW5zZm9ybWVyLCBzZWxmKS5fX2luaXRfXygpCiAgICAgICAgc2VsZi50cmFuc2Zvcm1lciA9IG5uLlRyYW5zZm9ybWVyKGRfbW9kZWw9bW9kZWxfZGltLCBuaGVhZD1udW1faGVhZHMsIG51bV9lbmNvZGVyX2xheWVycz1udW1fbGF5ZXJzLCBudW1fZGVjb2Rlcl9sYXllcnM9bnVtX2xheWVycykKICAgICAgICBzZWxmLmZjID0gbm4uTGluZWFyKG1vZGVsX2RpbSwgb3V0cHV0X2RpbSkKICAgIAogICAgZGVmIGZvcndhcmQoc2VsZiwgc3JjLCB0Z3QpOgogICAgICAgIHRyYW5zZm9ybWVyX291dHB1dCA9IHNlbGYudHJhbnNmb3JtZXIoc3JjLCB0Z3QpCiAgICAgICAgb3V0cHV0ID0gc2VsZi5mYyh0cmFuc2Zvcm1lcl9vdXRwdXQpCiAgICAgICAgcmV0dXJuIG91dHB1dAoKIyBIeXBlcnBhcmFtZXRlcnMKaW5wdXRfZGltID0gMTAKbW9kZWxfZGltID0gNTEyCm51bV9oZWFkcyA9IDgKbnVtX2xheWVycyA9IDYKb3V0cHV0X2RpbSA9IDEwCgojIEluaXRpYWxpemUgdGhlIG1vZGVsLCBsb3NzIGZ1bmN0aW9uLCBhbmQgb3B0aW1pemVyCm1vZGVsID0gU2ltcGxlVHJhbnNmb3JtZXIoaW5wdXRfZGltLCBtb2RlbF9kaW0sIG51bV9oZWFkcywgbnVtX2xheWVycywgb3V0cHV0X2RpbSkKY3JpdGVyaW9uID0gbm4uTVNFTG9zcygpCm9wdGltaXplciA9IG9wdGltLkFkYW0obW9kZWwucGFyYW1ldGVycygpLCBscj0wLjAwMSkKCiMgRHVtbXkgZGF0YQpzcmMgPSB0b3JjaC5yYW5kKCgxMCwgMzIsIGlucHV0X2RpbSkpICAjIChzZXF1ZW5jZV9sZW5ndGgsIGJhdGNoX3NpemUsIGlucHV0X2RpbSkKdGd0ID0gdG9yY2gucmFuZCgoMjAsIDMyLCBpbnB1dF9kaW0pKSAgIyAoc2VxdWVuY2VfbGVuZ3RoLCBiYXRjaF9zaXplLCBpbnB1dF9kaW0pCm91dHB1dCA9IG1vZGVsKHNyYywgdGd0KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn
import torch.optim as optim

# Define the transformer model
class SimpleTransformer(nn.Module):
    def __init__(self, input_dim, model_dim, num_heads, num_layers, output_dim):
        super(SimpleTransformer, self).__init__()
        self.transformer = nn.Transformer(d_model=model_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.fc = nn.Linear(model_dim, output_dim)
    
    def forward(self, src, tgt):
        transformer_output = self.transformer(src, tgt)
        output = self.fc(transformer_output)
        return output

# Hyperparameters
input_dim = 10
model_dim = 512
num_heads = 8
num_layers = 6
output_dim = 10

# Initialize the model, loss function, and optimizer
model = SimpleTransformer(input_dim, model_dim, num_heads, num_layers, output_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Dummy data
src = torch.rand((10, 32, input_dim))  # (sequence_length, batch_size, input_dim)
tgt = torch.rand((20, 32, input_dim))  # (sequence_length, batch_size, input_dim)
output = model(src, tgt)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Model Initialization</strong>: We define a <code>SimpleTransformer</code> class that inherits from <code>nn.Module</code>.</li>
<li><strong>Transformer Layer</strong>: We use <code>nn.Transformer</code> to create the transformer architecture.</li>
<li><strong>Fully Connected Layer</strong>: A linear layer to map the transformer output to the desired output dimension.</li>
<li><strong>Forward Method</strong>: Defines how the input data flows through the model.</li>
<li><strong>Dummy Data</strong>: We create random tensors to simulate input and target sequences.</li>
</ul>
</div><h1>Advanced Transformer Techniques</h1>
<div class='content'><p>As you progress, you can explore more advanced techniques and customizations.</p>
</div><h2>Custom Attention Mechanism</h2>
<div class='content'><p>You can create custom attention mechanisms to better suit specific tasks.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgQ3VzdG9tQXR0ZW50aW9uKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgbW9kZWxfZGltLCBudW1faGVhZHMpOgogICAgICAgIHN1cGVyKEN1c3RvbUF0dGVudGlvbiwgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYuYXR0ZW50aW9uID0gbm4uTXVsdGloZWFkQXR0ZW50aW9uKGVtYmVkX2RpbT1tb2RlbF9kaW0sIG51bV9oZWFkcz1udW1faGVhZHMpCiAgICAKICAgIGRlZiBmb3J3YXJkKHNlbGYsIHF1ZXJ5LCBrZXksIHZhbHVlKToKICAgICAgICBhdHRuX291dHB1dCwgXyA9IHNlbGYuYXR0ZW50aW9uKHF1ZXJ5LCBrZXksIHZhbHVlKQogICAgICAgIHJldHVybiBhdHRuX291dHB1dAoKIyBFeGFtcGxlIHVzYWdlCmN1c3RvbV9hdHRlbnRpb24gPSBDdXN0b21BdHRlbnRpb24obW9kZWxfZGltPTUxMiwgbnVtX2hlYWRzPTgpCnF1ZXJ5ID0gdG9yY2gucmFuZCgoMTAsIDMyLCA1MTIpKSAgIyAoc2VxdWVuY2VfbGVuZ3RoLCBiYXRjaF9zaXplLCBtb2RlbF9kaW0pCmtleSA9IHRvcmNoLnJhbmQoKDEwLCAzMiwgNTEyKSkKdmFsdWUgPSB0b3JjaC5yYW5kKCgxMCwgMzIsIDUxMikpCmF0dG5fb3V0cHV0ID0gY3VzdG9tX2F0dGVudGlvbihxdWVyeSwga2V5LCB2YWx1ZSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class CustomAttention(nn.Module):
    def __init__(self, model_dim, num_heads):
        super(CustomAttention, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim=model_dim, num_heads=num_heads)
    
    def forward(self, query, key, value):
        attn_output, _ = self.attention(query, key, value)
        return attn_output

# Example usage
custom_attention = CustomAttention(model_dim=512, num_heads=8)
query = torch.rand((10, 32, 512))  # (sequence_length, batch_size, model_dim)
key = torch.rand((10, 32, 512))
value = torch.rand((10, 32, 512))
attn_output = custom_attention(query, key, value)</pre></div><div class='content'></div><h2>Fine-Tuning Pretrained Transformers</h2>
<div class='content'><p>PyTorch also supports fine-tuning pretrained transformer models from libraries like Hugging Face's <code>transformers</code>.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSB0cmFuc2Zvcm1lcnMgaW1wb3J0IEJlcnRNb2RlbCwgQmVydFRva2VuaXplcgoKIyBMb2FkIHByZXRyYWluZWQgQkVSVCBtb2RlbCBhbmQgdG9rZW5pemVyCm1vZGVsX25hbWUgPSAnYmVydC1iYXNlLXVuY2FzZWQnCnRva2VuaXplciA9IEJlcnRUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUpCm1vZGVsID0gQmVydE1vZGVsLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQoKIyBUb2tlbml6ZSBpbnB1dAppbnB1dF90ZXh0ID0gIkhlbGxvLCBob3cgYXJlIHlvdT8iCmlucHV0cyA9IHRva2VuaXplcihpbnB1dF90ZXh0LCByZXR1cm5fdGVuc29ycz0ncHQnKQoKIyBGb3J3YXJkIHBhc3MKb3V0cHV0cyA9IG1vZGVsKCoqaW5wdXRzKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from transformers import BertModel, BertTokenizer

# Load pretrained BERT model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)

# Tokenize input
input_text = &quot;Hello, how are you?&quot;
inputs = tokenizer(input_text, return_tensors='pt')

# Forward pass
outputs = model(**inputs)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Custom Attention</strong>: We define a <code>CustomAttention</code> class to create a custom multi-head attention mechanism.</li>
<li><strong>Pretrained Models</strong>: Using the <code>transformers</code> library, we load a pretrained BERT model and tokenizer, tokenize input text, and perform a forward pass.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>Transformers are a powerful tool in modern NLP, and PyTorch provides flexible modules to build and customize these models. Starting from basic transformer models to advanced techniques like custom attention mechanisms and fine-tuning pretrained models, you can leverage PyTorch to implement state-of-the-art NLP solutions. Continue experimenting with different architectures and datasets to deepen your understanding and improve your models.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='long-short-term-memory-networks-lstms'>&#x25C4;Long Short-Term Memory Networks (LSTMs)</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Transformers in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='custom-datasets-and-dataloaders'>Custom Datasets and DataLoaders &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
