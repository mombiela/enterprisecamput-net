<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kafka with Hadoop</title>

    <link rel="alternate" href="https://campusempresa.com/mod/kafka/06-01-kafka-hadoop" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/kafka/06-01-kafka-hadoop" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/kafka/06-01-kafka-hadoop" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/kafka/06-01-kafka-hadoop" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/kafka/06-01-kafka-hadoop" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='05-04-kafka-streams-advanced' title="Kafka Streams Advanced">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Kafka with Hadoop</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='06-02-kafka-spark' title="Kafka with Spark">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>In this section, we will explore how Kafka integrates with Hadoop, a popular framework for distributed storage and processing of large data sets. This integration allows for efficient data ingestion, processing, and analysis, leveraging the strengths of both Kafka and Hadoop.</p>
</div><h1>Objectives</h1>
<div class='content'><p>By the end of this section, you will:</p>
<ul>
<li>Understand the benefits of integrating Kafka with Hadoop.</li>
<li>Learn how to set up Kafka to work with Hadoop.</li>
<li>Explore practical examples of Kafka-Hadoop integration.</li>
<li>Complete exercises to reinforce your understanding.</li>
</ul>
</div><h1>Benefits of Integrating Kafka with Hadoop</h1>
<div class='content'><p>Integrating Kafka with Hadoop offers several advantages:</p>
<ol>
<li><strong>Real-time Data Ingestion</strong>: Kafka can stream data in real-time to Hadoop, enabling timely data processing and analysis.</li>
<li><strong>Scalability</strong>: Both Kafka and Hadoop are designed to scale horizontally, making them suitable for handling large volumes of data.</li>
<li><strong>Fault Tolerance</strong>: Kafka's distributed architecture ensures data durability, while Hadoop's HDFS provides reliable storage.</li>
<li><strong>Flexibility</strong>: Kafka can ingest data from various sources, and Hadoop can process and analyze this data using different tools and frameworks.</li>
</ol>
</div><h1>Setting Up Kafka to Work with Hadoop</h1>
<div class='content'></div><h2>Prerequisites</h2>
<div class='content'><ul>
<li>A running Kafka cluster.</li>
<li>A Hadoop cluster with HDFS (Hadoop Distributed File System) set up.</li>
<li>Kafka Connect installed.</li>
</ul>
</div><h2>Step-by-Step Setup</h2>
<div class='content'><ol>
<li>
<p><strong>Install Kafka Connect HDFS Connector</strong>:
Kafka Connect provides a connector for HDFS, which allows Kafka to write data directly to HDFS.</p>
<pre><code class="language-bash">confluent-hub install confluentinc/kafka-connect-hdfs:latest
</code></pre>
</li>
<li>
<p><strong>Configure the HDFS Connector</strong>:
Create a configuration file for the HDFS connector (e.g., <code>hdfs-sink.properties</code>).</p>
<pre><code class="language-properties">name=hdfs-sink-connector
connector.class=io.confluent.connect.hdfs.HdfsSinkConnector
tasks.max=1
topics=your_topic
hdfs.url=hdfs://namenode:8020
flush.size=1000
</code></pre>
<ul>
<li><code>name</code>: Name of the connector.</li>
<li><code>connector.class</code>: The class for the HDFS sink connector.</li>
<li><code>tasks.max</code>: Maximum number of tasks to use for this connector.</li>
<li><code>topics</code>: The Kafka topic(s) to read from.</li>
<li><code>hdfs.url</code>: The URL of the HDFS namenode.</li>
<li><code>flush.size</code>: Number of records to write before flushing to HDFS.</li>
</ul>
</li>
<li>
<p><strong>Start the HDFS Connector</strong>:
Use the Kafka Connect REST API to start the connector.</p>
<pre><code class="language-bash">curl -X POST -H &quot;Content-Type: application/json&quot; --data @hdfs-sink.properties http://localhost:8083/connectors
</code></pre>
</li>
<li>
<p><strong>Verify Data in HDFS</strong>:
Check the HDFS directory to ensure that data from Kafka is being written correctly.</p>
<pre><code class="language-bash">hdfs dfs -ls /path/to/hdfs/directory
</code></pre>
</li>
</ol>
</div><h1>Practical Example</h1>
<div class='content'></div><h2>Example: Streaming Logs from Kafka to Hadoop</h2>
<div class='content'><ol>
<li>
<p><strong>Produce Log Messages to Kafka</strong>:
Create a simple producer to send log messages to a Kafka topic.</p>
<pre><code class="language-python">from kafka import KafkaProducer
import json

producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))

log_message = {
    'timestamp': '2023-10-01T12:00:00Z',
    'level': 'INFO',
    'message': 'This is a log message'
}

producer.send('logs', log_message)
producer.flush()
</code></pre>
</li>
<li>
<p><strong>Configure and Start the HDFS Connector</strong>:
Use the configuration steps mentioned earlier to set up the HDFS connector.</p>
</li>
<li>
<p><strong>Verify Data in HDFS</strong>:
Check the HDFS directory to see the ingested log messages.</p>
<pre><code class="language-bash">hdfs dfs -cat /path/to/hdfs/directory/logs/part-00000
</code></pre>
</li>
</ol>
</div><h1>Exercises</h1>
<div class='content'></div><h2>Exercise 1: Set Up Kafka-Hadoop Integration</h2>
<div class='content'><ol>
<li>Install the Kafka Connect HDFS connector.</li>
<li>Configure the connector to write data from a Kafka topic to HDFS.</li>
<li>Produce sample messages to the Kafka topic.</li>
<li>Verify that the messages are written to HDFS.</li>
</ol>
</div><h2>Exercise 2: Stream Sensor Data to Hadoop</h2>
<div class='content'><ol>
<li>Create a Kafka producer to send sensor data (e.g., temperature, humidity) to a Kafka topic.</li>
<li>Configure the HDFS connector to write the sensor data to HDFS.</li>
<li>Verify the data in HDFS.</li>
</ol>
</div><h2>Solutions</h2>
<div class='content'><h4>Solution to Exercise 1</h4>
<ol>
<li>
<p><strong>Install the Kafka Connect HDFS connector</strong>:</p>
<pre><code class="language-bash">confluent-hub install confluentinc/kafka-connect-hdfs:latest
</code></pre>
</li>
<li>
<p><strong>Configure the connector</strong>:
Create <code>hdfs-sink.properties</code>:</p>
<pre><code class="language-properties">name=hdfs-sink-connector
connector.class=io.confluent.connect.hdfs.HdfsSinkConnector
tasks.max=1
topics=sample_topic
hdfs.url=hdfs://namenode:8020
flush.size=1000
</code></pre>
</li>
<li>
<p><strong>Start the connector</strong>:</p>
<pre><code class="language-bash">curl -X POST -H &quot;Content-Type: application/json&quot; --data @hdfs-sink.properties http://localhost:8083/connectors
</code></pre>
</li>
<li>
<p><strong>Produce sample messages</strong>:</p>
<pre><code class="language-python">from kafka import KafkaProducer
import json

producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))

sample_message = {
    'id': 1,
    'value': 'sample data'
}

producer.send('sample_topic', sample_message)
producer.flush()
</code></pre>
</li>
<li>
<p><strong>Verify data in HDFS</strong>:</p>
<pre><code class="language-bash">hdfs dfs -cat /path/to/hdfs/directory/sample_topic/part-00000
</code></pre>
</li>
</ol>
<h4>Solution to Exercise 2</h4>
<ol>
<li>
<p><strong>Create a Kafka producer for sensor data</strong>:</p>
<pre><code class="language-python">from kafka import KafkaProducer
import json

producer = KafkaProducer(bootstrap_servers='localhost:9092', value_serializer=lambda v: json.dumps(v).encode('utf-8'))

sensor_data = {
    'timestamp': '2023-10-01T12:00:00Z',
    'temperature': 22.5,
    'humidity': 60
}

producer.send('sensor_data', sensor_data)
producer.flush()
</code></pre>
</li>
<li>
<p><strong>Configure the HDFS connector</strong>:
Create <code>hdfs-sink.properties</code>:</p>
<pre><code class="language-properties">name=hdfs-sink-connector
connector.class=io.confluent.connect.hdfs.HdfsSinkConnector
tasks.max=1
topics=sensor_data
hdfs.url=hdfs://namenode:8020
flush.size=1000
</code></pre>
</li>
<li>
<p><strong>Start the connector</strong>:</p>
<pre><code class="language-bash">curl -X POST -H &quot;Content-Type: application/json&quot; --data @hdfs-sink.properties http://localhost:8083/connectors
</code></pre>
</li>
<li>
<p><strong>Verify data in HDFS</strong>:</p>
<pre><code class="language-bash">hdfs dfs -cat /path/to/hdfs/directory/sensor_data/part-00000
</code></pre>
</li>
</ol>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we explored the integration of Kafka with Hadoop, including the benefits, setup process, and practical examples. By completing the exercises, you should now have a solid understanding of how to stream data from Kafka to Hadoop for efficient storage and processing. This knowledge will be valuable as you continue to build and scale data pipelines using Kafka and Hadoop.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='05-04-kafka-streams-advanced' title="Kafka Streams Advanced">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='06-02-kafka-spark' title="Kafka with Spark">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
