<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Apache Spark</title>

    <link rel="alternate" href="https://campusempresa.com/mod/procesamiento_datos_masivos/03-02-apache-spark" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/procesamiento_datos_masivos/03-02-apache-spark" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/procesamiento_datos_masivos/03-02-apache-spark" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/procesamiento_datos_masivos/03-02-apache-spark" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/procesamiento_datos_masivos/03-02-apache-spark" class="px-2">CA</a>
					</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<!-- <a href="/">Home</a>  -->
									<a href="./">Course Content</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Technical Skills</a>
				<a href="/all/conocimientos">Knowledge</a>
				<a href="/all/soft_skills">Social Skills</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='03-01-mapreduce' title="MapReduce">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Apache Spark</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='03-03-real-time-processing' title="Real-Time Processing">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Apache Spark is an open-source unified analytics engine for large-scale data processing. It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance.</p>
</div><h1><p>Key Concepts</p>
</h1>
<div class='content'></div><h2><ol>
<li>Resilient Distributed Datasets (RDDs)</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: RDDs are the fundamental data structure of Spark. They are immutable distributed collections of objects that can be processed in parallel.</li>
<li><strong>Operations</strong>:
<ul>
<li><strong>Transformations</strong>: Operations that create a new RDD from an existing one (e.g., <code>map</code>, <code>filter</code>).</li>
<li><strong>Actions</strong>: Operations that return a value to the driver program after running a computation on the RDD (e.g., <code>count</code>, <code>collect</code>).</li>
</ul>
</li>
</ul>
</div><h2><ol start="2">
<li>Spark SQL</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: Spark SQL is a module for structured data processing. It provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine.</li>
<li><strong>DataFrames</strong>: Distributed collections of data organized into named columns, similar to a table in a relational database.</li>
</ul>
</div><h2><ol start="3">
<li>Spark Streaming</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.</li>
<li><strong>DStreams</strong>: Discretized Streams, which are a series of RDDs representing a continuous stream of data.</li>
</ul>
</div><h2><ol start="4">
<li>MLlib</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: MLlib is Sparkâ€™s scalable machine learning library. It provides various algorithms and utilities for classification, regression, clustering, collaborative filtering, and more.</li>
</ul>
</div><h2><ol start="5">
<li>GraphX</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: GraphX is a distributed graph-processing framework on top of Spark. It provides an API for graphs and graph-parallel computation.</li>
</ul>
</div><h1><p>Practical Examples</p>
</h1>
<div class='content'></div><h2><p>Example 1: Basic RDD Operations</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrIGltcG9ydCBTcGFya0NvbnRleHQKCiMgSW5pdGlhbGl6ZSBTcGFya0NvbnRleHQKc2MgPSBTcGFya0NvbnRleHQoImxvY2FsIiwgIkJhc2ljIFJERCBPcGVyYXRpb25zIikKCiMgQ3JlYXRlIGFuIFJERApkYXRhID0gWzEsIDIsIDMsIDQsIDVdCnJkZCA9IHNjLnBhcmFsbGVsaXplKGRhdGEpCgojIFRyYW5zZm9ybWF0aW9uOiBtYXAKc3F1YXJlZF9yZGQgPSByZGQubWFwKGxhbWJkYSB4OiB4ICogeCkKCiMgQWN0aW9uOiBjb2xsZWN0CnJlc3VsdCA9IHNxdWFyZWRfcmRkLmNvbGxlY3QoKQoKcHJpbnQocmVzdWx0KSAgIyBPdXRwdXQ6IFsxLCA0LCA5LCAxNiwgMjVdCgojIFN0b3AgU3BhcmtDb250ZXh0CnNjLnN0b3AoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext(&quot;local&quot;, &quot;Basic RDD Operations&quot;)

# Create an RDD
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Transformation: map
squared_rdd = rdd.map(lambda x: x * x)

# Action: collect
result = squared_rdd.collect()

print(result)  # Output: [1, 4, 9, 16, 25]

# Stop SparkContext
sc.stop()</pre></div><div class='content'></div><h2><p>Example 2: DataFrame Operations with Spark SQL</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgU3BhcmtTZXNzaW9uCgojIEluaXRpYWxpemUgU3BhcmtTZXNzaW9uCnNwYXJrID0gU3BhcmtTZXNzaW9uLmJ1aWxkZXIuYXBwTmFtZSgiRGF0YUZyYW1lIE9wZXJhdGlvbnMiKS5nZXRPckNyZWF0ZSgpCgojIENyZWF0ZSBhIERhdGFGcmFtZQpkYXRhID0gWygiQWxpY2UiLCAzNCksICgiQm9iIiwgNDUpLCAoIkNhdGh5IiwgMjkpXQpjb2x1bW5zID0gWyJOYW1lIiwgIkFnZSJdCmRmID0gc3BhcmsuY3JlYXRlRGF0YUZyYW1lKGRhdGEsIGNvbHVtbnMpCgojIFNob3cgdGhlIERhdGFGcmFtZQpkZi5zaG93KCkKCiMgUGVyZm9ybSBTUUwgcXVlcnkKZGYuY3JlYXRlT3JSZXBsYWNlVGVtcFZpZXcoInBlb3BsZSIpCnJlc3VsdCA9IHNwYXJrLnNxbCgiU0VMRUNUIE5hbWUsIEFnZSBGUk9NIHBlb3BsZSBXSEVSRSBBZ2UgPiAzMCIpCgpyZXN1bHQuc2hvdygpCgojIFN0b3AgU3BhcmtTZXNzaW9uCnNwYXJrLnN0b3AoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder.appName(&quot;DataFrame Operations&quot;).getOrCreate()

# Create a DataFrame
data = [(&quot;Alice&quot;, 34), (&quot;Bob&quot;, 45), (&quot;Cathy&quot;, 29)]
columns = [&quot;Name&quot;, &quot;Age&quot;]
df = spark.createDataFrame(data, columns)

# Show the DataFrame
df.show()

# Perform SQL query
df.createOrReplaceTempView(&quot;people&quot;)
result = spark.sql(&quot;SELECT Name, Age FROM people WHERE Age &gt; 30&quot;)

result.show()

# Stop SparkSession
spark.stop()</pre></div><div class='content'></div><h2><p>Example 3: Spark Streaming</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrIGltcG9ydCBTcGFya0NvbnRleHQKZnJvbSBweXNwYXJrLnN0cmVhbWluZyBpbXBvcnQgU3RyZWFtaW5nQ29udGV4dAoKIyBJbml0aWFsaXplIFNwYXJrQ29udGV4dCBhbmQgU3RyZWFtaW5nQ29udGV4dApzYyA9IFNwYXJrQ29udGV4dCgibG9jYWxbMl0iLCAiTmV0d29ya1dvcmRDb3VudCIpCnNzYyA9IFN0cmVhbWluZ0NvbnRleHQoc2MsIDEpCgojIENyZWF0ZSBhIERTdHJlYW0gdGhhdCB3aWxsIGNvbm5lY3QgdG8gaG9zdG5hbWU6cG9ydApsaW5lcyA9IHNzYy5zb2NrZXRUZXh0U3RyZWFtKCJsb2NhbGhvc3QiLCA5OTk5KQoKIyBTcGxpdCBlYWNoIGxpbmUgaW50byB3b3Jkcwp3b3JkcyA9IGxpbmVzLmZsYXRNYXAobGFtYmRhIGxpbmU6IGxpbmUuc3BsaXQoIiAiKSkKCiMgQ291bnQgZWFjaCB3b3JkIGluIGVhY2ggYmF0Y2gKcGFpcnMgPSB3b3Jkcy5tYXAobGFtYmRhIHdvcmQ6ICh3b3JkLCAxKSkKd29yZF9jb3VudHMgPSBwYWlycy5yZWR1Y2VCeUtleShsYW1iZGEgeCwgeTogeCArIHkpCgojIFByaW50IHRoZSBmaXJzdCB0ZW4gZWxlbWVudHMgb2YgZWFjaCBSREQgZ2VuZXJhdGVkIGluIHRoaXMgRFN0cmVhbSB0byB0aGUgY29uc29sZQp3b3JkX2NvdW50cy5wcHJpbnQoKQoKIyBTdGFydCB0aGUgY29tcHV0YXRpb24Kc3NjLnN0YXJ0KCkKCiMgV2FpdCBmb3IgdGhlIGNvbXB1dGF0aW9uIHRvIHRlcm1pbmF0ZQpzc2MuYXdhaXRUZXJtaW5hdGlvbigp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark import SparkContext
from pyspark.streaming import StreamingContext

# Initialize SparkContext and StreamingContext
sc = SparkContext(&quot;local[2]&quot;, &quot;NetworkWordCount&quot;)
ssc = StreamingContext(sc, 1)

# Create a DStream that will connect to hostname:port
lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)

# Split each line into words
words = lines.flatMap(lambda line: line.split(&quot; &quot;))

# Count each word in each batch
pairs = words.map(lambda word: (word, 1))
word_counts = pairs.reduceByKey(lambda x, y: x + y)

# Print the first ten elements of each RDD generated in this DStream to the console
word_counts.pprint()

# Start the computation
ssc.start()

# Wait for the computation to terminate
ssc.awaitTermination()</pre></div><div class='content'></div><h1><p>Exercises</p>
</h1>
<div class='content'></div><h2><p>Exercise 1: Create and Transform RDDs</p>
</h2>
<div class='content'><p><strong>Task</strong>: Create an RDD from a list of numbers and perform the following transformations:</p>
<ol>
<li>Filter out even numbers.</li>
<li>Multiply each remaining number by 10.</li>
<li>Collect and print the results.</li>
</ol>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrIGltcG9ydCBTcGFya0NvbnRleHQKCiMgSW5pdGlhbGl6ZSBTcGFya0NvbnRleHQKc2MgPSBTcGFya0NvbnRleHQoImxvY2FsIiwgIkV4ZXJjaXNlIDEiKQoKIyBDcmVhdGUgYW4gUkRECmRhdGEgPSBbMSwgMiwgMywgNCwgNSwgNiwgNywgOCwgOSwgMTBdCnJkZCA9IHNjLnBhcmFsbGVsaXplKGRhdGEpCgojIEZpbHRlciBvdXQgZXZlbiBudW1iZXJzCmZpbHRlcmVkX3JkZCA9IHJkZC5maWx0ZXIobGFtYmRhIHg6IHggJSAyICE9IDApCgojIE11bHRpcGx5IGVhY2ggcmVtYWluaW5nIG51bWJlciBieSAxMAp0cmFuc2Zvcm1lZF9yZGQgPSBmaWx0ZXJlZF9yZGQubWFwKGxhbWJkYSB4OiB4ICogMTApCgojIENvbGxlY3QgYW5kIHByaW50IHRoZSByZXN1bHRzCnJlc3VsdCA9IHRyYW5zZm9ybWVkX3JkZC5jb2xsZWN0KCkKcHJpbnQocmVzdWx0KSAgIyBPdXRwdXQ6IFsxMCwgMzAsIDUwLCA3MCwgOTBdCgojIFN0b3AgU3BhcmtDb250ZXh0CnNjLnN0b3AoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark import SparkContext

# Initialize SparkContext
sc = SparkContext(&quot;local&quot;, &quot;Exercise 1&quot;)

# Create an RDD
data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
rdd = sc.parallelize(data)

# Filter out even numbers
filtered_rdd = rdd.filter(lambda x: x % 2 != 0)

# Multiply each remaining number by 10
transformed_rdd = filtered_rdd.map(lambda x: x * 10)

# Collect and print the results
result = transformed_rdd.collect()
print(result)  # Output: [10, 30, 50, 70, 90]

# Stop SparkContext
sc.stop()</pre></div><div class='content'></div><h2><p>Exercise 2: DataFrame Operations</p>
</h2>
<div class='content'><p><strong>Task</strong>: Create a DataFrame from a list of tuples containing names and ages. Perform the following operations:</p>
<ol>
<li>Filter out rows where age is less than 30.</li>
<li>Select only the &quot;Name&quot; column.</li>
<li>Show the results.</li>
</ol>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBweXNwYXJrLnNxbCBpbXBvcnQgU3BhcmtTZXNzaW9uCgojIEluaXRpYWxpemUgU3BhcmtTZXNzaW9uCnNwYXJrID0gU3BhcmtTZXNzaW9uLmJ1aWxkZXIuYXBwTmFtZSgiRXhlcmNpc2UgMiIpLmdldE9yQ3JlYXRlKCkKCiMgQ3JlYXRlIGEgRGF0YUZyYW1lCmRhdGEgPSBbKCJBbGljZSIsIDM0KSwgKCJCb2IiLCA0NSksICgiQ2F0aHkiLCAyOSksICgiRGF2aWQiLCAyNSldCmNvbHVtbnMgPSBbIk5hbWUiLCAiQWdlIl0KZGYgPSBzcGFyay5jcmVhdGVEYXRhRnJhbWUoZGF0YSwgY29sdW1ucykKCiMgRmlsdGVyIG91dCByb3dzIHdoZXJlIGFnZSBpcyBsZXNzIHRoYW4gMzAKZmlsdGVyZWRfZGYgPSBkZi5maWx0ZXIoZGYuQWdlID49IDMwKQoKIyBTZWxlY3Qgb25seSB0aGUgIk5hbWUiIGNvbHVtbgpyZXN1bHRfZGYgPSBmaWx0ZXJlZF9kZi5zZWxlY3QoIk5hbWUiKQoKIyBTaG93IHRoZSByZXN1bHRzCnJlc3VsdF9kZi5zaG93KCkKCiMgU3RvcCBTcGFya1Nlc3Npb24Kc3Bhcmsuc3RvcCgp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder.appName(&quot;Exercise 2&quot;).getOrCreate()

# Create a DataFrame
data = [(&quot;Alice&quot;, 34), (&quot;Bob&quot;, 45), (&quot;Cathy&quot;, 29), (&quot;David&quot;, 25)]
columns = [&quot;Name&quot;, &quot;Age&quot;]
df = spark.createDataFrame(data, columns)

# Filter out rows where age is less than 30
filtered_df = df.filter(df.Age &gt;= 30)

# Select only the &quot;Name&quot; column
result_df = filtered_df.select(&quot;Name&quot;)

# Show the results
result_df.show()

# Stop SparkSession
spark.stop()</pre></div><div class='content'></div><h1><p>Common Mistakes and Tips</p>
</h1>
<div class='content'><ul>
<li>
<p><strong>Mistake</strong>: Not initializing the SparkContext or SparkSession properly.</p>
<ul>
<li><strong>Tip</strong>: Always ensure that the SparkContext or SparkSession is initialized before performing any operations.</li>
</ul>
</li>
<li>
<p><strong>Mistake</strong>: Forgetting to stop the SparkContext or SparkSession.</p>
<ul>
<li><strong>Tip</strong>: Always stop the SparkContext or SparkSession at the end of your program to release resources.</li>
</ul>
</li>
<li>
<p><strong>Mistake</strong>: Misunderstanding the difference between transformations and actions.</p>
<ul>
<li><strong>Tip</strong>: Remember that transformations are lazy and only define a new RDD, while actions trigger the execution of the transformations.</li>
</ul>
</li>
</ul>
</div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we explored Apache Spark, a powerful tool for large-scale data processing. We covered its key components, including RDDs, Spark SQL, Spark Streaming, MLlib, and GraphX. Through practical examples and exercises, you learned how to create and manipulate RDDs and DataFrames, and how to perform stream processing. Understanding these concepts and tools will enable you to handle massive datasets efficiently and effectively. In the next module, we will delve into real-time processing techniques, further expanding your big data processing toolkit.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='03-01-mapreduce' title="MapReduce">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='03-03-real-time-processing' title="Real-Time Processing">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
