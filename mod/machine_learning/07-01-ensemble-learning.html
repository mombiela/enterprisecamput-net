<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ensemble Learning</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/07-01-ensemble-learning" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/07-01-ensemble-learning" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/07-01-ensemble-learning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 px-0 py-2 py-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 px-0 py-2 py-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/machine_learning/07-01-ensemble-learning" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/07-01-ensemble-learning" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='06-04-overfitting-underfitting' title="Overfitting and Underfitting">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Ensemble Learning</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='07-02-gradient-boosting' title="Gradient Boosting">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Ensemble learning is a powerful machine learning technique where multiple models, often referred to as &quot;weak learners,&quot; are combined to produce a stronger model. The idea is that by aggregating the predictions of several models, the ensemble can achieve better performance and generalization than any individual model.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ol>
<li><strong>Weak Learners</strong>: These are models that perform slightly better than random guessing. Examples include decision stumps (single-level decision trees) and simple linear classifiers.</li>
<li><strong>Strong Learner</strong>: An ensemble of weak learners that performs significantly better than any single weak learner.</li>
<li><strong>Diversity</strong>: The individual models should be diverse, meaning they should make different errors. This diversity is crucial for the ensemble to perform well.</li>
<li><strong>Aggregation Methods</strong>: Techniques used to combine the predictions of the weak learners. Common methods include averaging, voting, and weighted voting.</li>
</ol>
</div><h1>Types of Ensemble Methods</h1>
<div class='content'><ol>
<li><strong>Bagging (Bootstrap Aggregating)</strong></li>
<li><strong>Boosting</strong></li>
<li><strong>Stacking</strong></li>
</ol>
</div><h2>Bagging (Bootstrap Aggregating)</h2>
<div class='content'><p>Bagging involves training multiple instances of the same model on different subsets of the training data, generated by bootstrapping (random sampling with replacement). The final prediction is typically made by averaging the predictions (for regression) or majority voting (for classification).</p>
<h4>Example: Random Forest</h4>
<p>Random Forest is a popular bagging method that uses decision trees as the base learners. Each tree is trained on a different bootstrap sample of the data, and the final prediction is made by averaging the predictions of all trees.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBSYW5kb21Gb3Jlc3RDbGFzc2lmaWVyCmZyb20gc2tsZWFybi5kYXRhc2V0cyBpbXBvcnQgbG9hZF9pcmlzCmZyb20gc2tsZWFybi5tb2RlbF9zZWxlY3Rpb24gaW1wb3J0IHRyYWluX3Rlc3Rfc3BsaXQKZnJvbSBza2xlYXJuLm1ldHJpY3MgaW1wb3J0IGFjY3VyYWN5X3Njb3JlCgojIExvYWQgZGF0YXNldAppcmlzID0gbG9hZF9pcmlzKCkKWCwgeSA9IGlyaXMuZGF0YSwgaXJpcy50YXJnZXQKCiMgU3BsaXQgZGF0YXNldCBpbnRvIHRyYWluaW5nIGFuZCB0ZXN0aW5nIHNldHMKWF90cmFpbiwgWF90ZXN0LCB5X3RyYWluLCB5X3Rlc3QgPSB0cmFpbl90ZXN0X3NwbGl0KFgsIHksIHRlc3Rfc2l6ZT0wLjMsIHJhbmRvbV9zdGF0ZT00MikKCiMgSW5pdGlhbGl6ZSBhbmQgdHJhaW4gUmFuZG9tIEZvcmVzdCBjbGFzc2lmaWVyCnJmID0gUmFuZG9tRm9yZXN0Q2xhc3NpZmllcihuX2VzdGltYXRvcnM9MTAwLCByYW5kb21fc3RhdGU9NDIpCnJmLmZpdChYX3RyYWluLCB5X3RyYWluKQoKIyBNYWtlIHByZWRpY3Rpb25zCnlfcHJlZCA9IHJmLnByZWRpY3QoWF90ZXN0KQoKIyBFdmFsdWF0ZSB0aGUgbW9kZWwKYWNjdXJhY3kgPSBhY2N1cmFjeV9zY29yZSh5X3Rlc3QsIHlfcHJlZCkKcHJpbnQoZiJBY2N1cmFjeToge2FjY3VyYWN5Oi4yZn0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train Random Forest classifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Make predictions
y_pred = rf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Accuracy: {accuracy:.2f}&quot;)</pre></div><div class='content'></div><h2>Boosting</h2>
<div class='content'><p>Boosting involves training weak learners sequentially, with each new model focusing on the errors made by the previous models. The final prediction is a weighted sum of the predictions of all models.</p>
<h4>Example: AdaBoost</h4>
<p>AdaBoost (Adaptive Boosting) is a popular boosting method that adjusts the weights of incorrectly classified instances, so subsequent models focus more on these hard-to-classify cases.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBBZGFCb29zdENsYXNzaWZpZXIKZnJvbSBza2xlYXJuLnRyZWUgaW1wb3J0IERlY2lzaW9uVHJlZUNsYXNzaWZpZXIKZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXMKZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgdHJhaW5fdGVzdF9zcGxpdApmcm9tIHNrbGVhcm4ubWV0cmljcyBpbXBvcnQgYWNjdXJhY3lfc2NvcmUKCiMgTG9hZCBkYXRhc2V0CmlyaXMgPSBsb2FkX2lyaXMoKQpYLCB5ID0gaXJpcy5kYXRhLCBpcmlzLnRhcmdldAoKIyBTcGxpdCBkYXRhc2V0IGludG8gdHJhaW5pbmcgYW5kIHRlc3Rpbmcgc2V0cwpYX3RyYWluLCBYX3Rlc3QsIHlfdHJhaW4sIHlfdGVzdCA9IHRyYWluX3Rlc3Rfc3BsaXQoWCwgeSwgdGVzdF9zaXplPTAuMywgcmFuZG9tX3N0YXRlPTQyKQoKIyBJbml0aWFsaXplIGFuZCB0cmFpbiBBZGFCb29zdCBjbGFzc2lmaWVyCmJhc2VfZXN0aW1hdG9yID0gRGVjaXNpb25UcmVlQ2xhc3NpZmllcihtYXhfZGVwdGg9MSkKYWRhID0gQWRhQm9vc3RDbGFzc2lmaWVyKGJhc2VfZXN0aW1hdG9yPWJhc2VfZXN0aW1hdG9yLCBuX2VzdGltYXRvcnM9NTAsIHJhbmRvbV9zdGF0ZT00MikKYWRhLmZpdChYX3RyYWluLCB5X3RyYWluKQoKIyBNYWtlIHByZWRpY3Rpb25zCnlfcHJlZCA9IGFkYS5wcmVkaWN0KFhfdGVzdCkKCiMgRXZhbHVhdGUgdGhlIG1vZGVsCmFjY3VyYWN5ID0gYWNjdXJhY3lfc2NvcmUoeV90ZXN0LCB5X3ByZWQpCnByaW50KGYiQWNjdXJhY3k6IHthY2N1cmFjeTouMmZ9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train AdaBoost classifier
base_estimator = DecisionTreeClassifier(max_depth=1)
ada = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)
ada.fit(X_train, y_train)

# Make predictions
y_pred = ada.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Accuracy: {accuracy:.2f}&quot;)</pre></div><div class='content'></div><h2>Stacking</h2>
<div class='content'><p>Stacking involves training multiple models (level-0 models) and then using their predictions as input features for a higher-level model (level-1 model), which makes the final prediction.</p>
<h4>Example: Stacking Classifier</h4>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBTdGFja2luZ0NsYXNzaWZpZXIKZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgTG9naXN0aWNSZWdyZXNzaW9uCmZyb20gc2tsZWFybi50cmVlIGltcG9ydCBEZWNpc2lvblRyZWVDbGFzc2lmaWVyCmZyb20gc2tsZWFybi5zdm0gaW1wb3J0IFNWQwpmcm9tIHNrbGVhcm4uZGF0YXNldHMgaW1wb3J0IGxvYWRfaXJpcwpmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCB0cmFpbl90ZXN0X3NwbGl0CmZyb20gc2tsZWFybi5tZXRyaWNzIGltcG9ydCBhY2N1cmFjeV9zY29yZQoKIyBMb2FkIGRhdGFzZXQKaXJpcyA9IGxvYWRfaXJpcygpClgsIHkgPSBpcmlzLmRhdGEsIGlyaXMudGFyZ2V0CgojIFNwbGl0IGRhdGFzZXQgaW50byB0cmFpbmluZyBhbmQgdGVzdGluZyBzZXRzClhfdHJhaW4sIFhfdGVzdCwgeV90cmFpbiwgeV90ZXN0ID0gdHJhaW5fdGVzdF9zcGxpdChYLCB5LCB0ZXN0X3NpemU9MC4zLCByYW5kb21fc3RhdGU9NDIpCgojIERlZmluZSBiYXNlIGxlYXJuZXJzCmJhc2VfbGVhcm5lcnMgPSBbCiAgICAoJ2R0JywgRGVjaXNpb25UcmVlQ2xhc3NpZmllcihtYXhfZGVwdGg9MSkpLAogICAgKCdzdm0nLCBTVkMoa2VybmVsPSdsaW5lYXInLCBwcm9iYWJpbGl0eT1UcnVlKSkKXQoKIyBEZWZpbmUgbWV0YS1sZWFybmVyCm1ldGFfbGVhcm5lciA9IExvZ2lzdGljUmVncmVzc2lvbigpCgojIEluaXRpYWxpemUgYW5kIHRyYWluIFN0YWNraW5nIGNsYXNzaWZpZXIKc3RhY2tpbmdfY2xmID0gU3RhY2tpbmdDbGFzc2lmaWVyKGVzdGltYXRvcnM9YmFzZV9sZWFybmVycywgZmluYWxfZXN0aW1hdG9yPW1ldGFfbGVhcm5lcikKc3RhY2tpbmdfY2xmLmZpdChYX3RyYWluLCB5X3RyYWluKQoKIyBNYWtlIHByZWRpY3Rpb25zCnlfcHJlZCA9IHN0YWNraW5nX2NsZi5wcmVkaWN0KFhfdGVzdCkKCiMgRXZhbHVhdGUgdGhlIG1vZGVsCmFjY3VyYWN5ID0gYWNjdXJhY3lfc2NvcmUoeV90ZXN0LCB5X3ByZWQpCnByaW50KGYiQWNjdXJhY3k6IHthY2N1cmFjeTouMmZ9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Define base learners
base_learners = [
    ('dt', DecisionTreeClassifier(max_depth=1)),
    ('svm', SVC(kernel='linear', probability=True))
]

# Define meta-learner
meta_learner = LogisticRegression()

# Initialize and train Stacking classifier
stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner)
stacking_clf.fit(X_train, y_train)

# Make predictions
y_pred = stacking_clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Accuracy: {accuracy:.2f}&quot;)</pre></div><div class='content'></div><h1>Practical Exercises</h1>
<div class='content'></div><h2>Exercise 1: Implementing Bagging with Decision Trees</h2>
<div class='content'><p><strong>Task</strong>: Implement a Bagging classifier using decision trees on the Iris dataset and evaluate its performance.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBCYWdnaW5nQ2xhc3NpZmllcgpmcm9tIHNrbGVhcm4udHJlZSBpbXBvcnQgRGVjaXNpb25UcmVlQ2xhc3NpZmllcgpmcm9tIHNrbGVhcm4uZGF0YXNldHMgaW1wb3J0IGxvYWRfaXJpcwpmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCB0cmFpbl90ZXN0X3NwbGl0CmZyb20gc2tsZWFybi5tZXRyaWNzIGltcG9ydCBhY2N1cmFjeV9zY29yZQoKIyBMb2FkIGRhdGFzZXQKaXJpcyA9IGxvYWRfaXJpcygpClgsIHkgPSBpcmlzLmRhdGEsIGlyaXMudGFyZ2V0CgojIFNwbGl0IGRhdGFzZXQgaW50byB0cmFpbmluZyBhbmQgdGVzdGluZyBzZXRzClhfdHJhaW4sIFhfdGVzdCwgeV90cmFpbiwgeV90ZXN0ID0gdHJhaW5fdGVzdF9zcGxpdChYLCB5LCB0ZXN0X3NpemU9MC4zLCByYW5kb21fc3RhdGU9NDIpCgojIEluaXRpYWxpemUgYW5kIHRyYWluIEJhZ2dpbmcgY2xhc3NpZmllcgpiYWdnaW5nX2NsZiA9IEJhZ2dpbmdDbGFzc2lmaWVyKGJhc2VfZXN0aW1hdG9yPURlY2lzaW9uVHJlZUNsYXNzaWZpZXIoKSwgbl9lc3RpbWF0b3JzPTUwLCByYW5kb21fc3RhdGU9NDIpCmJhZ2dpbmdfY2xmLmZpdChYX3RyYWluLCB5X3RyYWluKQoKIyBNYWtlIHByZWRpY3Rpb25zCnlfcHJlZCA9IGJhZ2dpbmdfY2xmLnByZWRpY3QoWF90ZXN0KQoKIyBFdmFsdWF0ZSB0aGUgbW9kZWwKYWNjdXJhY3kgPSBhY2N1cmFjeV9zY29yZSh5X3Rlc3QsIHlfcHJlZCkKcHJpbnQoZiJBY2N1cmFjeToge2FjY3VyYWN5Oi4yZn0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train Bagging classifier
bagging_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)
bagging_clf.fit(X_train, y_train)

# Make predictions
y_pred = bagging_clf.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Accuracy: {accuracy:.2f}&quot;)</pre></div><div class='content'></div><h2>Exercise 2: Implementing Boosting with AdaBoost</h2>
<div class='content'><p><strong>Task</strong>: Implement an AdaBoost classifier using decision trees on the Iris dataset and evaluate its performance.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBBZGFCb29zdENsYXNzaWZpZXIKZnJvbSBza2xlYXJuLnRyZWUgaW1wb3J0IERlY2lzaW9uVHJlZUNsYXNzaWZpZXIKZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXMKZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgdHJhaW5fdGVzdF9zcGxpdApmcm9tIHNrbGVhcm4ubWV0cmljcyBpbXBvcnQgYWNjdXJhY3lfc2NvcmUKCiMgTG9hZCBkYXRhc2V0CmlyaXMgPSBsb2FkX2lyaXMoKQpYLCB5ID0gaXJpcy5kYXRhLCBpcmlzLnRhcmdldAoKIyBTcGxpdCBkYXRhc2V0IGludG8gdHJhaW5pbmcgYW5kIHRlc3Rpbmcgc2V0cwpYX3RyYWluLCBYX3Rlc3QsIHlfdHJhaW4sIHlfdGVzdCA9IHRyYWluX3Rlc3Rfc3BsaXQoWCwgeSwgdGVzdF9zaXplPTAuMywgcmFuZG9tX3N0YXRlPTQyKQoKIyBJbml0aWFsaXplIGFuZCB0cmFpbiBBZGFCb29zdCBjbGFzc2lmaWVyCmJhc2VfZXN0aW1hdG9yID0gRGVjaXNpb25UcmVlQ2xhc3NpZmllcihtYXhfZGVwdGg9MSkKYWRhID0gQWRhQm9vc3RDbGFzc2lmaWVyKGJhc2VfZXN0aW1hdG9yPWJhc2VfZXN0aW1hdG9yLCBuX2VzdGltYXRvcnM9NTAsIHJhbmRvbV9zdGF0ZT00MikKYWRhLmZpdChYX3RyYWluLCB5X3RyYWluKQoKIyBNYWtlIHByZWRpY3Rpb25zCnlfcHJlZCA9IGFkYS5wcmVkaWN0KFhfdGVzdCkKCiMgRXZhbHVhdGUgdGhlIG1vZGVsCmFjY3VyYWN5ID0gYWNjdXJhY3lfc2NvcmUoeV90ZXN0LCB5X3ByZWQpCnByaW50KGYiQWNjdXJhY3k6IHthY2N1cmFjeTouMmZ9Iik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize and train AdaBoost classifier
base_estimator = DecisionTreeClassifier(max_depth=1)
ada = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)
ada.fit(X_train, y_train)

# Make predictions
y_pred = ada.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Accuracy: {accuracy:.2f}&quot;)</pre></div><div class='content'></div><h1>Common Mistakes and Tips</h1>
<div class='content'><ol>
<li><strong>Overfitting</strong>: While ensembles can reduce overfitting, using too many complex models can still lead to overfitting. Ensure you use cross-validation to tune hyperparameters.</li>
<li><strong>Diversity</strong>: Ensure the base learners are diverse. Using the same model with the same parameters may not yield the best results.</li>
<li><strong>Computational Cost</strong>: Ensembles can be computationally expensive. Consider the trade-off between performance and computational resources.</li>
</ol>
</div><h1>Conclusion</h1>
<div class='content'><p>Ensemble learning is a robust technique that leverages the strengths of multiple models to achieve superior performance. By understanding and implementing methods like bagging, boosting, and stacking, you can significantly enhance the accuracy and robustness of your machine learning models. In the next section, we will delve into Gradient Boosting, a powerful boosting technique that has become a cornerstone in many machine learning competitions and applications.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='06-04-overfitting-underfitting' title="Overfitting and Underfitting">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='07-02-gradient-boosting' title="Gradient Boosting">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
