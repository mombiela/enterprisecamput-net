<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Principal Component Analysis (PCA)</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/05-03-pca" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/05-03-pca" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/05-03-pca" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
				<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/machine_learning/05-03-pca" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/machine_learning/05-03-pca" class="px-2">CA</a>
<br>
			<cite>Building today's and tomorrow's society</cite>
		</div>
	</div>
</div>
<div id="subheader" class="container-xxl">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
									<a href="./">Course Content</a>
					<span class="sep">|</span>
								<a href="/all/competencias">Technical Skills</a>
				<a href="/all/conocimientos">Knowledge</a>
				<a href="/all/soft_skills">Social Skills</a>
			</div>
		</div>
	</div>
</div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-02-hierarchical-clustering' title="Hierarchical Clustering">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Principal Component Analysis (PCA)</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='05-04-dbscan' title="DBSCAN Clustering Analysis">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Principal Component Analysis (PCA) is a powerful technique used in machine learning and statistics for dimensionality reduction. It transforms the data into a new coordinate system such that the greatest variances by any projection of the data come to lie on the first coordinate (called the first principal component), the second greatest variances on the second coordinate, and so on.</p>
</div><h1><p>Key Concepts</p>
</h1>
<div class='content'></div><h2><ol>
<li>Dimensionality Reduction</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Definition</strong>: The process of reducing the number of random variables under consideration.</li>
<li><strong>Purpose</strong>: Simplifies models, reduces computational cost, and helps in visualizing high-dimensional data.</li>
</ul>
</div><h2><ol start="2">
<li>Principal Components</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Principal Components</strong>: New variables that are linear combinations of the original variables.</li>
<li><strong>Variance Maximization</strong>: Principal components are ordered by the amount of variance they capture from the data.</li>
<li><strong>Orthogonality</strong>: Principal components are orthogonal (uncorrelated) to each other.</li>
</ul>
</div><h2><ol start="3">
<li>Eigenvalues and Eigenvectors</li>
</ol>
</h2>
<div class='content'><ul>
<li><strong>Eigenvalues</strong>: Indicate the amount of variance captured by each principal component.</li>
<li><strong>Eigenvectors</strong>: Directions of the principal components in the original feature space.</li>
</ul>
</div><h1><p>Steps to Perform PCA</p>
</h1>
<div class='content'><ol>
<li><strong>Standardize the Data</strong>: Ensure each feature has a mean of zero and a standard deviation of one.</li>
<li><strong>Compute the Covariance Matrix</strong>: Measure how much the dimensions vary from the mean with respect to each other.</li>
<li><strong>Calculate Eigenvalues and Eigenvectors</strong>: Determine the principal components.</li>
<li><strong>Sort Eigenvalues and Eigenvectors</strong>: Rank them in descending order of eigenvalues.</li>
<li><strong>Select Principal Components</strong>: Choose the top k eigenvectors based on the highest eigenvalues.</li>
<li><strong>Transform the Data</strong>: Project the original data onto the new k-dimensional subspace.</li>
</ol>
</div><h1><p>Practical Example</p>
</h1>
<div class='content'></div><h2><p>Step-by-Step PCA Implementation in Python</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBwYW5kYXMgYXMgcGQKZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFN0YW5kYXJkU2NhbGVyCmZyb20gc2tsZWFybi5kZWNvbXBvc2l0aW9uIGltcG9ydCBQQ0EKaW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKIyBTYW1wbGUgRGF0YQpkYXRhID0gewogICAgJ0ZlYXR1cmUxJzogWzIuNSwgMC41LCAyLjIsIDEuOSwgMy4xLCAyLjMsIDIuMCwgMS4wLCAxLjUsIDEuMV0sCiAgICAnRmVhdHVyZTInOiBbMi40LCAwLjcsIDIuOSwgMi4yLCAzLjAsIDIuNywgMS42LCAxLjEsIDEuNiwgMC45XQp9CmRmID0gcGQuRGF0YUZyYW1lKGRhdGEpCgojIFN0ZXAgMTogU3RhbmRhcmRpemUgdGhlIERhdGEKc2NhbGVyID0gU3RhbmRhcmRTY2FsZXIoKQpzY2FsZWRfZGF0YSA9IHNjYWxlci5maXRfdHJhbnNmb3JtKGRmKQoKIyBTdGVwIDI6IENvbXB1dGUgdGhlIENvdmFyaWFuY2UgTWF0cml4CmNvdl9tYXRyaXggPSBucC5jb3Yoc2NhbGVkX2RhdGEuVCkKCiMgU3RlcCAzOiBDYWxjdWxhdGUgRWlnZW52YWx1ZXMgYW5kIEVpZ2VudmVjdG9ycwplaWdlbnZhbHVlcywgZWlnZW52ZWN0b3JzID0gbnAubGluYWxnLmVpZyhjb3ZfbWF0cml4KQoKIyBTdGVwIDQ6IFNvcnQgRWlnZW52YWx1ZXMgYW5kIEVpZ2VudmVjdG9ycwpzb3J0ZWRfaW5kZXggPSBucC5hcmdzb3J0KGVpZ2VudmFsdWVzKVs6Oi0xXQpzb3J0ZWRfZWlnZW52YWx1ZXMgPSBlaWdlbnZhbHVlc1tzb3J0ZWRfaW5kZXhdCnNvcnRlZF9laWdlbnZlY3RvcnMgPSBlaWdlbnZlY3RvcnNbOiwgc29ydGVkX2luZGV4XQoKIyBTdGVwIDU6IFNlbGVjdCBQcmluY2lwYWwgQ29tcG9uZW50cwpuX2NvbXBvbmVudHMgPSAyCnNlbGVjdGVkX2VpZ2VudmVjdG9ycyA9IHNvcnRlZF9laWdlbnZlY3RvcnNbOiwgOm5fY29tcG9uZW50c10KCiMgU3RlcCA2OiBUcmFuc2Zvcm0gdGhlIERhdGEKdHJhbnNmb3JtZWRfZGF0YSA9IG5wLmRvdChzY2FsZWRfZGF0YSwgc2VsZWN0ZWRfZWlnZW52ZWN0b3JzKQoKIyBQbG90dGluZyB0aGUgVHJhbnNmb3JtZWQgRGF0YQpwbHQuc2NhdHRlcih0cmFuc2Zvcm1lZF9kYXRhWzosIDBdLCB0cmFuc2Zvcm1lZF9kYXRhWzosIDFdKQpwbHQueGxhYmVsKCdQcmluY2lwYWwgQ29tcG9uZW50IDEnKQpwbHQueWxhYmVsKCdQcmluY2lwYWwgQ29tcG9uZW50IDInKQpwbHQudGl0bGUoJ1BDQSBSZXN1bHQnKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Sample Data
data = {
    'Feature1': [2.5, 0.5, 2.2, 1.9, 3.1, 2.3, 2.0, 1.0, 1.5, 1.1],
    'Feature2': [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]
}
df = pd.DataFrame(data)

# Step 1: Standardize the Data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# Step 2: Compute the Covariance Matrix
cov_matrix = np.cov(scaled_data.T)

# Step 3: Calculate Eigenvalues and Eigenvectors
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Step 4: Sort Eigenvalues and Eigenvectors
sorted_index = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[sorted_index]
sorted_eigenvectors = eigenvectors[:, sorted_index]

# Step 5: Select Principal Components
n_components = 2
selected_eigenvectors = sorted_eigenvectors[:, :n_components]

# Step 6: Transform the Data
transformed_data = np.dot(scaled_data, selected_eigenvectors)

# Plotting the Transformed Data
plt.scatter(transformed_data[:, 0], transformed_data[:, 1])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Result')
plt.show()</pre></div><div class='content'></div><h2><p>Explanation of the Code</p>
</h2>
<div class='content'><ul>
<li><strong>Data Preparation</strong>: A sample dataset is created with two features.</li>
<li><strong>Standardization</strong>: The data is standardized to have a mean of zero and a standard deviation of one.</li>
<li><strong>Covariance Matrix</strong>: The covariance matrix of the standardized data is computed.</li>
<li><strong>Eigenvalues and Eigenvectors</strong>: Eigenvalues and eigenvectors of the covariance matrix are calculated.</li>
<li><strong>Sorting</strong>: Eigenvalues and their corresponding eigenvectors are sorted in descending order.</li>
<li><strong>Selection</strong>: The top <code>n_components</code> eigenvectors are selected.</li>
<li><strong>Transformation</strong>: The original data is projected onto the new subspace defined by the selected eigenvectors.</li>
</ul>
</div><h1><p>Practical Exercises</p>
</h1>
<div class='content'></div><h2><p>Exercise 1: PCA on Iris Dataset</p>
</h2>
<div class='content'><p><strong>Task</strong>: Perform PCA on the Iris dataset and visualize the first two principal components.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXMKaW1wb3J0IHNlYWJvcm4gYXMgc25zCgojIExvYWQgSXJpcyBEYXRhc2V0CmlyaXMgPSBsb2FkX2lyaXMoKQppcmlzX2RhdGEgPSBpcmlzLmRhdGEKaXJpc190YXJnZXQgPSBpcmlzLnRhcmdldAoKIyBTdGFuZGFyZGl6ZSB0aGUgRGF0YQpzY2FsZWRfaXJpc19kYXRhID0gc2NhbGVyLmZpdF90cmFuc2Zvcm0oaXJpc19kYXRhKQoKIyBQZXJmb3JtIFBDQQpwY2EgPSBQQ0Eobl9jb21wb25lbnRzPTIpCmlyaXNfcGNhID0gcGNhLmZpdF90cmFuc2Zvcm0oc2NhbGVkX2lyaXNfZGF0YSkKCiMgUGxvdHRpbmcgdGhlIFBDQSByZXN1bHQKcGx0LmZpZ3VyZShmaWdzaXplPSg4LCA2KSkKc25zLnNjYXR0ZXJwbG90KHg9aXJpc19wY2FbOiwgMF0sIHk9aXJpc19wY2FbOiwgMV0sIGh1ZT1pcmlzX3RhcmdldCwgcGFsZXR0ZT0ndmlyaWRpcycpCnBsdC54bGFiZWwoJ1ByaW5jaXBhbCBDb21wb25lbnQgMScpCnBsdC55bGFiZWwoJ1ByaW5jaXBhbCBDb21wb25lbnQgMicpCnBsdC50aXRsZSgnUENBIG9uIElyaXMgRGF0YXNldCcpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_iris
import seaborn as sns

# Load Iris Dataset
iris = load_iris()
iris_data = iris.data
iris_target = iris.target

# Standardize the Data
scaled_iris_data = scaler.fit_transform(iris_data)

# Perform PCA
pca = PCA(n_components=2)
iris_pca = pca.fit_transform(scaled_iris_data)

# Plotting the PCA result
plt.figure(figsize=(8, 6))
sns.scatterplot(x=iris_pca[:, 0], y=iris_pca[:, 1], hue=iris_target, palette='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA on Iris Dataset')
plt.show()</pre></div><div class='content'></div><h2><p>Exercise 2: Explained Variance Ratio</p>
</h2>
<div class='content'><p><strong>Task</strong>: Calculate and plot the explained variance ratio of each principal component for the Iris dataset.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBFeHBsYWluZWQgVmFyaWFuY2UgUmF0aW8KZXhwbGFpbmVkX3ZhcmlhbmNlX3JhdGlvID0gcGNhLmV4cGxhaW5lZF92YXJpYW5jZV9yYXRpb18KCiMgUGxvdHRpbmcgdGhlIEV4cGxhaW5lZCBWYXJpYW5jZSBSYXRpbwpwbHQuZmlndXJlKGZpZ3NpemU9KDgsIDYpKQpwbHQuYmFyKHJhbmdlKDEsIGxlbihleHBsYWluZWRfdmFyaWFuY2VfcmF0aW8pICsgMSksIGV4cGxhaW5lZF92YXJpYW5jZV9yYXRpbywgYWxwaGE9MC41LCBhbGlnbj0nY2VudGVyJykKcGx0LnhsYWJlbCgnUHJpbmNpcGFsIENvbXBvbmVudHMnKQpwbHQueWxhYmVsKCdFeHBsYWluZWQgVmFyaWFuY2UgUmF0aW8nKQpwbHQudGl0bGUoJ0V4cGxhaW5lZCBWYXJpYW5jZSBSYXRpbyBieSBQcmluY2lwYWwgQ29tcG9uZW50cycpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Explained Variance Ratio
explained_variance_ratio = pca.explained_variance_ratio_

# Plotting the Explained Variance Ratio
plt.figure(figsize=(8, 6))
plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.5, align='center')
plt.xlabel('Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.title('Explained Variance Ratio by Principal Components')
plt.show()</pre></div><div class='content'></div><h1><p>Common Mistakes and Tips</p>
</h1>
<div class='content'><ul>
<li><strong>Not Standardizing Data</strong>: Always standardize the data before applying PCA.</li>
<li><strong>Choosing Too Many Components</strong>: Select the number of components that capture the most variance without overfitting.</li>
<li><strong>Interpreting Principal Components</strong>: Understand that principal components are linear combinations of original features and may not have a direct interpretation.</li>
</ul>
</div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>Principal Component Analysis (PCA) is an essential technique for reducing the dimensionality of data while retaining most of the variance. It simplifies the complexity of high-dimensional data, making it easier to visualize and analyze. By following the steps outlined and practicing with exercises, you can effectively apply PCA to various datasets.</p>
<p>Next, we will explore <strong>DBSCAN Clustering Analysis</strong>, another powerful unsupervised learning technique.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='05-02-hierarchical-clustering' title="Hierarchical Clustering">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='05-04-dbscan' title="DBSCAN Clustering Analysis">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
