<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow, noarchive">
    <title>Overfitting and Underfitting</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/06-04-overfitting-underfitting" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/06-04-overfitting-underfitting" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/06-04-overfitting-underfitting" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=3" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-2 p-md-0 text-end">
			<span>	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/machine_learning/06-04-overfitting-underfitting" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/machine_learning/06-04-overfitting-underfitting" class="px-2">CA</a>
</span>
			<span class="d-none d-md-inline"><br><cite>Building today's and tomorrow's society</cite></span>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				 					<a href="/categ/languages">Languages</a>
				 					<a href="/categ/frameworks">Frameworks</a>
				 					<a href="/categ/tech-tools">Tools</a>
				 					<a href="/categ/foundations">Foundations</a>
				 					<a href="/categ/soft-skills">Skills</a>
							</div>
		</div>
	</div>
</div>
		
<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-03-roc-curve-auc' title="ROC Curve and AUC">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Overfitting and Underfitting</h2></a>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='07-01-ensemble-learning' title="Ensemble Learning">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>In this section, we will explore two common issues in machine learning: overfitting and underfitting. Understanding these concepts is crucial for building models that generalize well to new, unseen data.</p>
</div><h1><p>What is Overfitting?</p>
</h1>
<div class='content'><p>Overfitting occurs when a machine learning model learns the details and noise in the training data to such an extent that it negatively impacts the model's performance on new data. This means the model is too complex and captures the random fluctuations in the training data rather than the intended outputs.</p>
</div><h2><p>Characteristics of Overfitting:</p>
</h2>
<div class='content'><ul>
<li>High accuracy on training data.</li>
<li>Poor performance on validation/test data.</li>
<li>Model complexity is too high.</li>
</ul>
</div><h2><p>Example:</p>
</h2>
<div class='content'><p>Consider a polynomial regression model trying to fit a dataset:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFBvbHlub21pYWxGZWF0dXJlcwpmcm9tIHNrbGVhcm4ubGluZWFyX21vZGVsIGltcG9ydCBMaW5lYXJSZWdyZXNzaW9uCgojIEdlbmVyYXRlIHNvbWUgZGF0YQpucC5yYW5kb20uc2VlZCgwKQpYID0gbnAuc29ydChucC5yYW5kb20ucmFuZCgyMCwgMSkgKiAxMCwgYXhpcz0wKQp5ID0gbnAuc2luKFgpLnJhdmVsKCkgKyBucC5yYW5kb20ucmFuZG4oMjApICogMC41CgojIEZpdCBwb2x5bm9taWFsIHJlZ3Jlc3Npb24gbW9kZWwKcG9seSA9IFBvbHlub21pYWxGZWF0dXJlcyhkZWdyZWU9MTUpClhfcG9seSA9IHBvbHkuZml0X3RyYW5zZm9ybShYKQptb2RlbCA9IExpbmVhclJlZ3Jlc3Npb24oKQptb2RlbC5maXQoWF9wb2x5LCB5KQoKIyBQcmVkaWN0IGFuZCBwbG90ClhfdGVzdCA9IG5wLmxpbnNwYWNlKDAsIDEwLCAxMDApLnJlc2hhcGUoLTEsIDEpClhfdGVzdF9wb2x5ID0gcG9seS50cmFuc2Zvcm0oWF90ZXN0KQp5X3ByZWQgPSBtb2RlbC5wcmVkaWN0KFhfdGVzdF9wb2x5KQoKcGx0LnNjYXR0ZXIoWCwgeSwgY29sb3I9J2JsYWNrJykKcGx0LnBsb3QoWF90ZXN0LCB5X3ByZWQsIGNvbG9yPSdibHVlJywgbGluZXdpZHRoPTIpCnBsdC50aXRsZSgnT3ZlcmZpdHRpbmcgRXhhbXBsZScpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Generate some data
np.random.seed(0)
X = np.sort(np.random.rand(20, 1) * 10, axis=0)
y = np.sin(X).ravel() + np.random.randn(20) * 0.5

# Fit polynomial regression model
poly = PolynomialFeatures(degree=15)
X_poly = poly.fit_transform(X)
model = LinearRegression()
model.fit(X_poly, y)

# Predict and plot
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
X_test_poly = poly.transform(X_test)
y_pred = model.predict(X_test_poly)

plt.scatter(X, y, color='black')
plt.plot(X_test, y_pred, color='blue', linewidth=2)
plt.title('Overfitting Example')
plt.show()</pre></div><div class='content'><p>In this example, the model fits the training data very well but is likely to perform poorly on new data due to its complexity.</p>
</div><h1><p>What is Underfitting?</p>
</h1>
<div class='content'><p>Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. This means the model performs poorly on both the training data and new data.</p>
</div><h2><p>Characteristics of Underfitting:</p>
</h2>
<div class='content'><ul>
<li>Low accuracy on training data.</li>
<li>Poor performance on validation/test data.</li>
<li>Model complexity is too low.</li>
</ul>
</div><h2><p>Example:</p>
</h2>
<div class='content'><p>Consider a linear regression model trying to fit a dataset:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBHZW5lcmF0ZSBzb21lIGRhdGEKbnAucmFuZG9tLnNlZWQoMCkKWCA9IG5wLnNvcnQobnAucmFuZG9tLnJhbmQoMjAsIDEpICogMTAsIGF4aXM9MCkKeSA9IG5wLnNpbihYKS5yYXZlbCgpICsgbnAucmFuZG9tLnJhbmRuKDIwKSAqIDAuNQoKIyBGaXQgbGluZWFyIHJlZ3Jlc3Npb24gbW9kZWwKbW9kZWwgPSBMaW5lYXJSZWdyZXNzaW9uKCkKbW9kZWwuZml0KFgsIHkpCgojIFByZWRpY3QgYW5kIHBsb3QKeV9wcmVkID0gbW9kZWwucHJlZGljdChYX3Rlc3QpCgpwbHQuc2NhdHRlcihYLCB5LCBjb2xvcj0nYmxhY2snKQpwbHQucGxvdChYX3Rlc3QsIHlfcHJlZCwgY29sb3I9J3JlZCcsIGxpbmV3aWR0aD0yKQpwbHQudGl0bGUoJ1VuZGVyZml0dGluZyBFeGFtcGxlJykKcGx0LnNob3coKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Generate some data
np.random.seed(0)
X = np.sort(np.random.rand(20, 1) * 10, axis=0)
y = np.sin(X).ravel() + np.random.randn(20) * 0.5

# Fit linear regression model
model = LinearRegression()
model.fit(X, y)

# Predict and plot
y_pred = model.predict(X_test)

plt.scatter(X, y, color='black')
plt.plot(X_test, y_pred, color='red', linewidth=2)
plt.title('Underfitting Example')
plt.show()</pre></div><div class='content'><p>In this example, the linear model is too simple to capture the non-linear relationship in the data.</p>
</div><h1><p>Balancing Model Complexity</p>
</h1>
<div class='content'><p>To achieve a balance between overfitting and underfitting, we need to find the right model complexity. This can be done through techniques such as:</p>
<ul>
<li><strong>Cross-Validation:</strong> Splitting the data into training and validation sets multiple times to ensure the model generalizes well.</li>
<li><strong>Regularization:</strong> Adding a penalty to the model for complexity (e.g., L1 or L2 regularization).</li>
<li><strong>Pruning:</strong> Reducing the complexity of decision trees by removing branches that have little importance.</li>
<li><strong>Early Stopping:</strong> Halting the training process when the model's performance on a validation set starts to degrade.</li>
</ul>
</div><h2><p>Example of Regularization:</p>
</h2>
<div class='content'><p>Using Ridge Regression (L2 regularization):</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgUmlkZ2UKCiMgRml0IFJpZGdlIHJlZ3Jlc3Npb24gbW9kZWwKcmlkZ2VfbW9kZWwgPSBSaWRnZShhbHBoYT0xLjApCnJpZGdlX21vZGVsLmZpdChYX3BvbHksIHkpCgojIFByZWRpY3QgYW5kIHBsb3QKeV9wcmVkX3JpZGdlID0gcmlkZ2VfbW9kZWwucHJlZGljdChYX3Rlc3RfcG9seSkKCnBsdC5zY2F0dGVyKFgsIHksIGNvbG9yPSdibGFjaycpCnBsdC5wbG90KFhfdGVzdCwgeV9wcmVkX3JpZGdlLCBjb2xvcj0nZ3JlZW4nLCBsaW5ld2lkdGg9MikKcGx0LnRpdGxlKCdSZWd1bGFyaXphdGlvbiBFeGFtcGxlJykKcGx0LnNob3coKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.linear_model import Ridge

# Fit Ridge regression model
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_poly, y)

# Predict and plot
y_pred_ridge = ridge_model.predict(X_test_poly)

plt.scatter(X, y, color='black')
plt.plot(X_test, y_pred_ridge, color='green', linewidth=2)
plt.title('Regularization Example')
plt.show()</pre></div><div class='content'></div><h1><p>Practical Exercise</p>
</h1>
<div class='content'></div><h2><p>Exercise:</p>
</h2>
<div class='content'><ol>
<li>Generate a synthetic dataset with a non-linear relationship.</li>
<li>Split the dataset into training and test sets.</li>
<li>Fit a linear regression model and a polynomial regression model to the training data.</li>
<li>Evaluate the performance of both models on the test data.</li>
<li>Apply Ridge regression to the polynomial model and observe the changes in performance.</li>
</ol>
</div><h2><p>Solution:</p>
</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Generate synthetic data
np.random.seed(0)
X = np.sort(np.random.rand(100, 1) * 10, axis=0)
y = np.sin(X).ravel() + np.random.randn(100) * 0.5

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Fit linear regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)

# Fit polynomial regression model
poly = PolynomialFeatures(degree=15)
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)
poly_model = LinearRegression()
poly_model.fit(X_train_poly, y_train)
y_pred_poly = poly_model.predict(X_test_poly)

# Fit Ridge regression model
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_poly, y_train)
y_pred_ridge = ridge_model.predict(X_test_poly)

# Evaluate models
mse_linear = mean_squared_error(y_test, y_pred_linear)
mse_poly = mean_squared_error(y_test, y_pred_poly)
mse_ridge = mean_squared_error(y_test, y_pred_ridge)

print(f&quot;Linear Regression MSE: {mse_linear}&quot;)
print(f&quot;Polynomial Regression MSE: {mse_poly}&quot;)
print(f&quot;Ridge Regression MSE: {mse_ridge}&quot;)

# Plot results
plt.scatter(X_test, y_test, color='black', label='Test Data')
plt.plot(X_test, y_pred_linear, color='red', linewidth=2, label='Linear Model')
plt.plot(X_test, y_pred_poly, color='blue', linewidth=2, label='Polynomial Model')
plt.plot(X_test, y_pred_ridge, color='green', linewidth=2, label='Ridge Model')
plt.legend()
plt.title('Model Comparison')
plt.show()</pre></div><div class='content'></div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>In this section, we have learned about overfitting and underfitting, two critical issues in machine learning. We explored their characteristics, examples, and techniques to balance model complexity. By understanding these concepts, you can build models that generalize well to new data, ensuring better performance and reliability.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-03-roc-curve-auc' title="ROC Curve and AUC">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='07-01-ensemble-learning' title="Ensemble Learning">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<!-- 
<h1>Advertising</h1>
<p>This space is reserved for advertising.</p>
<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
<p>Thank you for collaborating!</p>
-->

<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
     crossorigin="anonymous"></script>
<!-- enterprise_campus -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-0611338592562725"
     data-ad-slot="6914733106"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
		</div>
	</div>
</div>

<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
