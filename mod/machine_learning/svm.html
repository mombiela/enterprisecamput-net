<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Support Vector Machines (SVM)</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/svm" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/svm" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/svm" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/machine_learning/svm" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/svm" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introduction to Support Vector Machines</h1>
<div class='content'><p>Support Vector Machines (SVM) are a set of supervised learning algorithms used for classification and regression. However, they are best known for their application in classification problems.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ul>
<li><strong>Hyperplane</strong>: In the context of SVM, a hyperplane is a line (in 2D), a plane (in 3D), or a higher-dimensional space that separates the data into different classes.</li>
<li><strong>Margin</strong>: The distance between the hyperplane and the closest points of each class. SVM aims to maximize this margin.</li>
<li><strong>Support Vectors</strong>: The data points that are closest to the hyperplane and are critical in defining its position and orientation.</li>
<li><strong>Kernel</strong>: A function that transforms the data into a higher-dimensional space to make them separable by a hyperplane.</li>
</ul>
</div><h1>How SVM Works</h1>
<div class='content'></div><h2>Linear Classification</h2>
<div class='content'><p>In linear classification, SVM tries to find the hyperplane that best separates the two classes of data.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuIGltcG9ydCBkYXRhc2V0cwpmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCB0cmFpbl90ZXN0X3NwbGl0CmZyb20gc2tsZWFybi5zdm0gaW1wb3J0IFNWQwppbXBvcnQgbWF0cGxvdGxpYi5weXBsb3QgYXMgcGx0CgojIExvYWQgdGhlIGlyaXMgZGF0YXNldAppcmlzID0gZGF0YXNldHMubG9hZF9pcmlzKCkKWCA9IGlyaXMuZGF0YVs6LCA6Ml0gICMgV2UgdXNlIG9ubHkgdGhlIGZpcnN0IHR3byBmZWF0dXJlcwp5ID0gaXJpcy50YXJnZXQKCiMgU3BsaXQgdGhlIGRhdGFzZXQgaW50byB0cmFpbmluZyBhbmQgdGVzdCBzZXRzClhfdHJhaW4sIFhfdGVzdCwgeV90cmFpbiwgeV90ZXN0ID0gdHJhaW5fdGVzdF9zcGxpdChYLCB5LCB0ZXN0X3NpemU9MC4zLCByYW5kb21fc3RhdGU9NDIpCgojIENyZWF0ZSB0aGUgU1ZNIG1vZGVsIHdpdGggYSBsaW5lYXIga2VybmVsCm1vZGVsID0gU1ZDKGtlcm5lbD0nbGluZWFyJykKbW9kZWwuZml0KFhfdHJhaW4sIHlfdHJhaW4pCgojIFZpc3VhbGl6ZSB0aGUgaHlwZXJwbGFuZQpwbHQuc2NhdHRlcihYWzosIDBdLCBYWzosIDFdLCBjPXksIGNtYXA9J3ZpcmlkaXMnKQpheCA9IHBsdC5nY2EoKQp4bGltID0gYXguZ2V0X3hsaW0oKQp5bGltID0gYXguZ2V0X3lsaW0oKQoKIyBDcmVhdGUgYSBtZXNoIHRvIHBsb3QgdGhlIGh5cGVycGxhbmUKeHgsIHl5ID0gbnAubWVzaGdyaWQobnAubGluc3BhY2UoeGxpbVswXSwgeGxpbVsxXSwgMzApLCBucC5saW5zcGFjZSh5bGltWzBdLCB5bGltWzFdLCAzMCkpCnh5ID0gbnAudnN0YWNrKFt4eC5yYXZlbCgpLCB5eS5yYXZlbCgpXSkuVApaID0gbW9kZWwuZGVjaXNpb25fZnVuY3Rpb24oeHkpLnJlc2hhcGUoeHguc2hhcGUpCgojIFBsb3QgdGhlIGh5cGVycGxhbmUgYW5kIG1hcmdpbnMKYXguY29udG91cih4eCwgeXksIFosIGNvbG9ycz0naycsIGxldmVscz1bLTEsIDAsIDFdLCBhbHBoYT0wLjUsIGxpbmVzdHlsZXM9WyctLScsICctJywgJy0tJ10pCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
import matplotlib.pyplot as plt

# Load the iris dataset
iris = datasets.load_iris()
X = iris.data[:, :2]  # We use only the first two features
y = iris.target

# Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create the SVM model with a linear kernel
model = SVC(kernel='linear')
model.fit(X_train, y_train)

# Visualize the hyperplane
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create a mesh to plot the hyperplane
xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 30), np.linspace(ylim[0], ylim[1], 30))
xy = np.vstack([xx.ravel(), yy.ravel()]).T
Z = model.decision_function(xy).reshape(xx.shape)

# Plot the hyperplane and margins
ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
plt.show()</pre></div><div class='content'></div><h2>Non-Linear Classification</h2>
<div class='content'><p>For problems where the data are not linearly separable, SVM uses kernel functions to project the data into a higher-dimensional space.</p>
<ul>
<li><strong>Linear Kernel</strong>: <code>kernel='linear'</code></li>
<li><strong>Polynomial Kernel</strong>: <code>kernel='poly'</code></li>
<li><strong>Radial Basis Function (RBF) Kernel</strong>: <code>kernel='rbf'</code></li>
</ul>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("IyBDcmVhdGUgdGhlIFNWTSBtb2RlbCB3aXRoIGEgcmFkaWFsIGtlcm5lbAptb2RlbCA9IFNWQyhrZXJuZWw9J3JiZicpCm1vZGVsLmZpdChYX3RyYWluLCB5X3RyYWluKQoKIyBWaXN1YWxpemUgdGhlIGh5cGVycGxhbmUgaW4gdGhlIHRyYW5zZm9ybWVkIHNwYWNlCnBsdC5zY2F0dGVyKFhbOiwgMF0sIFhbOiwgMV0sIGM9eSwgY21hcD0ndmlyaWRpcycpCmF4ID0gcGx0LmdjYSgpCnhsaW0gPSBheC5nZXRfeGxpbSgpCnlsaW0gPSBheC5nZXRfeWxpbSgpCgojIENyZWF0ZSBhIG1lc2ggdG8gcGxvdCB0aGUgaHlwZXJwbGFuZQp4eCwgeXkgPSBucC5tZXNoZ3JpZChucC5saW5zcGFjZSh4bGltWzBdLCB4bGltWzFdLCAzMCksIG5wLmxpbnNwYWNlKHlsaW1bMF0sIHlsaW1bMV0sIDMwKSkKeHkgPSBucC52c3RhY2soW3h4LnJhdmVsKCksIHl5LnJhdmVsKCldKS5UClogPSBtb2RlbC5kZWNpc2lvbl9mdW5jdGlvbih4eSkucmVzaGFwZSh4eC5zaGFwZSkKCiMgUGxvdCB0aGUgaHlwZXJwbGFuZSBhbmQgbWFyZ2lucwpheC5jb250b3VyKHh4LCB5eSwgWiwgY29sb3JzPSdrJywgbGV2ZWxzPVstMSwgMCwgMV0sIGFscGhhPTAuNSwgbGluZXN0eWxlcz1bJy0tJywgJy0nLCAnLS0nXSkKcGx0LnNob3coKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'># Create the SVM model with a radial kernel
model = SVC(kernel='rbf')
model.fit(X_train, y_train)

# Visualize the hyperplane in the transformed space
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis')
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Create a mesh to plot the hyperplane
xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 30), np.linspace(ylim[0], ylim[1], 30))
xy = np.vstack([xx.ravel(), yy.ravel()]).T
Z = model.decision_function(xy).reshape(xx.shape)

# Plot the hyperplane and margins
ax.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
plt.show()</pre></div><div class='content'></div><h1>Kernel Comparison</h1>
<div class='content'><p>| Kernel       | Description                                                                 | Application                                                                 |
|--------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------|
| Linear       | Separates the data with a linear hyperplane.                                | When the data are linearly or almost linearly separable.                    |
| Polynomial   | Separates the data with a polynomial hyperplane.                            | When the data have a polynomial relationship.                               |
| Radial (RBF) | Separates the data in a higher-dimensional space using a radial function.   | When the data are not linearly separable.                                   |</p>
</div><h1>Practical Exercises</h1>
<div class='content'><ol>
<li><strong>Exercise 1</strong>: Implement an SVM with a linear kernel for a two-class dataset and visualize the hyperplane.</li>
<li><strong>Exercise 2</strong>: Try different kernels (linear, polynomial, radial) on a non-linearly separable dataset and compare the results.</li>
<li><strong>Exercise 3</strong>: Adjust the regularization and gamma parameters in an SVM with a radial kernel and observe how they affect the classification.</li>
</ol>
</div><h1>Conclusion</h1>
<div class='content'><p>Support Vector Machines are powerful tools for classification and regression in Machine Learning. Their ability to find the optimal hyperplane that separates the classes and their flexibility through the use of different kernels make them very versatile. By understanding the key concepts and practicing with examples, professionals can effectively apply SVM to a variety of classification problems.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
