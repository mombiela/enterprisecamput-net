<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Trees</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/04-03-arboles-decision" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/04-03-arboles-decision" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/04-03-decision-trees" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/machine_learning/04-03-arboles-decision" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/04-03-arboles-decision" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-logistic-regression' title="Logistic Regression">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Decision Trees</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-svm' title="Support Vector Machines (SVM)">Next &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction to Decision Trees</h1>
<div class='content'><p>Decision Trees are a popular supervised learning algorithm used for both classification and regression tasks. They are intuitive, easy to interpret, and can handle both numerical and categorical data.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ol>
<li><strong>Node</strong>: A point in the tree where a decision is made.</li>
<li><strong>Root Node</strong>: The topmost node of the tree, representing the entire dataset.</li>
<li><strong>Leaf Node</strong>: The terminal nodes that represent the outcome or decision.</li>
<li><strong>Splitting</strong>: The process of dividing a node into two or more sub-nodes.</li>
<li><strong>Branch/Sub-Tree</strong>: A subsection of the entire tree.</li>
<li><strong>Pruning</strong>: The process of removing sub-nodes to reduce the complexity of the model and prevent overfitting.</li>
</ol>
</div><h2>How Decision Trees Work</h2>
<div class='content'><ol>
<li><strong>Start at the Root Node</strong>: Begin with the entire dataset at the root node.</li>
<li><strong>Select the Best Feature</strong>: Choose the feature that best splits the data based on a criterion (e.g., Gini impurity, Information Gain).</li>
<li><strong>Split the Data</strong>: Divide the dataset into subsets based on the selected feature.</li>
<li><strong>Repeat</strong>: Recursively apply the process to each subset until a stopping condition is met (e.g., maximum depth, minimum samples per leaf).</li>
</ol>
</div><h2>Splitting Criteria</h2>
<div class='content'><ul>
<li><strong>Gini Impurity</strong>: Measures the impurity of a node. Lower values indicate a purer node.</li>
<li><strong>Information Gain</strong>: Measures the reduction in entropy after a dataset is split on an attribute.</li>
<li><strong>Chi-Square</strong>: Measures the statistical significance of the differences between observed and expected frequencies.</li>
</ul>
</div><h2>Example of a Decision Tree</h2>
<div class='content'><p>Let's consider a simple example where we want to classify whether a person will buy a computer based on their age and income.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLnRyZWUgaW1wb3J0IERlY2lzaW9uVHJlZUNsYXNzaWZpZXIKZnJvbSBza2xlYXJuIGltcG9ydCBkYXRhc2V0cwppbXBvcnQgcGFuZGFzIGFzIHBkCgojIFNhbXBsZSBEYXRhCmRhdGEgPSB7CiAgICAnQWdlJzogWzI1LCA0NSwgMzUsIDUwLCAyMywgNDAsIDYwLCA0OCwgMzMsIDU1XSwKICAgICdJbmNvbWUnOiBbJ0hpZ2gnLCAnSGlnaCcsICdNZWRpdW0nLCAnTWVkaXVtJywgJ0xvdycsICdMb3cnLCAnTG93JywgJ01lZGl1bScsICdIaWdoJywgJ0xvdyddLAogICAgJ0J1eXNfQ29tcHV0ZXInOiBbJ05vJywgJ05vJywgJ1llcycsICdZZXMnLCAnTm8nLCAnTm8nLCAnWWVzJywgJ1llcycsICdZZXMnLCAnTm8nXQp9CgojIENvbnZlcnQgdG8gRGF0YUZyYW1lCmRmID0gcGQuRGF0YUZyYW1lKGRhdGEpCgojIEVuY29kZSBjYXRlZ29yaWNhbCB2YXJpYWJsZXMKZGZbJ0luY29tZSddID0gZGZbJ0luY29tZSddLm1hcCh7J0xvdyc6IDAsICdNZWRpdW0nOiAxLCAnSGlnaCc6IDJ9KQpkZlsnQnV5c19Db21wdXRlciddID0gZGZbJ0J1eXNfQ29tcHV0ZXInXS5tYXAoeydObyc6IDAsICdZZXMnOiAxfSkKCiMgRmVhdHVyZXMgYW5kIFRhcmdldApYID0gZGZbWydBZ2UnLCAnSW5jb21lJ11dCnkgPSBkZlsnQnV5c19Db21wdXRlciddCgojIEluaXRpYWxpemUgYW5kIFRyYWluIHRoZSBNb2RlbAptb2RlbCA9IERlY2lzaW9uVHJlZUNsYXNzaWZpZXIoKQptb2RlbC5maXQoWCwgeSkKCiMgUHJlZGljdApwcmVkaWN0aW9ucyA9IG1vZGVsLnByZWRpY3QoW1szMCwgMV0sIFs0MCwgMl1dKQpwcmludChwcmVkaWN0aW9ucykgICMgT3V0cHV0OiBbMSAwXQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.tree import DecisionTreeClassifier
from sklearn import datasets
import pandas as pd

# Sample Data
data = {
    'Age': [25, 45, 35, 50, 23, 40, 60, 48, 33, 55],
    'Income': ['High', 'High', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low'],
    'Buys_Computer': ['No', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No']
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Encode categorical variables
df['Income'] = df['Income'].map({'Low': 0, 'Medium': 1, 'High': 2})
df['Buys_Computer'] = df['Buys_Computer'].map({'No': 0, 'Yes': 1})

# Features and Target
X = df[['Age', 'Income']]
y = df['Buys_Computer']

# Initialize and Train the Model
model = DecisionTreeClassifier()
model.fit(X, y)

# Predict
predictions = model.predict([[30, 1], [40, 2]])
print(predictions)  # Output: [1 0]</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ul>
<li><strong>Data Preparation</strong>: We created a sample dataset and converted it into a DataFrame.</li>
<li><strong>Encoding</strong>: Categorical variables were encoded into numerical values.</li>
<li><strong>Features and Target</strong>: Selected 'Age' and 'Income' as features and 'Buys_Computer' as the target variable.</li>
<li><strong>Model Training</strong>: Initialized and trained the Decision Tree model.</li>
<li><strong>Prediction</strong>: Made predictions for new data points.</li>
</ul>
</div><h1>Practical Exercises</h1>
<div class='content'></div><h2>Exercise 1: Building a Decision Tree</h2>
<div class='content'><p><strong>Task</strong>: Use the provided dataset to build a Decision Tree classifier and predict whether a person will buy a computer.</p>
<p><strong>Dataset</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGF0YSA9IHsKICAgICdBZ2UnOiBbMjUsIDQ1LCAzNSwgNTAsIDIzLCA0MCwgNjAsIDQ4LCAzMywgNTVdLAogICAgJ0luY29tZSc6IFsnSGlnaCcsICdIaWdoJywgJ01lZGl1bScsICdNZWRpdW0nLCAnTG93JywgJ0xvdycsICdMb3cnLCAnTWVkaXVtJywgJ0hpZ2gnLCAnTG93J10sCiAgICAnQnV5c19Db21wdXRlcic6IFsnTm8nLCAnTm8nLCAnWWVzJywgJ1llcycsICdObycsICdObycsICdZZXMnLCAnWWVzJywgJ1llcycsICdObyddCn0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>data = {
    'Age': [25, 45, 35, 50, 23, 40, 60, 48, 33, 55],
    'Income': ['High', 'High', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low'],
    'Buys_Computer': ['No', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No']
}</pre></div><div class='content'><p><strong>Steps</strong>:</p>
<ol>
<li>Convert the dataset into a DataFrame.</li>
<li>Encode the categorical variables.</li>
<li>Split the data into features and target.</li>
<li>Initialize and train the Decision Tree model.</li>
<li>Make predictions for new data points.</li>
</ol>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHBhbmRhcyBhcyBwZApmcm9tIHNrbGVhcm4udHJlZSBpbXBvcnQgRGVjaXNpb25UcmVlQ2xhc3NpZmllcgoKIyBTYW1wbGUgRGF0YQpkYXRhID0gewogICAgJ0FnZSc6IFsyNSwgNDUsIDM1LCA1MCwgMjMsIDQwLCA2MCwgNDgsIDMzLCA1NV0sCiAgICAnSW5jb21lJzogWydIaWdoJywgJ0hpZ2gnLCAnTWVkaXVtJywgJ01lZGl1bScsICdMb3cnLCAnTG93JywgJ0xvdycsICdNZWRpdW0nLCAnSGlnaCcsICdMb3cnXSwKICAgICdCdXlzX0NvbXB1dGVyJzogWydObycsICdObycsICdZZXMnLCAnWWVzJywgJ05vJywgJ05vJywgJ1llcycsICdZZXMnLCAnWWVzJywgJ05vJ10KfQoKIyBDb252ZXJ0IHRvIERhdGFGcmFtZQpkZiA9IHBkLkRhdGFGcmFtZShkYXRhKQoKIyBFbmNvZGUgY2F0ZWdvcmljYWwgdmFyaWFibGVzCmRmWydJbmNvbWUnXSA9IGRmWydJbmNvbWUnXS5tYXAoeydMb3cnOiAwLCAnTWVkaXVtJzogMSwgJ0hpZ2gnOiAyfSkKZGZbJ0J1eXNfQ29tcHV0ZXInXSA9IGRmWydCdXlzX0NvbXB1dGVyJ10ubWFwKHsnTm8nOiAwLCAnWWVzJzogMX0pCgojIEZlYXR1cmVzIGFuZCBUYXJnZXQKWCA9IGRmW1snQWdlJywgJ0luY29tZSddXQp5ID0gZGZbJ0J1eXNfQ29tcHV0ZXInXQoKIyBJbml0aWFsaXplIGFuZCBUcmFpbiB0aGUgTW9kZWwKbW9kZWwgPSBEZWNpc2lvblRyZWVDbGFzc2lmaWVyKCkKbW9kZWwuZml0KFgsIHkpCgojIFByZWRpY3QKcHJlZGljdGlvbnMgPSBtb2RlbC5wcmVkaWN0KFtbMzAsIDFdLCBbNDAsIDJdXSkKcHJpbnQocHJlZGljdGlvbnMpICAjIE91dHB1dDogWzEgMF0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import pandas as pd
from sklearn.tree import DecisionTreeClassifier

# Sample Data
data = {
    'Age': [25, 45, 35, 50, 23, 40, 60, 48, 33, 55],
    'Income': ['High', 'High', 'Medium', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'High', 'Low'],
    'Buys_Computer': ['No', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No']
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Encode categorical variables
df['Income'] = df['Income'].map({'Low': 0, 'Medium': 1, 'High': 2})
df['Buys_Computer'] = df['Buys_Computer'].map({'No': 0, 'Yes': 1})

# Features and Target
X = df[['Age', 'Income']]
y = df['Buys_Computer']

# Initialize and Train the Model
model = DecisionTreeClassifier()
model.fit(X, y)

# Predict
predictions = model.predict([[30, 1], [40, 2]])
print(predictions)  # Output: [1 0]</pre></div><div class='content'></div><h2>Exercise 2: Visualizing the Decision Tree</h2>
<div class='content'><p><strong>Task</strong>: Visualize the trained Decision Tree using the <code>graphviz</code> library.</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Install the <code>graphviz</code> library.</li>
<li>Export the trained Decision Tree to a DOT format.</li>
<li>Visualize the tree using <code>graphviz</code>.</li>
</ol>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLnRyZWUgaW1wb3J0IGV4cG9ydF9ncmFwaHZpegppbXBvcnQgZ3JhcGh2aXoKCiMgRXhwb3J0IHRoZSB0cmVlIHRvIERPVCBmb3JtYXQKZG90X2RhdGEgPSBleHBvcnRfZ3JhcGh2aXoobW9kZWwsIG91dF9maWxlPU5vbmUsIAogICAgICAgICAgICAgICAgICAgICAgICAgICBmZWF0dXJlX25hbWVzPVsnQWdlJywgJ0luY29tZSddLCAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgIGNsYXNzX25hbWVzPVsnTm8nLCAnWWVzJ10sICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgZmlsbGVkPVRydWUsIHJvdW5kZWQ9VHJ1ZSwgIAogICAgICAgICAgICAgICAgICAgICAgICAgICBzcGVjaWFsX2NoYXJhY3RlcnM9VHJ1ZSkgIAoKIyBWaXN1YWxpemUgdGhlIHRyZWUKZ3JhcGggPSBncmFwaHZpei5Tb3VyY2UoZG90X2RhdGEpICAKZ3JhcGgucmVuZGVyKCJkZWNpc2lvbl90cmVlIikgICMgVGhpcyB3aWxsIHNhdmUgdGhlIHRyZWUgYXMgYSBQREYgZmlsZQpncmFwaC52aWV3KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.tree import export_graphviz
import graphviz

# Export the tree to DOT format
dot_data = export_graphviz(model, out_file=None, 
                           feature_names=['Age', 'Income'],  
                           class_names=['No', 'Yes'],  
                           filled=True, rounded=True,  
                           special_characters=True)  

# Visualize the tree
graph = graphviz.Source(dot_data)  
graph.render(&quot;decision_tree&quot;)  # This will save the tree as a PDF file
graph.view()</pre></div><div class='content'></div><h1>Common Mistakes and Tips</h1>
<div class='content'><ol>
<li><strong>Overfitting</strong>: Decision Trees are prone to overfitting. Use techniques like pruning, setting a maximum depth, or minimum samples per leaf to mitigate this.</li>
<li><strong>Data Preprocessing</strong>: Ensure that the data is properly preprocessed, including handling missing values and encoding categorical variables.</li>
<li><strong>Feature Selection</strong>: Carefully select features that are relevant to the problem to improve the model's performance.</li>
</ol>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we covered the basics of Decision Trees, including key concepts, how they work, and practical examples. We also provided exercises to reinforce the learned concepts. Decision Trees are a powerful tool in the machine learning toolkit, and understanding their workings is crucial for building effective models. In the next section, we will explore another popular supervised learning algorithm: Support Vector Machines (SVM).</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='04-02-logistic-regression' title="Logistic Regression">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='04-04-svm' title="Support Vector Machines (SVM)">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
