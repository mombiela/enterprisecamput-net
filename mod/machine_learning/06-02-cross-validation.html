<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow, noarchive">
    <title>Cross-Validation</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/06-02-validacion-cruzada" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/06-02-validacion-cruzada" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/06-02-cross-validation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css?v=4" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>var DEVELOP = window.location.href.indexOf("localhost")!=-1 && true;</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>  	
	</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/machine_learning/06-02-validacion-cruzada" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/machine_learning/06-02-validacion-cruzada" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col" id="left_menu">
					<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Course Content
	</a>

			</div>
		</div>
	</div>
</div>

<!--  
<div class="top-bar container-fluid">
	<div class="container-xxl">
		<nav class="navbar navbar-expand-md p-0">
			<div class="container-fluid">
				<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
					<span class="navbar-toggler-icon"></span>
				</button>
				
				<div class="collapse navbar-collapse" id="navbarNav">
					<div class="navbar-nav me-auto">
							<a href="/"  class="nav-link px-3">
		<i class="bi bi-house-fill"></i>
		HOME
	</a>

	<a href="./"  class="nav-link px-3">
		<i class="bi bi-journal-bookmark"></i>
		Course Content
	</a>

					</div>
				</div>			
							</div>
		</nav>
	</div>
</div>
 -->				<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
				<div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-01-evaluation-metrics' title="Evaluation Metrics">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
					<h2 style="text-decoration:underline">Cross-Validation</h2>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='06-03-roc-curve-auc' title="ROC Curve and AUC">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>
<div class='content'><p>Cross-validation is a statistical method used to estimate the skill of machine learning models. It is primarily used in applied machine learning to estimate the performance of a model on unseen data. This method helps in assessing how the results of a statistical analysis will generalize to an independent dataset. In this section, we will cover the following:</p>
<ol>
<li><strong>What is Cross-Validation?</strong></li>
<li><strong>Types of Cross-Validation</strong></li>
<li><strong>Implementing Cross-Validation in Python</strong></li>
<li><strong>Practical Exercises</strong></li>
</ol>
</div><h1><ol>
<li>What is Cross-Validation?</li>
</ol></h1>
<div class='content'><p>Cross-validation involves partitioning a dataset into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation or testing set). The primary goal is to test the model's ability to predict new data that was not used in estimating it, to flag problems like overfitting or selection bias, and to give an insight into how the model will generalize to an independent dataset.</p>
</div><h2>Key Concepts:</h2>
<div class='content'><ul>
<li><strong>Training Set</strong>: The subset of the dataset used to train the model.</li>
<li><strong>Validation Set</strong>: The subset of the dataset used to validate the model's performance.</li>
<li><strong>Test Set</strong>: An independent subset used to test the final model's performance.</li>
</ul>
</div><h1><ol start="2">
<li>Types of Cross-Validation</li>
</ol></h1>
<div class='content'></div><h2>2.1. Holdout Method</h2>
<div class='content'><p>The dataset is randomly divided into two subsets: a training set and a testing set. Typically, 70-80% of the data is used for training, and the remaining 20-30% is used for testing.</p>
</div><h2>2.2. K-Fold Cross-Validation</h2>
<div class='content'><p>The dataset is divided into 'k' equally sized folds. The model is trained on 'k-1' folds and tested on the remaining fold. This process is repeated 'k' times, with each fold being used exactly once as the test set. The final performance metric is the average of the metrics from each fold.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgS0ZvbGQKaW1wb3J0IG51bXB5IGFzIG5wCgojIEV4YW1wbGUgZGF0YXNldApkYXRhID0gbnAuYXJyYXkoWzEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwXSkKCiMgS0ZvbGQgY3Jvc3MtdmFsaWRhdGlvbgprZiA9IEtGb2xkKG5fc3BsaXRzPTUpCmZvciB0cmFpbl9pbmRleCwgdGVzdF9pbmRleCBpbiBrZi5zcGxpdChkYXRhKToKICAgIHByaW50KCJUUkFJTjoiLCB0cmFpbl9pbmRleCwgIlRFU1Q6IiwgdGVzdF9pbmRleCkKICAgIFhfdHJhaW4sIFhfdGVzdCA9IGRhdGFbdHJhaW5faW5kZXhdLCBkYXRhW3Rlc3RfaW5kZXhdCiAgICBwcmludCgiWF90cmFpbjoiLCBYX3RyYWluLCAiWF90ZXN0OiIsIFhfdGVzdCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import KFold
import numpy as np

# Example dataset
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# KFold cross-validation
kf = KFold(n_splits=5)
for train_index, test_index in kf.split(data):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = data[train_index], data[test_index]
    print(&quot;X_train:&quot;, X_train, &quot;X_test:&quot;, X_test)</pre></div><div class='content'></div><h2>2.3. Stratified K-Fold Cross-Validation</h2>
<div class='content'><p>Similar to K-Fold Cross-Validation, but it ensures that each fold is representative of the whole dataset, particularly useful for imbalanced datasets.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgU3RyYXRpZmllZEtGb2xkCgojIEV4YW1wbGUgZGF0YXNldApYID0gbnAuYXJyYXkoW1sxXSwgWzJdLCBbM10sIFs0XSwgWzVdLCBbNl0sIFs3XSwgWzhdLCBbOV0sIFsxMF1dKQp5ID0gbnAuYXJyYXkoWzAsIDAsIDAsIDAsIDEsIDEsIDEsIDEsIDEsIDFdKQoKIyBTdHJhdGlmaWVkS0ZvbGQgY3Jvc3MtdmFsaWRhdGlvbgpza2YgPSBTdHJhdGlmaWVkS0ZvbGQobl9zcGxpdHM9NSkKZm9yIHRyYWluX2luZGV4LCB0ZXN0X2luZGV4IGluIHNrZi5zcGxpdChYLCB5KToKICAgIHByaW50KCJUUkFJTjoiLCB0cmFpbl9pbmRleCwgIlRFU1Q6IiwgdGVzdF9pbmRleCkKICAgIFhfdHJhaW4sIFhfdGVzdCA9IFhbdHJhaW5faW5kZXhdLCBYW3Rlc3RfaW5kZXhdCiAgICB5X3RyYWluLCB5X3Rlc3QgPSB5W3RyYWluX2luZGV4XSwgeVt0ZXN0X2luZGV4XQogICAgcHJpbnQoIlhfdHJhaW46IiwgWF90cmFpbiwgIlhfdGVzdDoiLCBYX3Rlc3QsICJ5X3RyYWluOiIsIHlfdHJhaW4sICJ5X3Rlc3Q6IiwgeV90ZXN0KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import StratifiedKFold

# Example dataset
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

# StratifiedKFold cross-validation
skf = StratifiedKFold(n_splits=5)
for train_index, test_index in skf.split(X, y):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(&quot;X_train:&quot;, X_train, &quot;X_test:&quot;, X_test, &quot;y_train:&quot;, y_train, &quot;y_test:&quot;, y_test)</pre></div><div class='content'></div><h2>2.4. Leave-One-Out Cross-Validation (LOOCV)</h2>
<div class='content'><p>Each sample in the dataset is used once as a test set while the remaining samples form the training set. This method is computationally expensive but useful for small datasets.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgTGVhdmVPbmVPdXQKCiMgRXhhbXBsZSBkYXRhc2V0CmRhdGEgPSBucC5hcnJheShbMSwgMiwgMywgNCwgNV0pCgojIExlYXZlLU9uZS1PdXQgY3Jvc3MtdmFsaWRhdGlvbgpsb28gPSBMZWF2ZU9uZU91dCgpCmZvciB0cmFpbl9pbmRleCwgdGVzdF9pbmRleCBpbiBsb28uc3BsaXQoZGF0YSk6CiAgICBwcmludCgiVFJBSU46IiwgdHJhaW5faW5kZXgsICJURVNUOiIsIHRlc3RfaW5kZXgpCiAgICBYX3RyYWluLCBYX3Rlc3QgPSBkYXRhW3RyYWluX2luZGV4XSwgZGF0YVt0ZXN0X2luZGV4XQogICAgcHJpbnQoIlhfdHJhaW46IiwgWF90cmFpbiwgIlhfdGVzdDoiLCBYX3Rlc3Qp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import LeaveOneOut

# Example dataset
data = np.array([1, 2, 3, 4, 5])

# Leave-One-Out cross-validation
loo = LeaveOneOut()
for train_index, test_index in loo.split(data):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = data[train_index], data[test_index]
    print(&quot;X_train:&quot;, X_train, &quot;X_test:&quot;, X_test)</pre></div><div class='content'></div><h1><ol start="3">
<li>Implementing Cross-Validation in Python</li>
</ol></h1>
<div class='content'></div><h2>Example: Using K-Fold Cross-Validation with Scikit-Learn</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXMKZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgY3Jvc3NfdmFsX3Njb3JlLCBLRm9sZApmcm9tIHNrbGVhcm4ubGluZWFyX21vZGVsIGltcG9ydCBMb2dpc3RpY1JlZ3Jlc3Npb24KCiMgTG9hZCBkYXRhc2V0CmlyaXMgPSBsb2FkX2lyaXMoKQpYLCB5ID0gaXJpcy5kYXRhLCBpcmlzLnRhcmdldAoKIyBEZWZpbmUgbW9kZWwKbW9kZWwgPSBMb2dpc3RpY1JlZ3Jlc3Npb24obWF4X2l0ZXI9MjAwKQoKIyBLRm9sZCBjcm9zcy12YWxpZGF0aW9uCmtmID0gS0ZvbGQobl9zcGxpdHM9NSwgc2h1ZmZsZT1UcnVlLCByYW5kb21fc3RhdGU9NDIpCnNjb3JlcyA9IGNyb3NzX3ZhbF9zY29yZShtb2RlbCwgWCwgeSwgY3Y9a2YpCgpwcmludCgiQ3Jvc3MtVmFsaWRhdGlvbiBTY29yZXM6Iiwgc2NvcmVzKQpwcmludCgiTWVhbiBBY2N1cmFjeToiLCBzY29yZXMubWVhbigpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LogisticRegression

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Define model
model = LogisticRegression(max_iter=200)

# KFold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf)

print(&quot;Cross-Validation Scores:&quot;, scores)
print(&quot;Mean Accuracy:&quot;, scores.mean())</pre></div><div class='content'></div><h2>Explanation:</h2>
<div class='content'><ul>
<li><strong>load_iris()</strong>: Loads the Iris dataset.</li>
<li><strong>LogisticRegression()</strong>: Defines the logistic regression model.</li>
<li><strong>KFold()</strong>: Creates a K-Fold cross-validator.</li>
<li><strong>cross_val_score()</strong>: Evaluates the model using cross-validation.</li>
</ul>
</div><h1><ol start="4">
<li>Practical Exercises</li>
</ol></h1>
<div class='content'></div><h2>Exercise 1: Implement K-Fold Cross-Validation</h2>
<div class='content'><p><strong>Task</strong>: Implement K-Fold Cross-Validation on the <code>digits</code> dataset using a <code>DecisionTreeClassifier</code>.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2RpZ2l0cwpmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCBjcm9zc192YWxfc2NvcmUsIEtGb2xkCmZyb20gc2tsZWFybi50cmVlIGltcG9ydCBEZWNpc2lvblRyZWVDbGFzc2lmaWVyCgojIExvYWQgZGF0YXNldApkaWdpdHMgPSBsb2FkX2RpZ2l0cygpClgsIHkgPSBkaWdpdHMuZGF0YSwgZGlnaXRzLnRhcmdldAoKIyBEZWZpbmUgbW9kZWwKbW9kZWwgPSBEZWNpc2lvblRyZWVDbGFzc2lmaWVyKCkKCiMgS0ZvbGQgY3Jvc3MtdmFsaWRhdGlvbgprZiA9IEtGb2xkKG5fc3BsaXRzPTEwLCBzaHVmZmxlPVRydWUsIHJhbmRvbV9zdGF0ZT00MikKc2NvcmVzID0gY3Jvc3NfdmFsX3Njb3JlKG1vZGVsLCBYLCB5LCBjdj1rZikKCnByaW50KCJDcm9zcy1WYWxpZGF0aW9uIFNjb3JlczoiLCBzY29yZXMpCnByaW50KCJNZWFuIEFjY3VyYWN5OiIsIHNjb3Jlcy5tZWFuKCkp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_digits
from sklearn.model_selection import cross_val_score, KFold
from sklearn.tree import DecisionTreeClassifier

# Load dataset
digits = load_digits()
X, y = digits.data, digits.target

# Define model
model = DecisionTreeClassifier()

# KFold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf)

print(&quot;Cross-Validation Scores:&quot;, scores)
print(&quot;Mean Accuracy:&quot;, scores.mean())</pre></div><div class='content'></div><h2>Solution Explanation:</h2>
<div class='content'><ul>
<li><strong>load_digits()</strong>: Loads the Digits dataset.</li>
<li><strong>DecisionTreeClassifier()</strong>: Defines the decision tree classifier model.</li>
<li><strong>KFold()</strong>: Creates a K-Fold cross-validator with 10 splits.</li>
<li><strong>cross_val_score()</strong>: Evaluates the model using cross-validation.</li>
</ul>
</div><h2>Exercise 2: Implement Stratified K-Fold Cross-Validation</h2>
<div class='content'><p><strong>Task</strong>: Implement Stratified K-Fold Cross-Validation on the <code>breast_cancer</code> dataset using a <code>RandomForestClassifier</code>.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2JyZWFzdF9jYW5jZXIKZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgU3RyYXRpZmllZEtGb2xkLCBjcm9zc192YWxfc2NvcmUKZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBSYW5kb21Gb3Jlc3RDbGFzc2lmaWVyCgojIExvYWQgZGF0YXNldApicmVhc3RfY2FuY2VyID0gbG9hZF9icmVhc3RfY2FuY2VyKCkKWCwgeSA9IGJyZWFzdF9jYW5jZXIuZGF0YSwgYnJlYXN0X2NhbmNlci50YXJnZXQKCiMgRGVmaW5lIG1vZGVsCm1vZGVsID0gUmFuZG9tRm9yZXN0Q2xhc3NpZmllcigpCgojIFN0cmF0aWZpZWRLRm9sZCBjcm9zcy12YWxpZGF0aW9uCnNrZiA9IFN0cmF0aWZpZWRLRm9sZChuX3NwbGl0cz01LCBzaHVmZmxlPVRydWUsIHJhbmRvbV9zdGF0ZT00MikKc2NvcmVzID0gY3Jvc3NfdmFsX3Njb3JlKG1vZGVsLCBYLCB5LCBjdj1za2YpCgpwcmludCgiQ3Jvc3MtVmFsaWRhdGlvbiBTY29yZXM6Iiwgc2NvcmVzKQpwcmludCgiTWVhbiBBY2N1cmFjeToiLCBzY29yZXMubWVhbigpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Load dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

# Define model
model = RandomForestClassifier()

# StratifiedKFold cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)

print(&quot;Cross-Validation Scores:&quot;, scores)
print(&quot;Mean Accuracy:&quot;, scores.mean())</pre></div><div class='content'></div><h2>Solution Explanation:</h2>
<div class='content'><ul>
<li><strong>load_breast_cancer()</strong>: Loads the Breast Cancer dataset.</li>
<li><strong>RandomForestClassifier()</strong>: Defines the random forest classifier model.</li>
<li><strong>StratifiedKFold()</strong>: Creates a Stratified K-Fold cross-validator with 5 splits.</li>
<li><strong>cross_val_score()</strong>: Evaluates the model using cross-validation.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>Cross-validation is a crucial technique in machine learning for assessing model performance and ensuring that the model generalizes well to unseen data. By understanding and implementing various cross-validation methods, you can improve the robustness and reliability of your machine learning models. In the next section, we will delve into evaluation metrics to further enhance our understanding of model performance.</p>
</div><div class='row navigation'>
	<div class='col-1 col-md-2'>
					<a href='06-01-evaluation-metrics' title="Evaluation Metrics">
				<span class="d-none d-md-inline">&#x25C4; Previous</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-left-square-fill"></i></span>
			</a>
			</div>
	<div class='col-10 col-md-8 text-center'>
			</div>
	<div class='col-1 col-md-2 text-end'>
					<a href='06-03-roc-curve-auc' title="ROC Curve and AUC">
				<span class="d-none d-md-inline">Next &#x25BA;</span>
				<span class="d-inline d-md-none"><i class="bi bi-caret-right-square-fill"></i></span>
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
				<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725"
	     crossorigin="anonymous"></script>
	<!-- enterprise_campus -->
	<ins class="adsbygoogle"
	     style="display:block"
	     data-ad-client="ca-pub-0611338592562725"
	     data-ad-slot="6914733106"
	     data-ad-format="auto"
	     data-full-width-responsive="true"></ins>
	<script>
	     (adsbygoogle = window.adsbygoogle || []).push({});
	</script>
			
	<div class="container mt-2 d-none d-md-block index">
		<h1>Machine Learning Course</h1>
<h2>Module 1: Introduction to Machine Learning</h2>
<ul>
<li><a href="01-01-what-is-machine-learning">What is Machine Learning?</a></li>
<li><a href="01-02-history-evolution-machine-learning">History and Evolution of Machine Learning</a></li>
<li><a href="01-03-types-machine-learning">Types of Machine Learning</a></li>
<li><a href="01-04-applications-machine-learning">Applications of Machine Learning</a></li>
</ul>
<h2>Module 2: Fundamentals of Statistics and Probability</h2>
<ul>
<li><a href="02-01-basic-concepts-statistics">Basic Concepts of Statistics</a></li>
<li><a href="02-02-probability-distributions">Probability Distributions</a></li>
<li><a href="02-03-statistical-inference">Statistical Inference</a></li>
<li><a href="02-04-bayes-theorem">Bayes' Theorem</a></li>
</ul>
<h2>Module 3: Data Preprocessing</h2>
<ul>
<li><a href="03-01-data-cleaning">Data Cleaning</a></li>
<li><a href="03-02-data-transformation">Data Transformation</a></li>
<li><a href="03-03-normalization-standardization">Normalization and Standardization</a></li>
<li><a href="03-04-handling-missing-data">Handling Missing Data</a></li>
</ul>
<h2>Module 4: Supervised Machine Learning Algorithms</h2>
<ul>
<li><a href="04-01-linear-regression">Linear Regression</a></li>
<li><a href="04-02-logistic-regression">Logistic Regression</a></li>
<li><a href="04-03-decision-trees">Decision Trees</a></li>
<li><a href="04-04-svm">Support Vector Machines (SVM)</a></li>
<li><a href="04-05-knn">K-Nearest Neighbors (K-NN)</a></li>
<li><a href="04-06-neural-networks">Neural Networks</a></li>
</ul>
<h2>Module 5: Unsupervised Machine Learning Algorithms</h2>
<ul>
<li><a href="05-01-clustering-k-means">Clustering: K-means</a></li>
<li><a href="05-02-hierarchical-clustering">Hierarchical Clustering</a></li>
<li><a href="05-03-pca">Principal Component Analysis (PCA)</a></li>
<li><a href="05-04-dbscan">DBSCAN Clustering Analysis</a></li>
</ul>
<h2>Module 6: Model Evaluation and Validation</h2>
<ul>
<li><a href="06-01-evaluation-metrics">Evaluation Metrics</a></li>
<li><a href="06-02-cross-validation">Cross-Validation</a></li>
<li><a href="06-03-roc-curve-auc">ROC Curve and AUC</a></li>
<li><a href="06-04-overfitting-underfitting">Overfitting and Underfitting</a></li>
</ul>
<h2>Module 7: Advanced Techniques and Optimization</h2>
<ul>
<li><a href="07-01-ensemble-learning">Ensemble Learning</a></li>
<li><a href="07-02-gradient-boosting">Gradient Boosting</a></li>
<li><a href="07-03-deep-learning">Deep Learning</a></li>
<li><a href="07-04-hyperparameter-optimization">Hyperparameter Optimization</a></li>
</ul>
<h2>Module 8: Model Implementation and Deployment</h2>
<ul>
<li><a href="08-01-frameworks-libraries">Popular Frameworks and Libraries</a></li>
<li><a href="08-02-implementation-production">Model Implementation in Production</a></li>
<li><a href="08-03-maintenance-monitoring">Model Maintenance and Monitoring</a></li>
<li><a href="08-04-ethics-privacy">Ethical and Privacy Considerations</a></li>
</ul>
<h2>Module 9: Practical Projects</h2>
<ul>
<li><a href="09-01-project-housing-price-prediction">Project 1: Housing Price Prediction</a></li>
<li><a href="09-02-project-image-classification">Project 2: Image Classification</a></li>
<li><a href="09-03-project-sentiment-analysis">Project 3: Sentiment Analysis on Social Media</a></li>
<li><a href="09-04-project-fraud-detection">Project 4: Fraud Detection</a></li>
</ul>
<h2>Module 10: Additional Resources</h2>
<ul>
<li><a href="10-01-recommended-books">Recommended Books</a></li>
<li><a href="10-02-online-courses">Online Courses</a></li>
<li><a href="10-03-communities-forums">Communities and Forums</a></li>
<li><a href="10-04-tools-software">Tools and Software</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective">The Project</a> | 
<a href="/about">About Us</a> | 
<a href="/contribute">Contribute</a> | 
<a href="/donate">Donations</a> | 
<a href="/licence">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
