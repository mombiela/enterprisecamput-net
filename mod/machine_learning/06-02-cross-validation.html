<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross-Validation</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/06-02-validacion-cruzada" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/06-02-validacion-cruzada" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/06-02-cross-validation" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-8 p-2 p-md-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-4 p-2 p-md-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/machine_learning/06-02-validacion-cruzada" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/06-02-validacion-cruzada" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
								<div class='row navigation'>
	<div class='col-2'>
					<a href='06-01-evaluation-metrics' title="Evaluation Metrics">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
					<a href="./"><h2 style="text-decoration:underline">Cross-Validation</h2></a>
			</div>
	<div class='col-2 text-end'>
					<a href='06-03-roc-curve-auc' title="ROC Curve and AUC">Next &#x25BA;</a>
			</div>
</div>
<div class='content'><p>Cross-validation is a statistical method used to estimate the skill of machine learning models. It is primarily used in applied machine learning to estimate the performance of a model on unseen data. This method helps in assessing how the results of a statistical analysis will generalize to an independent dataset. In this section, we will cover the following:</p>
<ol>
<li><strong>What is Cross-Validation?</strong></li>
<li><strong>Types of Cross-Validation</strong></li>
<li><strong>Implementing Cross-Validation in Python</strong></li>
<li><strong>Practical Exercises</strong></li>
</ol>
</div><h1><ol>
<li>What is Cross-Validation?</li>
</ol>
</h1>
<div class='content'><p>Cross-validation involves partitioning a dataset into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation or testing set). The primary goal is to test the model's ability to predict new data that was not used in estimating it, to flag problems like overfitting or selection bias, and to give an insight into how the model will generalize to an independent dataset.</p>
</div><h2><p>Key Concepts:</p>
</h2>
<div class='content'><ul>
<li><strong>Training Set</strong>: The subset of the dataset used to train the model.</li>
<li><strong>Validation Set</strong>: The subset of the dataset used to validate the model's performance.</li>
<li><strong>Test Set</strong>: An independent subset used to test the final model's performance.</li>
</ul>
</div><h1><ol start="2">
<li>Types of Cross-Validation</li>
</ol>
</h1>
<div class='content'></div><h2><p>2.1. Holdout Method</p>
</h2>
<div class='content'><p>The dataset is randomly divided into two subsets: a training set and a testing set. Typically, 70-80% of the data is used for training, and the remaining 20-30% is used for testing.</p>
</div><h2><p>2.2. K-Fold Cross-Validation</p>
</h2>
<div class='content'><p>The dataset is divided into 'k' equally sized folds. The model is trained on 'k-1' folds and tested on the remaining fold. This process is repeated 'k' times, with each fold being used exactly once as the test set. The final performance metric is the average of the metrics from each fold.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgS0ZvbGQKaW1wb3J0IG51bXB5IGFzIG5wCgojIEV4YW1wbGUgZGF0YXNldApkYXRhID0gbnAuYXJyYXkoWzEsIDIsIDMsIDQsIDUsIDYsIDcsIDgsIDksIDEwXSkKCiMgS0ZvbGQgY3Jvc3MtdmFsaWRhdGlvbgprZiA9IEtGb2xkKG5fc3BsaXRzPTUpCmZvciB0cmFpbl9pbmRleCwgdGVzdF9pbmRleCBpbiBrZi5zcGxpdChkYXRhKToKICAgIHByaW50KCJUUkFJTjoiLCB0cmFpbl9pbmRleCwgIlRFU1Q6IiwgdGVzdF9pbmRleCkKICAgIFhfdHJhaW4sIFhfdGVzdCA9IGRhdGFbdHJhaW5faW5kZXhdLCBkYXRhW3Rlc3RfaW5kZXhdCiAgICBwcmludCgiWF90cmFpbjoiLCBYX3RyYWluLCAiWF90ZXN0OiIsIFhfdGVzdCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import KFold
import numpy as np

# Example dataset
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# KFold cross-validation
kf = KFold(n_splits=5)
for train_index, test_index in kf.split(data):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = data[train_index], data[test_index]
    print(&quot;X_train:&quot;, X_train, &quot;X_test:&quot;, X_test)</pre></div><div class='content'></div><h2><p>2.3. Stratified K-Fold Cross-Validation</p>
</h2>
<div class='content'><p>Similar to K-Fold Cross-Validation, but it ensures that each fold is representative of the whole dataset, particularly useful for imbalanced datasets.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgU3RyYXRpZmllZEtGb2xkCgojIEV4YW1wbGUgZGF0YXNldApYID0gbnAuYXJyYXkoW1sxXSwgWzJdLCBbM10sIFs0XSwgWzVdLCBbNl0sIFs3XSwgWzhdLCBbOV0sIFsxMF1dKQp5ID0gbnAuYXJyYXkoWzAsIDAsIDAsIDAsIDEsIDEsIDEsIDEsIDEsIDFdKQoKIyBTdHJhdGlmaWVkS0ZvbGQgY3Jvc3MtdmFsaWRhdGlvbgpza2YgPSBTdHJhdGlmaWVkS0ZvbGQobl9zcGxpdHM9NSkKZm9yIHRyYWluX2luZGV4LCB0ZXN0X2luZGV4IGluIHNrZi5zcGxpdChYLCB5KToKICAgIHByaW50KCJUUkFJTjoiLCB0cmFpbl9pbmRleCwgIlRFU1Q6IiwgdGVzdF9pbmRleCkKICAgIFhfdHJhaW4sIFhfdGVzdCA9IFhbdHJhaW5faW5kZXhdLCBYW3Rlc3RfaW5kZXhdCiAgICB5X3RyYWluLCB5X3Rlc3QgPSB5W3RyYWluX2luZGV4XSwgeVt0ZXN0X2luZGV4XQogICAgcHJpbnQoIlhfdHJhaW46IiwgWF90cmFpbiwgIlhfdGVzdDoiLCBYX3Rlc3QsICJ5X3RyYWluOiIsIHlfdHJhaW4sICJ5X3Rlc3Q6IiwgeV90ZXN0KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import StratifiedKFold

# Example dataset
X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])
y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])

# StratifiedKFold cross-validation
skf = StratifiedKFold(n_splits=5)
for train_index, test_index in skf.split(X, y):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    print(&quot;X_train:&quot;, X_train, &quot;X_test:&quot;, X_test, &quot;y_train:&quot;, y_train, &quot;y_test:&quot;, y_test)</pre></div><div class='content'></div><h2><p>2.4. Leave-One-Out Cross-Validation (LOOCV)</p>
</h2>
<div class='content'><p>Each sample in the dataset is used once as a test set while the remaining samples form the training set. This method is computationally expensive but useful for small datasets.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgTGVhdmVPbmVPdXQKCiMgRXhhbXBsZSBkYXRhc2V0CmRhdGEgPSBucC5hcnJheShbMSwgMiwgMywgNCwgNV0pCgojIExlYXZlLU9uZS1PdXQgY3Jvc3MtdmFsaWRhdGlvbgpsb28gPSBMZWF2ZU9uZU91dCgpCmZvciB0cmFpbl9pbmRleCwgdGVzdF9pbmRleCBpbiBsb28uc3BsaXQoZGF0YSk6CiAgICBwcmludCgiVFJBSU46IiwgdHJhaW5faW5kZXgsICJURVNUOiIsIHRlc3RfaW5kZXgpCiAgICBYX3RyYWluLCBYX3Rlc3QgPSBkYXRhW3RyYWluX2luZGV4XSwgZGF0YVt0ZXN0X2luZGV4XQogICAgcHJpbnQoIlhfdHJhaW46IiwgWF90cmFpbiwgIlhfdGVzdDoiLCBYX3Rlc3Qp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import LeaveOneOut

# Example dataset
data = np.array([1, 2, 3, 4, 5])

# Leave-One-Out cross-validation
loo = LeaveOneOut()
for train_index, test_index in loo.split(data):
    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)
    X_train, X_test = data[train_index], data[test_index]
    print(&quot;X_train:&quot;, X_train, &quot;X_test:&quot;, X_test)</pre></div><div class='content'></div><h1><ol start="3">
<li>Implementing Cross-Validation in Python</li>
</ol>
</h1>
<div class='content'></div><h2><p>Example: Using K-Fold Cross-Validation with Scikit-Learn</p>
</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2lyaXMKZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgY3Jvc3NfdmFsX3Njb3JlLCBLRm9sZApmcm9tIHNrbGVhcm4ubGluZWFyX21vZGVsIGltcG9ydCBMb2dpc3RpY1JlZ3Jlc3Npb24KCiMgTG9hZCBkYXRhc2V0CmlyaXMgPSBsb2FkX2lyaXMoKQpYLCB5ID0gaXJpcy5kYXRhLCBpcmlzLnRhcmdldAoKIyBEZWZpbmUgbW9kZWwKbW9kZWwgPSBMb2dpc3RpY1JlZ3Jlc3Npb24obWF4X2l0ZXI9MjAwKQoKIyBLRm9sZCBjcm9zcy12YWxpZGF0aW9uCmtmID0gS0ZvbGQobl9zcGxpdHM9NSwgc2h1ZmZsZT1UcnVlLCByYW5kb21fc3RhdGU9NDIpCnNjb3JlcyA9IGNyb3NzX3ZhbF9zY29yZShtb2RlbCwgWCwgeSwgY3Y9a2YpCgpwcmludCgiQ3Jvc3MtVmFsaWRhdGlvbiBTY29yZXM6Iiwgc2NvcmVzKQpwcmludCgiTWVhbiBBY2N1cmFjeToiLCBzY29yZXMubWVhbigpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_iris
from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LogisticRegression

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Define model
model = LogisticRegression(max_iter=200)

# KFold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf)

print(&quot;Cross-Validation Scores:&quot;, scores)
print(&quot;Mean Accuracy:&quot;, scores.mean())</pre></div><div class='content'></div><h2><p>Explanation:</p>
</h2>
<div class='content'><ul>
<li><strong>load_iris()</strong>: Loads the Iris dataset.</li>
<li><strong>LogisticRegression()</strong>: Defines the logistic regression model.</li>
<li><strong>KFold()</strong>: Creates a K-Fold cross-validator.</li>
<li><strong>cross_val_score()</strong>: Evaluates the model using cross-validation.</li>
</ul>
</div><h1><ol start="4">
<li>Practical Exercises</li>
</ol>
</h1>
<div class='content'></div><h2><p>Exercise 1: Implement K-Fold Cross-Validation</p>
</h2>
<div class='content'><p><strong>Task</strong>: Implement K-Fold Cross-Validation on the <code>digits</code> dataset using a <code>DecisionTreeClassifier</code>.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2RpZ2l0cwpmcm9tIHNrbGVhcm4ubW9kZWxfc2VsZWN0aW9uIGltcG9ydCBjcm9zc192YWxfc2NvcmUsIEtGb2xkCmZyb20gc2tsZWFybi50cmVlIGltcG9ydCBEZWNpc2lvblRyZWVDbGFzc2lmaWVyCgojIExvYWQgZGF0YXNldApkaWdpdHMgPSBsb2FkX2RpZ2l0cygpClgsIHkgPSBkaWdpdHMuZGF0YSwgZGlnaXRzLnRhcmdldAoKIyBEZWZpbmUgbW9kZWwKbW9kZWwgPSBEZWNpc2lvblRyZWVDbGFzc2lmaWVyKCkKCiMgS0ZvbGQgY3Jvc3MtdmFsaWRhdGlvbgprZiA9IEtGb2xkKG5fc3BsaXRzPTEwLCBzaHVmZmxlPVRydWUsIHJhbmRvbV9zdGF0ZT00MikKc2NvcmVzID0gY3Jvc3NfdmFsX3Njb3JlKG1vZGVsLCBYLCB5LCBjdj1rZikKCnByaW50KCJDcm9zcy1WYWxpZGF0aW9uIFNjb3JlczoiLCBzY29yZXMpCnByaW50KCJNZWFuIEFjY3VyYWN5OiIsIHNjb3Jlcy5tZWFuKCkp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_digits
from sklearn.model_selection import cross_val_score, KFold
from sklearn.tree import DecisionTreeClassifier

# Load dataset
digits = load_digits()
X, y = digits.data, digits.target

# Define model
model = DecisionTreeClassifier()

# KFold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf)

print(&quot;Cross-Validation Scores:&quot;, scores)
print(&quot;Mean Accuracy:&quot;, scores.mean())</pre></div><div class='content'></div><h2><p>Solution Explanation:</p>
</h2>
<div class='content'><ul>
<li><strong>load_digits()</strong>: Loads the Digits dataset.</li>
<li><strong>DecisionTreeClassifier()</strong>: Defines the decision tree classifier model.</li>
<li><strong>KFold()</strong>: Creates a K-Fold cross-validator with 10 splits.</li>
<li><strong>cross_val_score()</strong>: Evaluates the model using cross-validation.</li>
</ul>
</div><h2><p>Exercise 2: Implement Stratified K-Fold Cross-Validation</p>
</h2>
<div class='content'><p><strong>Task</strong>: Implement Stratified K-Fold Cross-Validation on the <code>breast_cancer</code> dataset using a <code>RandomForestClassifier</code>.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRhdGFzZXRzIGltcG9ydCBsb2FkX2JyZWFzdF9jYW5jZXIKZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgU3RyYXRpZmllZEtGb2xkLCBjcm9zc192YWxfc2NvcmUKZnJvbSBza2xlYXJuLmVuc2VtYmxlIGltcG9ydCBSYW5kb21Gb3Jlc3RDbGFzc2lmaWVyCgojIExvYWQgZGF0YXNldApicmVhc3RfY2FuY2VyID0gbG9hZF9icmVhc3RfY2FuY2VyKCkKWCwgeSA9IGJyZWFzdF9jYW5jZXIuZGF0YSwgYnJlYXN0X2NhbmNlci50YXJnZXQKCiMgRGVmaW5lIG1vZGVsCm1vZGVsID0gUmFuZG9tRm9yZXN0Q2xhc3NpZmllcigpCgojIFN0cmF0aWZpZWRLRm9sZCBjcm9zcy12YWxpZGF0aW9uCnNrZiA9IFN0cmF0aWZpZWRLRm9sZChuX3NwbGl0cz01LCBzaHVmZmxlPVRydWUsIHJhbmRvbV9zdGF0ZT00MikKc2NvcmVzID0gY3Jvc3NfdmFsX3Njb3JlKG1vZGVsLCBYLCB5LCBjdj1za2YpCgpwcmludCgiQ3Jvc3MtVmFsaWRhdGlvbiBTY29yZXM6Iiwgc2NvcmVzKQpwcmludCgiTWVhbiBBY2N1cmFjeToiLCBzY29yZXMubWVhbigpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Load dataset
breast_cancer = load_breast_cancer()
X, y = breast_cancer.data, breast_cancer.target

# Define model
model = RandomForestClassifier()

# StratifiedKFold cross-validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf)

print(&quot;Cross-Validation Scores:&quot;, scores)
print(&quot;Mean Accuracy:&quot;, scores.mean())</pre></div><div class='content'></div><h2><p>Solution Explanation:</p>
</h2>
<div class='content'><ul>
<li><strong>load_breast_cancer()</strong>: Loads the Breast Cancer dataset.</li>
<li><strong>RandomForestClassifier()</strong>: Defines the random forest classifier model.</li>
<li><strong>StratifiedKFold()</strong>: Creates a Stratified K-Fold cross-validator with 5 splits.</li>
<li><strong>cross_val_score()</strong>: Evaluates the model using cross-validation.</li>
</ul>
</div><h1><p>Conclusion</p>
</h1>
<div class='content'><p>Cross-validation is a crucial technique in machine learning for assessing model performance and ensuring that the model generalizes well to unseen data. By understanding and implementing various cross-validation methods, you can improve the robustness and reliability of your machine learning models. In the next section, we will delve into evaluation metrics to further enhance our understanding of model performance.</p>
</div><div class='row navigation'>
	<div class='col-2'>
					<a href='06-01-evaluation-metrics' title="Evaluation Metrics">&#x25C4;Previous</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end'>
					<a href='06-03-roc-curve-auc' title="ROC Curve and AUC">Next &#x25BA;</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
