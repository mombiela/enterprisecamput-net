<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross-Validation</title>

    <link rel="alternate" href="https://campusempresa.com/mod/machine_learning/validacion-cruzada" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/machine_learning/validacion-cruzada" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/machine_learning/validacion-cruzada" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body  class="test" >
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/mod/machine_learning/validacion-cruzada" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/mod/machine_learning/validacion-cruzada" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content">
									<div class="assert">
						<p><b>Attention!</b> There has been an error in the course generation, and it may contain translation errors.We are working to resolve this issue, so please use the content with caution.You can check the correct content in another language at the following link:<br>
						<a href="https://campusempresa.com/mod/machine_learning/validacion-cruzada">https://campusempresa.com/mod/machine_learning/validacion-cruzada</a></p>
					</div>
								<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Cross-validation is a fundamental technique in Machine Learning to evaluate a model's generalization capability. Instead of relying on a single data partition for training and testing, cross-validation allows the use of multiple partitions to obtain a more robust estimate of the model's performance.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ul>
<li><strong>Overfitting:</strong> Occurs when a model fits too closely to the training data, capturing noise and irrelevant patterns, resulting in poor performance on unseen data.</li>
<li><strong>Underfitting:</strong> Occurs when a model is too simple to capture the underlying structure of the data, resulting in poor performance on both training and test data.</li>
<li><strong>Data Partitioning:</strong> Dividing the dataset into training and test subsets to evaluate the model's performance.</li>
<li><strong>K-Fold Cross-Validation:</strong> A cross-validation technique where the data is divided into 'k' subsets (folds) and the model is trained and evaluated 'k' times, each time using a different fold as the test set and the remaining folds as the training set.</li>
</ul>
</div><h1>Types of Cross-Validation</h1>
<div class='content'></div><h2>K-Fold Cross-Validation</h2>
<div class='content'><p>In K-Fold Cross-Validation, the dataset is divided into 'k' equal parts. The model is trained 'k' times, each time using a different fold as the test set and the remaining folds as the training set.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgS0ZvbGQKZnJvbSBza2xlYXJuLmxpbmVhcl9tb2RlbCBpbXBvcnQgTGluZWFyUmVncmVzc2lvbgpmcm9tIHNrbGVhcm4ubWV0cmljcyBpbXBvcnQgbWVhbl9zcXVhcmVkX2Vycm9yCmltcG9ydCBudW1weSBhcyBucAoKIyBFeGFtcGxlIGRhdGEKWCA9IG5wLmFycmF5KFtbMSwgMl0sIFszLCA0XSwgWzUsIDZdLCBbNywgOF0sIFs5LCAxMF1dKQp5ID0gbnAuYXJyYXkoWzEsIDIsIDMsIDQsIDVdKQoKIyBLLUZvbGQgY29uZmlndXJhdGlvbgprZiA9IEtGb2xkKG5fc3BsaXRzPTUpCgojIE1vZGVsIGluaXRpYWxpemF0aW9uCm1vZGVsID0gTGluZWFyUmVncmVzc2lvbigpCgojIExpc3QgdG8gc3RvcmUgZXJyb3JzCmVycm9ycyA9IFtdCgojIENyb3NzLXZhbGlkYXRpb24KZm9yIHRyYWluX2luZGV4LCB0ZXN0X2luZGV4IGluIGtmLnNwbGl0KFgpOgogICAgWF90cmFpbiwgWF90ZXN0ID0gWFt0cmFpbl9pbmRleF0sIFhbdGVzdF9pbmRleF0KICAgIHlfdHJhaW4sIHlfdGVzdCA9IHlbdHJhaW5faW5kZXhdLCB5W3Rlc3RfaW5kZXhdCiAgICAKICAgIG1vZGVsLmZpdChYX3RyYWluLCB5X3RyYWluKQogICAgcHJlZGljdGlvbnMgPSBtb2RlbC5wcmVkaWN0KFhfdGVzdCkKICAgIGVycm9yID0gbWVhbl9zcXVhcmVkX2Vycm9yKHlfdGVzdCwgcHJlZGljdGlvbnMpCiAgICBlcnJvcnMuYXBwZW5kKGVycm9yKQoKcHJpbnQoIkVycm9ycyBpbiBlYWNoIGZvbGQ6IiwgZXJyb3JzKQpwcmludCgiQXZlcmFnZSBlcnJvcjoiLCBucC5tZWFuKGVycm9ycykp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Example data
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
y = np.array([1, 2, 3, 4, 5])

# K-Fold configuration
kf = KFold(n_splits=5)

# Model initialization
model = LinearRegression()

# List to store errors
errors = []

# Cross-validation
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    error = mean_squared_error(y_test, predictions)
    errors.append(error)

print(&quot;Errors in each fold:&quot;, errors)
print(&quot;Average error:&quot;, np.mean(errors))</pre></div><div class='content'></div><h2>Leave-One-Out Cross-Validation (LOOCV)</h2>
<div class='content'><p>In LOOCV, each observation in the dataset is used once as the test set, while the rest are used as the training set. This results in 'n' iterations, where 'n' is the number of observations.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgTGVhdmVPbmVPdXQKCiMgTE9PQ1YgY29uZmlndXJhdGlvbgpsb28gPSBMZWF2ZU9uZU91dCgpCgojIExpc3QgdG8gc3RvcmUgZXJyb3JzCmVycm9ycyA9IFtdCgojIENyb3NzLXZhbGlkYXRpb24KZm9yIHRyYWluX2luZGV4LCB0ZXN0X2luZGV4IGluIGxvby5zcGxpdChYKToKICAgIFhfdHJhaW4sIFhfdGVzdCA9IFhbdHJhaW5faW5kZXhdLCBYW3Rlc3RfaW5kZXhdCiAgICB5X3RyYWluLCB5X3Rlc3QgPSB5W3RyYWluX2luZGV4XSwgeVt0ZXN0X2luZGV4XQogICAgCiAgICBtb2RlbC5maXQoWF90cmFpbiwgeV90cmFpbikKICAgIHByZWRpY3Rpb25zID0gbW9kZWwucHJlZGljdChYX3Rlc3QpCiAgICBlcnJvciA9IG1lYW5fc3F1YXJlZF9lcnJvcih5X3Rlc3QsIHByZWRpY3Rpb25zKQogICAgZXJyb3JzLmFwcGVuZChlcnJvcikKCnByaW50KCJFcnJvcnMgaW4gZWFjaCBpdGVyYXRpb246IiwgZXJyb3JzKQpwcmludCgiQXZlcmFnZSBlcnJvcjoiLCBucC5tZWFuKGVycm9ycykp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import LeaveOneOut

# LOOCV configuration
loo = LeaveOneOut()

# List to store errors
errors = []

# Cross-validation
for train_index, test_index in loo.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    error = mean_squared_error(y_test, predictions)
    errors.append(error)

print(&quot;Errors in each iteration:&quot;, errors)
print(&quot;Average error:&quot;, np.mean(errors))</pre></div><div class='content'></div><h2>Stratified K-Fold Cross-Validation</h2>
<div class='content'><p>In classification problems, it is important that each fold has a similar proportion of classes. Stratified K-Fold ensures that each fold is representative of the class distribution in the entire dataset.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1vZGVsX3NlbGVjdGlvbiBpbXBvcnQgU3RyYXRpZmllZEtGb2xkCgojIEV4YW1wbGUgZGF0YSBmb3IgY2xhc3NpZmljYXRpb24KWCA9IG5wLmFycmF5KFtbMSwgMl0sIFszLCA0XSwgWzUsIDZdLCBbNywgOF0sIFs5LCAxMF1dKQp5ID0gbnAuYXJyYXkoWzAsIDEsIDAsIDEsIDBdKQoKIyBTdHJhdGlmaWVkIEstRm9sZCBjb25maWd1cmF0aW9uCnNrZiA9IFN0cmF0aWZpZWRLRm9sZChuX3NwbGl0cz0zKQoKIyBMaXN0IHRvIHN0b3JlIGVycm9ycwplcnJvcnMgPSBbXQoKIyBDcm9zcy12YWxpZGF0aW9uCmZvciB0cmFpbl9pbmRleCwgdGVzdF9pbmRleCBpbiBza2Yuc3BsaXQoWCwgeSk6CiAgICBYX3RyYWluLCBYX3Rlc3QgPSBYW3RyYWluX2luZGV4XSwgWFt0ZXN0X2luZGV4XQogICAgeV90cmFpbiwgeV90ZXN0ID0geVt0cmFpbl9pbmRleF0sIHlbdGVzdF9pbmRleF0KICAgIAogICAgbW9kZWwuZml0KFhfdHJhaW4sIHlfdHJhaW4pCiAgICBwcmVkaWN0aW9ucyA9IG1vZGVsLnByZWRpY3QoWF90ZXN0KQogICAgZXJyb3IgPSBtZWFuX3NxdWFyZWRfZXJyb3IoeV90ZXN0LCBwcmVkaWN0aW9ucykKICAgIGVycm9ycy5hcHBlbmQoZXJyb3IpCgpwcmludCgiRXJyb3JzIGluIGVhY2ggZm9sZDoiLCBlcnJvcnMpCnByaW50KCJBdmVyYWdlIGVycm9yOiIsIG5wLm1lYW4oZXJyb3JzKSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.model_selection import StratifiedKFold

# Example data for classification
X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])
y = np.array([0, 1, 0, 1, 0])

# Stratified K-Fold configuration
skf = StratifiedKFold(n_splits=3)

# List to store errors
errors = []

# Cross-validation
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    error = mean_squared_error(y_test, predictions)
    errors.append(error)

print(&quot;Errors in each fold:&quot;, errors)
print(&quot;Average error:&quot;, np.mean(errors))</pre></div><div class='content'></div><h1>Comparison of Methods</h1>
<div class='content'><table>
<thead>
<tr>
<th>Method</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td>K-Fold Cross-Validation</td>
<td>- Balance between bias and variance<br>- Lower computational cost than LOOCV</td>
<td>- Can be less precise than LOOCV on small datasets</td>
</tr>
<tr>
<td>LOOCV</td>
<td>- Uses all available information<br>- Lower bias</td>
<td>- High computational cost<br>- Higher variance in error estimation</td>
</tr>
<tr>
<td>Stratified K-Fold</td>
<td>- Better representation of class distribution</td>
<td>- Similar computational cost to K-Fold</td>
</tr>
</tbody>
</table>
</div><h1>Conclusion</h1>
<div class='content'><p>Cross-validation is an essential tool for evaluating the performance of Machine Learning models. By using techniques such as K-Fold, LOOCV, and Stratified K-Fold, professionals can obtain more accurate and robust estimates of model performance, helping to prevent overfitting and underfitting issues. The choice of cross-validation method will depend on the dataset size and the specific problem being addressed.</p>
</div>
			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
