<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dimensionality Reduction</title>

    <link rel="alternate" href="https://campusempresa.com/machine_learning/reduccion-dimensionalidad" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/machine_learning/reduccion-dimensionalidad" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/machine_learning/reduccion-dimensionalidad" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/machine_learning/reduccion-dimensionalidad" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/machine_learning/reduccion-dimensionalidad" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Dimensionality reduction is a crucial technique in Machine Learning used to simplify models, improve computational efficiency, and reduce noise in the data. This process involves transforming high-dimensional data into a lower-dimensional space while retaining as much relevant information as possible.</p>
</div><h1>Importance of Dimensionality Reduction</h1>
<div class='content'><ul>
<li><strong>Improved model performance</strong>: By reducing the number of features, overfitting can be avoided and model generalization can be improved.</li>
<li><strong>Computational efficiency</strong>: Fewer features mean fewer calculations, which speeds up training and prediction.</li>
<li><strong>Visualization</strong>: Facilitates the visualization of complex data in 2D or 3D.</li>
<li><strong>Noise elimination</strong>: Helps eliminate irrelevant or redundant features that can introduce noise into the model.</li>
</ul>
</div><h1>Dimensionality Reduction Techniques</h1>
<div class='content'></div><h2>Principal Component Analysis (PCA)</h2>
<div class='content'><p>PCA is a statistical technique that transforms the data into a new coordinate system, where the new variables (principal components) are linear combinations of the original variables and are ordered by the amount of variance they explain.</p>
<h4>Steps to perform PCA:</h4>
<ol>
<li><strong>Standardize the data</strong>: Normalize the data so that each feature has a mean of 0 and a standard deviation of 1.</li>
<li><strong>Calculate the covariance matrix</strong>: Determine the relationship between the features.</li>
<li><strong>Calculate eigenvalues and eigenvectors</strong>: Identify the main directions of variance.</li>
<li><strong>Select principal components</strong>: Choose the components that explain most of the variance.</li>
<li><strong>Transform the data</strong>: Project the original data into the new space of principal components.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmZyb20gc2tsZWFybi5kZWNvbXBvc2l0aW9uIGltcG9ydCBQQ0EKZnJvbSBza2xlYXJuLnByZXByb2Nlc3NpbmcgaW1wb3J0IFN0YW5kYXJkU2NhbGVyCgojIEV4YW1wbGUgZGF0YQpYID0gbnAuYXJyYXkoW1syLjUsIDIuNF0sIFswLjUsIDAuN10sIFsyLjIsIDIuOV0sIFsxLjksIDIuMl0sIFszLjEsIDMuMF0sIFsyLjMsIDIuN10sIFsyLCAxLjZdLCBbMSwgMS4xXSwgWzEuNSwgMS42XSwgWzEuMSwgMC45XV0pCgojIERhdGEgc3RhbmRhcmRpemF0aW9uCnNjYWxlciA9IFN0YW5kYXJkU2NhbGVyKCkKWF9zY2FsZWQgPSBzY2FsZXIuZml0X3RyYW5zZm9ybShYKQoKIyBBcHBseWluZyBQQ0EKcGNhID0gUENBKG5fY29tcG9uZW50cz0yKQpYX3BjYSA9IHBjYS5maXRfdHJhbnNmb3JtKFhfc2NhbGVkKQoKcHJpbnQoIlByaW5jaXBhbCBjb21wb25lbnRzOlxuIiwgWF9wY2Ep"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Example data
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Data standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Applying PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print(&quot;Principal components:\n&quot;, X_pca)</pre></div><div class='content'></div><h2>Linear Discriminant Analysis (LDA)</h2>
<div class='content'><p>LDA is a supervised technique that seeks to maximize the separation between multiple classes. Unlike PCA, which does not consider class labels, LDA uses class information to find directions that maximize class separation.</p>
<h4>Steps to perform LDA:</h4>
<ol>
<li><strong>Calculate the mean of each class</strong>.</li>
<li><strong>Calculate the within-class and between-class scatter matrices</strong>.</li>
<li><strong>Calculate eigenvalues and eigenvectors</strong>.</li>
<li><strong>Select eigenvectors that maximize class separation</strong>.</li>
<li><strong>Transform the original data</strong>.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLmRpc2NyaW1pbmFudF9hbmFseXNpcyBpbXBvcnQgTGluZWFyRGlzY3JpbWluYW50QW5hbHlzaXMgYXMgTERBCgojIEV4YW1wbGUgZGF0YSBhbmQgbGFiZWxzClggPSBucC5hcnJheShbWzIuNSwgMi40XSwgWzAuNSwgMC43XSwgWzIuMiwgMi45XSwgWzEuOSwgMi4yXSwgWzMuMSwgMy4wXSwgWzIuMywgMi43XSwgWzIsIDEuNl0sIFsxLCAxLjFdLCBbMS41LCAxLjZdLCBbMS4xLCAwLjldXSkKeSA9IG5wLmFycmF5KFsxLCAwLCAxLCAxLCAxLCAxLCAwLCAwLCAwLCAwXSkKCiMgQXBwbHlpbmcgTERBCmxkYSA9IExEQShuX2NvbXBvbmVudHM9MSkKWF9sZGEgPSBsZGEuZml0X3RyYW5zZm9ybShYLCB5KQoKcHJpbnQoIkRpc2NyaW1pbmFudCBjb21wb25lbnRzOlxuIiwgWF9sZGEp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# Example data and labels
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])
y = np.array([1, 0, 1, 1, 1, 1, 0, 0, 0, 0])

# Applying LDA
lda = LDA(n_components=1)
X_lda = lda.fit_transform(X, y)

print(&quot;Discriminant components:\n&quot;, X_lda)</pre></div><div class='content'></div><h2>t-Distributed Stochastic Neighbor Embedding (t-SNE)</h2>
<div class='content'><p>t-SNE is a non-linear technique primarily used for visualizing high-dimensional data in 2D or 3D. It is especially useful for discovering patterns in complex data.</p>
<h4>Steps to perform t-SNE:</h4>
<ol>
<li><strong>Calculate the similarity probabilities between pairs of points in the high-dimensional space</strong>.</li>
<li><strong>Calculate the similarity probabilities in the low-dimensional space</strong>.</li>
<li><strong>Minimize the divergence between these two distributions using a gradient descent method</strong>.</li>
</ol>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZnJvbSBza2xlYXJuLm1hbmlmb2xkIGltcG9ydCBUU05FCgojIEV4YW1wbGUgZGF0YQpYID0gbnAuYXJyYXkoW1syLjUsIDIuNF0sIFswLjUsIDAuN10sIFsyLjIsIDIuOV0sIFsxLjksIDIuMl0sIFszLjEsIDMuMF0sIFsyLjMsIDIuN10sIFsyLCAxLjZdLCBbMSwgMS4xXSwgWzEuNSwgMS42XSwgWzEuMSwgMC45XV0pCgojIEFwcGx5aW5nIHQtU05FCnRzbmUgPSBUU05FKG5fY29tcG9uZW50cz0yLCByYW5kb21fc3RhdGU9MCkKWF90c25lID0gdHNuZS5maXRfdHJhbnNmb3JtKFgpCgpwcmludCgidC1TTkUgcmVzdWx0OlxuIiwgWF90c25lKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from sklearn.manifold import TSNE

# Example data
X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])

# Applying t-SNE
tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X)

print(&quot;t-SNE result:\n&quot;, X_tsne)</pre></div><div class='content'></div><h1>Comparison of Techniques</h1>
<div class='content'><p>| Technique | Type | Supervised | Main Purpose |
|-----------|------|------------|--------------|
| PCA       | Linear | No        | General dimensionality reduction |
| LDA       | Linear | Yes       | Maximize class separation |
| t-SNE     | Non-Linear | No    | Visualization of complex data |</p>
</div><h1>Conclusion</h1>
<div class='content'><p>Dimensionality reduction is a powerful tool in a data scientist's arsenal. By understanding and applying techniques such as PCA, LDA, and t-SNE, we can significantly improve the performance of our models, make our data more manageable, and discover hidden patterns. It is essential to choose the appropriate technique based on the specific problem and available data.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
