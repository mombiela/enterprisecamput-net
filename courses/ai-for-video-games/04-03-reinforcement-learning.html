<!DOCTYPE html>
<html lang="en">
<head>
    <title> Reinforcement Learning </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, nofollow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/mod/ia_videojuegos/04-03-aprendizaje-refuerzo" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/mod/ia_videojuegos/04-03-aprenentatge-reforc" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/mod/ia_videojuegos/04-03-reinforcement-learning" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.ea63f62b9e.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "en";
  		var CATEGORY = "foundations";
  		var MOD_NAME = "ia_videojuegos";
  		var TEMA_NAME = "4-3";
  		var TYPE = "mod";
  		var PATH = "mod/ia_videojuegos/04-03-reinforcement-learning";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.902a5a267d.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/mod/ia_videojuegos/04-03-aprendizaje-refuerzo" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/mod/ia_videojuegos/04-03-aprenentatge-reforc" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/my-courses" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>My courses</b></i>
</a>
<a href="/completed-courses" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Completed             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="ia_videojuegos">
		<a  href="#" class="text-secondary d-none" data-read-mod="ia_videojuegos" data-read-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Mark as read
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="ia_videojuegos" data-unread-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Mark as unread
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Course Content
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-neural-networks' title="Neural Networks in Video Games" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-neural-networks' title="Neural Networks in Video Games" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h2 style="text-decoration:underline">Reinforcement Learning</h2>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-implementation-agent' title="Implementation of a Learning Agent" class="py-2 px-3 btn btn-primary"
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-implementation-agent' title="Implementation of a Learning Agent" class="py-2 px-3 btn btn-primary" 
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'><p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize some notion of cumulative reward. This module will cover the fundamental concepts of RL, its applications in video games, and practical examples to help you understand how to implement RL in your projects.</p>
</div><h1>Key Concepts in Reinforcement Learning</h1>
<div class='content'><ol>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: The external system with which the agent interacts.</li>
<li><strong>State (s)</strong>: A representation of the current situation of the agent.</li>
<li><strong>Action (a)</strong>: A set of all possible moves the agent can make.</li>
<li><strong>Reward (r)</strong>: The feedback from the environment based on the action taken.</li>
<li><strong>Policy (Ï€)</strong>: A strategy that the agent employs to determine the next action based on the current state.</li>
<li><strong>Value Function (V)</strong>: A function that estimates the expected reward for a given state.</li>
<li><strong>Q-Function (Q)</strong>: A function that estimates the expected reward for a given state-action pair.</li>
</ol>
</div><h1>How Reinforcement Learning Works</h1>
<div class='content'><ol>
<li><strong>Initialization</strong>: The agent starts with an initial state.</li>
<li><strong>Action Selection</strong>: The agent selects an action based on its policy.</li>
<li><strong>Transition</strong>: The action causes a transition to a new state.</li>
<li><strong>Reward</strong>: The agent receives a reward from the environment.</li>
<li><strong>Update</strong>: The agent updates its policy based on the reward and the new state.</li>
</ol>
</div><h1>Types of Reinforcement Learning</h1>
<div class='content'><ol>
<li>
<p><strong>Model-Free RL</strong>: The agent learns directly from interactions with the environment without a model of the environment.</p>
<ul>
<li><strong>Q-Learning</strong>: A value-based method where the agent learns the value of actions in states.</li>
<li><strong>SARSA (State-Action-Reward-State-Action)</strong>: Similar to Q-Learning but updates the policy based on the action taken in the next state.</li>
</ul>
</li>
<li>
<p><strong>Model-Based RL</strong>: The agent uses a model of the environment to make decisions.</p>
<ul>
<li><strong>Dynamic Programming</strong>: Uses a model of the environment to compute the optimal policy.</li>
</ul>
</li>
</ol>
</div><h1>Q-Learning Algorithm</h1>
<div class='content'><p>Q-Learning is one of the most popular model-free RL algorithms. It aims to learn the value of the optimal action-selection policy.</p>
</div><h2>Q-Learning Formula</h2>
<div class='content'><p>\[ Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \]</p>
<p>Where:</p>
<ul>
<li>\( Q(s, a) \) is the current Q-value.</li>
<li>\( \alpha \) is the learning rate.</li>
<li>\( r \) is the reward received after taking action \( a \) from state \( s \).</li>
<li>\( \gamma \) is the discount factor.</li>
<li>\( \max_{a'} Q(s', a') \) is the maximum Q-value for the next state \( s' \).</li>
</ul>
</div><h2>Example: Implementing Q-Learning in Python</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCByYW5kb20KCiMgSW5pdGlhbGl6ZSBwYXJhbWV0ZXJzCmFscGhhID0gMC4xICAjIExlYXJuaW5nIHJhdGUKZ2FtbWEgPSAwLjYgICMgRGlzY291bnQgZmFjdG9yCmVwc2lsb24gPSAwLjEgICMgRXhwbG9yYXRpb24gZmFjdG9yCgojIERlZmluZSB0aGUgZW52aXJvbm1lbnQKc3RhdGVzID0gWyJBIiwgIkIiLCAiQyIsICJEIl0KYWN0aW9ucyA9IFsibGVmdCIsICJyaWdodCJdCnJld2FyZHMgPSB7CiAgICAoIkEiLCAibGVmdCIpOiAxLAogICAgKCJBIiwgInJpZ2h0Iik6IDAsCiAgICAoIkIiLCAibGVmdCIpOiAwLAogICAgKCJCIiwgInJpZ2h0Iik6IDEsCiAgICAoIkMiLCAibGVmdCIpOiAxLAogICAgKCJDIiwgInJpZ2h0Iik6IDAsCiAgICAoIkQiLCAibGVmdCIpOiAwLAogICAgKCJEIiwgInJpZ2h0Iik6IDEsCn0KCiMgSW5pdGlhbGl6ZSBRLXRhYmxlClEgPSB7fQpmb3Igc3RhdGUgaW4gc3RhdGVzOgogICAgZm9yIGFjdGlvbiBpbiBhY3Rpb25zOgogICAgICAgIFFbKHN0YXRlLCBhY3Rpb24pXSA9IDAKCiMgRGVmaW5lIHRoZSBwb2xpY3kKZGVmIGNob29zZV9hY3Rpb24oc3RhdGUpOgogICAgaWYgcmFuZG9tLnVuaWZvcm0oMCwgMSkgPCBlcHNpbG9uOgogICAgICAgIHJldHVybiByYW5kb20uY2hvaWNlKGFjdGlvbnMpCiAgICBlbHNlOgogICAgICAgIHJldHVybiBtYXgoYWN0aW9ucywga2V5PWxhbWJkYSBhY3Rpb246IFFbKHN0YXRlLCBhY3Rpb24pXSkKCiMgVHJhaW5pbmcgdGhlIGFnZW50CmZvciBlcGlzb2RlIGluIHJhbmdlKDEwMDApOgogICAgc3RhdGUgPSByYW5kb20uY2hvaWNlKHN0YXRlcykKICAgIHdoaWxlIHN0YXRlICE9ICJEIjogICMgVGVybWluYWwgc3RhdGUKICAgICAgICBhY3Rpb24gPSBjaG9vc2VfYWN0aW9uKHN0YXRlKQogICAgICAgIG5leHRfc3RhdGUgPSAiRCIgaWYgc3RhdGUgPT0gIkMiIGFuZCBhY3Rpb24gPT0gInJpZ2h0IiBlbHNlIHJhbmRvbS5jaG9pY2Uoc3RhdGVzKQogICAgICAgIHJld2FyZCA9IHJld2FyZHMuZ2V0KChzdGF0ZSwgYWN0aW9uKSwgMCkKICAgICAgICBvbGRfdmFsdWUgPSBRWyhzdGF0ZSwgYWN0aW9uKV0KICAgICAgICBuZXh0X21heCA9IG1heChRWyhuZXh0X3N0YXRlLCBhKV0gZm9yIGEgaW4gYWN0aW9ucykKICAgICAgICBRWyhzdGF0ZSwgYWN0aW9uKV0gPSBvbGRfdmFsdWUgKyBhbHBoYSAqIChyZXdhcmQgKyBnYW1tYSAqIG5leHRfbWF4IC0gb2xkX3ZhbHVlKQogICAgICAgIHN0YXRlID0gbmV4dF9zdGF0ZQoKIyBEaXNwbGF5IHRoZSBsZWFybmVkIFEtdmFsdWVzCmZvciBzdGF0ZSBpbiBzdGF0ZXM6CiAgICBmb3IgYWN0aW9uIGluIGFjdGlvbnM6CiAgICAgICAgcHJpbnQoZiJRKHtzdGF0ZX0sIHthY3Rpb259KSA9IHtRWyhzdGF0ZSwgYWN0aW9uKV06LjJmfSIp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import random

# Initialize parameters
alpha = 0.1  # Learning rate
gamma = 0.6  # Discount factor
epsilon = 0.1  # Exploration factor

# Define the environment
states = [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]
actions = [&quot;left&quot;, &quot;right&quot;]
rewards = {
    (&quot;A&quot;, &quot;left&quot;): 1,
    (&quot;A&quot;, &quot;right&quot;): 0,
    (&quot;B&quot;, &quot;left&quot;): 0,
    (&quot;B&quot;, &quot;right&quot;): 1,
    (&quot;C&quot;, &quot;left&quot;): 1,
    (&quot;C&quot;, &quot;right&quot;): 0,
    (&quot;D&quot;, &quot;left&quot;): 0,
    (&quot;D&quot;, &quot;right&quot;): 1,
}

# Initialize Q-table
Q = {}
for state in states:
    for action in actions:
        Q[(state, action)] = 0

# Define the policy
def choose_action(state):
    if random.uniform(0, 1) &lt; epsilon:
        return random.choice(actions)
    else:
        return max(actions, key=lambda action: Q[(state, action)])

# Training the agent
for episode in range(1000):
    state = random.choice(states)
    while state != &quot;D&quot;:  # Terminal state
        action = choose_action(state)
        next_state = &quot;D&quot; if state == &quot;C&quot; and action == &quot;right&quot; else random.choice(states)
        reward = rewards.get((state, action), 0)
        old_value = Q[(state, action)]
        next_max = max(Q[(next_state, a)] for a in actions)
        Q[(state, action)] = old_value + alpha * (reward + gamma * next_max - old_value)
        state = next_state

# Display the learned Q-values
for state in states:
    for action in actions:
        print(f&quot;Q({state}, {action}) = {Q[(state, action)]:.2f}&quot;)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ol>
<li><strong>Initialization</strong>: We initialize the learning rate, discount factor, exploration factor, and Q-table.</li>
<li><strong>Policy</strong>: The agent chooses an action based on the epsilon-greedy policy.</li>
<li><strong>Training</strong>: The agent interacts with the environment, updates the Q-values based on the received rewards, and transitions to the next state.</li>
<li><strong>Output</strong>: The learned Q-values for each state-action pair are displayed.</li>
</ol>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Exercise: Implement Q-Learning for a Simple Grid World</h2>
<div class='content'><ol>
<li><strong>Environment</strong>: Create a 4x4 grid world where the agent starts at the top-left corner and the goal is at the bottom-right corner.</li>
<li><strong>Actions</strong>: The agent can move up, down, left, or right.</li>
<li><strong>Rewards</strong>: The agent receives a reward of +1 for reaching the goal and -1 for hitting a wall.</li>
</ol>
</div><h2>Solution</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Define the environment
grid_size = 4
actions = [&quot;up&quot;, &quot;down&quot;, &quot;left&quot;, &quot;right&quot;]
rewards = np.zeros((grid_size, grid_size))
rewards[3, 3] = 1  # Goal state

# Initialize Q-table
Q = np.zeros((grid_size, grid_size, len(actions)))

# Define the policy
def choose_action(state, epsilon):
    if np.random.uniform(0, 1) &lt; epsilon:
        return np.random.choice(actions)
    else:
        return actions[np.argmax(Q[state[0], state[1], :])]

# Define the next state function
def next_state(state, action):
    if action == &quot;up&quot;:
        return (max(state[0] - 1, 0), state[1])
    elif action == &quot;down&quot;:
        return (min(state[0] + 1, grid_size - 1), state[1])
    elif action == &quot;left&quot;:
        return (state[0], max(state[1] - 1, 0))
    elif action == &quot;right&quot;:
        return (state[0], min(state[1] + 1, grid_size - 1))

# Training the agent
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.1  # Exploration factor

for episode in range(1000):
    state = (0, 0)  # Start state
    while state != (3, 3):  # Goal state
        action = choose_action(state, epsilon)
        next_s = next_state(state, action)
        reward = rewards[next_s[0], next_s[1]]
        old_value = Q[state[0], state[1], actions.index(action)]
        next_max = np.max(Q[next_s[0], next_s[1], :])
        Q[state[0], state[1], actions.index(action)] = old_value + alpha * (reward + gamma * next_max - old_value)
        state = next_s

# Display the learned Q-values
for i in range(grid_size):
    for j in range(grid_size):
        print(f&quot;Q({i}, {j}) = {Q[i, j, :]}&quot;)

# Display the optimal policy
policy = np.zeros((grid_size, grid_size), dtype=str)
for i in range(grid_size):
    for j in range(grid_size):
        policy[i, j] = actions[np.argmax(Q[i, j, :])]
print(&quot;Optimal Policy:&quot;)
print(policy)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ol>
<li><strong>Environment</strong>: A 4x4 grid world is defined with rewards and actions.</li>
<li><strong>Q-Table</strong>: The Q-table is initialized to zeros.</li>
<li><strong>Policy</strong>: The agent chooses an action based on the epsilon-greedy policy.</li>
<li><strong>Training</strong>: The agent interacts with the environment, updates the Q-values, and transitions to the next state.</li>
<li><strong>Output</strong>: The learned Q-values and the optimal policy are displayed.</li>
</ol>
</div><h1>Summary</h1>
<div class='content'><p>In this section, we covered the basics of Reinforcement Learning, focusing on key concepts, types of RL, and the Q-Learning algorithm. We provided a detailed example of implementing Q-Learning in Python and a practical exercise to reinforce the learned concepts. Reinforcement Learning is a powerful tool for developing intelligent behaviors in game characters, and mastering it will significantly enhance your game development skills.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-neural-networks' title="Neural Networks in Video Games" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-neural-networks' title="Neural Networks in Video Games" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-implementation-agent' title="Implementation of a Learning Agent" class="py-2 px-3 btn btn-primary"
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-implementation-agent' title="Implementation of a Learning Agent" class="py-2 px-3 btn btn-primary" 
				data-read-mod="ia_videojuegos" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>AI for Video Games</h1>
<h2>Module 1: Introduction to AI in Video Games</h2>
<ul>
<li><a href="01-01-history-evolution">History and Evolution of AI in Video Games</a></li>
<li><a href="01-02-basic-concepts">Basic AI Concepts</a></li>
<li><a href="01-03-tools-languages">Tools and Programming Languages</a></li>
</ul>
<h2>Module 2: Navigation in Video Games</h2>
<ul>
<li><a href="02-01-pathfinding-algorithms">Pathfinding Algorithms</a></li>
<li><a href="02-02-a-star-implementation">A* Implementation</a></li>
<li><a href="02-03-navigation-navmesh">Navigation with NavMesh</a></li>
<li><a href="02-04-obstacle-avoidance">Obstacle Avoidance</a></li>
</ul>
<h2>Module 3: Decision Making</h2>
<ul>
<li><a href="03-01-finite-state-machines">Finite State Machines (FSM)</a></li>
<li><a href="03-02-decision-trees">Decision Trees</a></li>
<li><a href="03-03-behavior-trees">Behavior Trees</a></li>
<li><a href="03-04-rule-based-systems">Rule-Based Systems</a></li>
</ul>
<h2>Module 4: Machine Learning</h2>
<ul>
<li><a href="04-01-introduction-machine-learning">Introduction to Machine Learning</a></li>
<li><a href="04-02-neural-networks">Neural Networks in Video Games</a></li>
<li><a href="04-03-reinforcement-learning">Reinforcement Learning</a></li>
<li><a href="04-04-implementation-agent">Implementation of a Learning Agent</a></li>
</ul>
<h2>Module 5: Integration and Optimization</h2>
<ul>
<li><a href="05-01-integration-engines">Integration of AI in Game Engines</a></li>
<li><a href="05-02-optimization-algorithms">Optimization of AI Algorithms</a></li>
<li><a href="05-03-testing-debugging">AI Testing and Debugging</a></li>
</ul>
<h2>Module 6: Practical Projects</h2>
<ul>
<li><a href="06-01-project-navigation">Project 1: Implementation of Basic Navigation</a></li>
<li><a href="06-02-project-npc">Project 2: Creation of an NPC with Decision Making</a></li>
<li><a href="06-03-project-agent">Project 3: Development of an Agent with Machine Learning</a></li>
</ul>
<h2>Module 7: Additional Resources</h2>
<ul>
<li><a href="07-01-books-articles">Recommended Books and Articles</a></li>
<li><a href="07-02-communities-forums">Communities and Forums</a></li>
<li><a href="07-03-tools-libraries">Useful Tools and Libraries</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">User not authenticated</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
