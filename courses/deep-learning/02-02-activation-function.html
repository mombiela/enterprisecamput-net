<!DOCTYPE html>
<html lang="en">
<head>
    <title> Activation Function </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, nofollow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/cursos/deep-learning/02-02-funcion-de-activacion" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/cursos/deep-learning/02-02-funcion-de-activacion" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/courses/deep-learning/02-02-activation-function" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.188a386f47.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "en";
  		var CATEGORY = "foundations";
  		var MOD_NAME = "deep_learning";
  		var TEMA_NAME = "2-2";
  		var TYPE = "mod";
  		var PATH = "mod/deep_learning/02-02-activation-function";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.902a5a267d.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-VVPMPJSR3P"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());	
	  gtag('config', 'G-VVPMPJSR3P');
	</script>
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<a href="/"><img src="/img/logo-header_enterprise.png" alt="Logo Enterprise Campus"></a>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/cursos/deep-learning/02-02-funcion-de-activacion" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/cursos/deep-learning/02-02-funcion-de-activacion" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/my-courses" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>My courses</b></i>
</a>
<a href="/completed-courses" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Completed             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="deep_learning">
		<a  href="#" class="text-secondary d-none" data-read-mod="deep_learning" data-read-unit="2-2" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Mark as read
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="deep_learning" data-unread-unit="2-2" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Mark as unread
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Course Content
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h1 style="text-decoration:underline">Activation Function</h1>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation" class="py-2 px-3 btn btn-primary"
				data-read-mod="deep_learning" data-read-unit="2-2">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation" class="py-2 px-3 btn btn-primary" 
				data-read-mod="deep_learning" data-read-unit="2-2">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'><p>Activation functions play a crucial role in the functioning of neural networks. They introduce non-linearity into the network, enabling it to learn complex patterns and relationships in the data. In this section, we will explore different types of activation functions, their properties, and their applications.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ol>
<li><strong>Definition</strong>: An activation function is a mathematical function applied to the output of a neuron. It determines whether a neuron should be activated or not, based on the weighted sum of its inputs.</li>
<li><strong>Purpose</strong>: The primary purpose of an activation function is to introduce non-linearity into the neural network, allowing it to learn and model complex data.</li>
<li><strong>Types of Activation Functions</strong>: There are several types of activation functions, each with its own advantages and disadvantages.</li>
</ol>
</div><h2>Common Activation Functions</h2>
<div class='content'></div><h3><ol>
<li>Sigmoid Function</li>
</ol></h3>
<div class='content'><p>The sigmoid function maps any input value to a value between 0 and 1.</p>
<p><strong>Formula</strong>:
\[ \sigma(x) = \frac{1}{1 + e^{-x}} \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (0, 1)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>: \(\sigma'(x) = \sigma(x) \cdot (1 - \sigma(x))\)</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCmltcG9ydCBtYXRwbG90bGliLnB5cGxvdCBhcyBwbHQKCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgp4ID0gbnAubGluc3BhY2UoLTEwLCAxMCwgMTAwKQp5ID0gc2lnbW9pZCh4KQoKcGx0LnBsb3QoeCwgeSkKcGx0LnRpdGxlKCdTaWdtb2lkIEZ1bmN0aW9uJykKcGx0LnhsYWJlbCgnSW5wdXQnKQpwbHQueWxhYmVsKCdPdXRwdXQnKQpwbHQuZ3JpZCgpCnBsdC5zaG93KCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.linspace(-10, 10, 100)
y = sigmoid(x)

plt.plot(x, y)
plt.title('Sigmoid Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h3><ol start="2">
<li>Hyperbolic Tangent (Tanh) Function</li>
</ol></h3>
<div class='content'><p>The tanh function maps any input value to a value between -1 and 1.</p>
<p><strong>Formula</strong>:
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (-1, 1)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>: \(\tanh'(x) = 1 - \tanh^2(x)\)</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKeSA9IHRhbmgoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnVGFuaCBGdW5jdGlvbicpCnBsdC54bGFiZWwoJ0lucHV0JykKcGx0LnlsYWJlbCgnT3V0cHV0JykKcGx0LmdyaWQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def tanh(x):
    return np.tanh(x)

x = np.linspace(-10, 10, 100)
y = tanh(x)

plt.plot(x, y)
plt.title('Tanh Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h3><ol start="3">
<li>Rectified Linear Unit (ReLU) Function</li>
</ol></h3>
<div class='content'><p>The ReLU function is one of the most popular activation functions in deep learning.</p>
<p><strong>Formula</strong>:
\[ \text{ReLU}(x) = \max(0, x) \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: [0, ∞)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>:
\[
\text{ReLU}'(x) =
\begin{cases}
1 &amp; \text{if } x &gt; 0 <br>  0 &amp; \text{if } x \leq 0
\end{cases}
\]</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKeCA9IG5wLmxpbnNwYWNlKC0xMCwgMTAsIDEwMCkKeSA9IHJlbHUoeCkKCnBsdC5wbG90KHgsIHkpCnBsdC50aXRsZSgnUmVMVSBGdW5jdGlvbicpCnBsdC54bGFiZWwoJ0lucHV0JykKcGx0LnlsYWJlbCgnT3V0cHV0JykKcGx0LmdyaWQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def relu(x):
    return np.maximum(0, x)

x = np.linspace(-10, 10, 100)
y = relu(x)

plt.plot(x, y)
plt.title('ReLU Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h3><ol start="4">
<li>Leaky ReLU Function</li>
</ol></h3>
<div class='content'><p>The Leaky ReLU function is a variation of the ReLU function that allows a small, non-zero gradient when the input is negative.</p>
<p><strong>Formula</strong>:
\[ \text{Leaky ReLU}(x) =
\begin{cases}
x &amp; \text{if } x &gt; 0 <br>\alpha x &amp; \text{if } x \leq 0
\end{cases}
\]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (-∞, ∞)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>:
\[
\text{Leaky ReLU}'(x) =
\begin{cases}
1 &amp; \text{if } x &gt; 0 <br>  \alpha &amp; \text{if } x \leq 0
\end{cases}
\]</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCnggPSBucC5saW5zcGFjZSgtMTAsIDEwLCAxMDApCnkgPSBsZWFreV9yZWx1KHgpCgpwbHQucGxvdCh4LCB5KQpwbHQudGl0bGUoJ0xlYWt5IFJlTFUgRnVuY3Rpb24nKQpwbHQueGxhYmVsKCdJbnB1dCcpCnBsdC55bGFiZWwoJ091dHB1dCcpCnBsdC5ncmlkKCkKcGx0LnNob3coKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

x = np.linspace(-10, 10, 100)
y = leaky_relu(x)

plt.plot(x, y)
plt.title('Leaky ReLU Function')
plt.xlabel('Input')
plt.ylabel('Output')
plt.grid()
plt.show()</pre></div><div class='content'></div><h3><ol start="5">
<li>Softmax Function</li>
</ol></h3>
<div class='content'><p>The softmax function is often used in the output layer of a neural network for classification tasks.</p>
<p><strong>Formula</strong>:
\[ \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Range</strong>: (0, 1)</li>
<li><strong>Non-linearity</strong>: Yes</li>
<li><strong>Derivative</strong>: Complex, but ensures the sum of outputs is 1</li>
</ul>
<p><strong>Example</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCnggPSBucC5hcnJheShbMS4wLCAyLjAsIDMuMF0pCnkgPSBzb2Z0bWF4KHgpCgpwcmludCgiU29mdG1heCBPdXRwdXQ6IiwgeSk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

x = np.array([1.0, 2.0, 3.0])
y = softmax(x)

print(&quot;Softmax Output:&quot;, y)</pre></div><div class='content'></div><h2>Practical Exercises</h2>
<div class='content'></div><h3>Exercise 1: Implementing Activation Functions</h3>
<div class='content'><p><strong>Task</strong>: Implement the sigmoid, tanh, ReLU, and softmax functions in Python.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKZGVmIHRhbmgoeCk6CiAgICByZXR1cm4gbnAudGFuaCh4KQoKZGVmIHJlbHUoeCk6CiAgICByZXR1cm4gbnAubWF4aW11bSgwLCB4KQoKZGVmIHNvZnRtYXgoeCk6CiAgICBlX3ggPSBucC5leHAoeCAtIG5wLm1heCh4KSkKICAgIHJldHVybiBlX3ggLyBlX3guc3VtKGF4aXM9MCkKCiMgVGVzdCB0aGUgZnVuY3Rpb25zCnggPSBucC5hcnJheShbLTEuMCwgMC4wLCAxLjBdKQpwcmludCgiU2lnbW9pZDoiLCBzaWdtb2lkKHgpKQpwcmludCgiVGFuaDoiLCB0YW5oKHgpKQpwcmludCgiUmVMVToiLCByZWx1KHgpKQpwcmludCgiU29mdG1heDoiLCBzb2Z0bWF4KHgpKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# Test the functions
x = np.array([-1.0, 0.0, 1.0])
print(&quot;Sigmoid:&quot;, sigmoid(x))
print(&quot;Tanh:&quot;, tanh(x))
print(&quot;ReLU:&quot;, relu(x))
print(&quot;Softmax:&quot;, softmax(x))</pre></div><div class='content'></div><h3>Exercise 2: Visualizing Activation Functions</h3>
<div class='content'><p><strong>Task</strong>: Plot the sigmoid, tanh, ReLU, and leaky ReLU functions using Matplotlib.</p>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG1hdHBsb3RsaWIucHlwbG90IGFzIHBsdAoKZGVmIGxlYWt5X3JlbHUoeCwgYWxwaGE9MC4wMSk6CiAgICByZXR1cm4gbnAud2hlcmUoeCA+IDAsIHgsIGFscGhhICogeCkKCnggPSBucC5saW5zcGFjZSgtMTAsIDEwLCAxMDApCgojIFBsb3QgU2lnbW9pZApwbHQucGxvdCh4LCBzaWdtb2lkKHgpLCBsYWJlbD0nU2lnbW9pZCcpCiMgUGxvdCBUYW5oCnBsdC5wbG90KHgsIHRhbmgoeCksIGxhYmVsPSdUYW5oJykKIyBQbG90IFJlTFUKcGx0LnBsb3QoeCwgcmVsdSh4KSwgbGFiZWw9J1JlTFUnKQojIFBsb3QgTGVha3kgUmVMVQpwbHQucGxvdCh4LCBsZWFreV9yZWx1KHgpLCBsYWJlbD0nTGVha3kgUmVMVScpCgpwbHQudGl0bGUoJ0FjdGl2YXRpb24gRnVuY3Rpb25zJykKcGx0LnhsYWJlbCgnSW5wdXQnKQpwbHQueWxhYmVsKCdPdXRwdXQnKQpwbHQubGVnZW5kKCkKcGx0LmdyaWQoKQpwbHQuc2hvdygp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import matplotlib.pyplot as plt

def leaky_relu(x, alpha=0.01):
    return np.where(x &gt; 0, x, alpha * x)

x = np.linspace(-10, 10, 100)

# Plot Sigmoid
plt.plot(x, sigmoid(x), label='Sigmoid')
# Plot Tanh
plt.plot(x, tanh(x), label='Tanh')
# Plot ReLU
plt.plot(x, relu(x), label='ReLU')
# Plot Leaky ReLU
plt.plot(x, leaky_relu(x), label='Leaky ReLU')

plt.title('Activation Functions')
plt.xlabel('Input')
plt.ylabel('Output')
plt.legend()
plt.grid()
plt.show()</pre></div><div class='content'></div><h2>Common Mistakes and Tips</h2>
<div class='content'><ul>
<li><strong>Vanishing Gradient Problem</strong>: Sigmoid and tanh functions can cause the vanishing gradient problem, where gradients become very small, slowing down the training process. ReLU and its variants are often preferred to mitigate this issue.</li>
<li><strong>Choosing the Right Activation Function</strong>: The choice of activation function can significantly impact the performance of your neural network. Experiment with different functions to find the best one for your specific problem.</li>
<li><strong>Softmax for Classification</strong>: Use the softmax function in the output layer for multi-class classification problems to ensure the outputs sum to 1.</li>
</ul>
</div><h2>Conclusion</h2>
<div class='content'><p>In this section, we explored various activation functions, their properties, and their applications. Understanding these functions is crucial for designing effective neural networks. In the next section, we will delve into forward and backward propagation, which are essential for training neural networks.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation" class="py-2 px-3 btn btn-primary"
				data-read-mod="deep_learning" data-read-unit="2-2">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='02-03-forward-backward-propagation' title="Forward and Backward Propagation" class="py-2 px-3 btn btn-primary" 
				data-read-mod="deep_learning" data-read-unit="2-2">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>Deep Learning Course</h1>
<h2>Module 1: Introduction to Deep Learning</h2>
<ul>
<li><a href="01-01-what-is-deep-learning">What is Deep Learning?</a></li>
<li><a href="01-02-history-evolution-deep-learning">History and Evolution of Deep Learning</a></li>
<li><a href="01-03-applications-deep-learning">Applications of Deep Learning</a></li>
<li><a href="01-04-basic-concepts-neural-networks">Basic Concepts of Neural Networks</a></li>
</ul>
<h2>Module 2: Fundamentals of Neural Networks</h2>
<ul>
<li><a href="02-01-perceptron-multilayer-perceptron">Perceptron and Multilayer Perceptron</a></li>
<li><a href="02-02-activation-function">Activation Function</a></li>
<li><a href="02-03-forward-backward-propagation">Forward and Backward Propagation</a></li>
<li><a href="02-04-optimization-loss-function">Optimization and Loss Function</a></li>
</ul>
<h2>Module 3: Convolutional Neural Networks (CNN)</h2>
<ul>
<li><a href="03-01-introduction-cnn">Introduction to CNN</a></li>
<li><a href="03-02-convolutional-pooling-layers">Convolutional and Pooling Layers</a></li>
<li><a href="03-03-popular-cnn-architectures">Popular CNN Architectures</a></li>
<li><a href="03-04-cnn-applications-image-recognition">CNN Applications in Image Recognition</a></li>
</ul>
<h2>Module 4: Recurrent Neural Networks (RNN)</h2>
<ul>
<li><a href="04-01-introduction-rnn">Introduction to RNN</a></li>
<li><a href="04-02-lstm-gru">LSTM and GRU</a></li>
<li><a href="04-03-rnn-applications-nlp">RNN Applications in Natural Language Processing</a></li>
<li><a href="04-04-sequences-time-series">Sequences and Time Series</a></li>
</ul>
<h2>Module 5: Advanced Techniques in Deep Learning</h2>
<ul>
<li><a href="05-01-generative-adversarial-networks">Generative Adversarial Networks (GAN)</a></li>
<li><a href="05-02-autoencoders">Autoencoders</a></li>
<li><a href="05-03-transfer-learning">Transfer Learning</a></li>
<li><a href="05-04-regularization-improvement-techniques">Regularization and Improvement Techniques</a></li>
</ul>
<h2>Module 6: Tools and Frameworks</h2>
<ul>
<li><a href="06-01-introduction-tensorflow">Introduction to TensorFlow</a></li>
<li><a href="06-02-introduction-pytorch">Introduction to PyTorch</a></li>
<li><a href="06-03-framework-comparison">Framework Comparison</a></li>
<li><a href="06-04-development-environments-resources">Development Environments and Additional Resources</a></li>
</ul>
<h2>Module 7: Practical Projects</h2>
<ul>
<li><a href="07-01-image-classification-cnn">Image Classification with CNN</a></li>
<li><a href="07-02-text-generation-rnn">Text Generation with RNN</a></li>
<li><a href="07-03-anomaly-detection-autoencoders">Anomaly Detection with Autoencoders</a></li>
<li><a href="07-04-creating-gan-image-generation">Creating a GAN for Image Generation</a></li>
</ul>
<h2>Module 8: Ethical Considerations and the Future of Deep Learning</h2>
<ul>
<li><a href="08-01-ethics-deep-learning">Ethics in Deep Learning</a></li>
<li><a href="08-02-social-economic-impact">Social and Economic Impact</a></li>
<li><a href="08-03-future-trends-deep-learning">Future Trends in Deep Learning</a></li>
<li><a href="08-04-challenges-opportunities">Challenges and Opportunities</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">User not authenticated</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
