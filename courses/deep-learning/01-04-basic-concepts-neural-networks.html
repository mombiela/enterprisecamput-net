<!DOCTYPE html>
<html lang="en">
<head>
    <title> Basic Concepts of Neural Networks </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, nofollow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/cursos/deep-learning/01-04-conceptos-basicos-redes-neuronales" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/cursos/deep-learning/01-04-conceptos-basicos-redes-neuronales" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/courses/deep-learning/01-04-basic-concepts-neural-networks" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.ea63f62b9e.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "en";
  		var CATEGORY = "foundations";
  		var MOD_NAME = "deep_learning";
  		var TEMA_NAME = "1-4";
  		var TYPE = "mod";
  		var PATH = "mod/deep_learning/01-04-basic-concepts-neural-networks";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.902a5a267d.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/cursos/deep-learning/01-04-conceptos-basicos-redes-neuronales" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/cursos/deep-learning/01-04-conceptos-basicos-redes-neuronales" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/my-courses" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>My courses</b></i>
</a>
<a href="/completed-courses" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Completed             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="deep_learning">
		<a  href="#" class="text-secondary d-none" data-read-mod="deep_learning" data-read-unit="1-4" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Mark as read
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="deep_learning" data-unread-unit="1-4" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Mark as unread
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Course Content
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='01-03-applications-deep-learning' title="Applications of Deep Learning" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='01-03-applications-deep-learning' title="Applications of Deep Learning" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h2 style="text-decoration:underline">Basic Concepts of Neural Networks</h2>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary"
				data-read-mod="deep_learning" data-read-unit="1-4">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary" 
				data-read-mod="deep_learning" data-read-unit="1-4">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Neural networks are the foundation of deep learning. They are computational models inspired by the human brain, designed to recognize patterns and solve complex problems. In this section, we will cover the basic concepts of neural networks, including their structure, components, and how they function.</p>
</div><h1>Key Concepts</h1>
<div class='content'></div><h2><ol>
<li>Neurons and Layers</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Neuron</strong>: The basic unit of a neural network, also known as a node or perceptron. It receives input, processes it, and produces an output.</li>
<li><strong>Layers</strong>: Neural networks consist of multiple layers:
<ul>
<li><strong>Input Layer</strong>: The first layer that receives the input data.</li>
<li><strong>Hidden Layers</strong>: Intermediate layers that process the input data. There can be one or more hidden layers.</li>
<li><strong>Output Layer</strong>: The final layer that produces the output.</li>
</ul>
</li>
</ul>
</div><h2><ol start="2">
<li>Weights and Biases</li>
</ol></h2>
<div class='content'><ul>
<li><strong>Weights</strong>: Parameters that determine the importance of each input. Each connection between neurons has an associated weight.</li>
<li><strong>Biases</strong>: Additional parameters that allow the model to fit the data better by shifting the activation function.</li>
</ul>
</div><h2><ol start="3">
<li>Activation Functions</li>
</ol></h2>
<div class='content'><ul>
<li>Functions that determine the output of a neuron based on its input. Common activation functions include:
<ul>
<li><strong>Sigmoid</strong>: \( \sigma(x) = \frac{1}{1 + e^{-x}} \)</li>
<li><strong>ReLU (Rectified Linear Unit)</strong>: \( \text{ReLU}(x) = \max(0, x) \)</li>
<li><strong>Tanh</strong>: \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)</li>
</ul>
</li>
</ul>
</div><h2><ol start="4">
<li>Forward Propagation</li>
</ol></h2>
<div class='content'><ul>
<li>The process of passing input data through the network to obtain an output. Each neuron's output is calculated and passed to the next layer.</li>
</ul>
</div><h2><ol start="5">
<li>Loss Function</li>
</ol></h2>
<div class='content'><ul>
<li>A function that measures the difference between the predicted output and the actual output. Common loss functions include:
<ul>
<li><strong>Mean Squared Error (MSE)</strong>: \( \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \)</li>
<li><strong>Cross-Entropy Loss</strong>: Used for classification problems.</li>
</ul>
</li>
</ul>
</div><h2><ol start="6">
<li>Backward Propagation</li>
</ol></h2>
<div class='content'><ul>
<li>The process of updating the weights and biases to minimize the loss function. It involves calculating the gradient of the loss function with respect to each weight and bias and adjusting them accordingly.</li>
</ul>
</div><h1>Example: Simple Neural Network</h1>
<div class='content'><p>Let's create a simple neural network with one hidden layer to understand these concepts better.</p>
</div><h2>Network Structure</h2>
<div class='content'><ul>
<li><strong>Input Layer</strong>: 2 neurons (features)</li>
<li><strong>Hidden Layer</strong>: 3 neurons</li>
<li><strong>Output Layer</strong>: 1 neuron (binary classification)</li>
</ul>
</div><h2>Code Example</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation function: Sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of Sigmoid
def sigmoid_derivative(x):
    return x * (1 - x)

# Input data (4 samples, 2 features)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

# Output data (4 samples, 1 output)
y = np.array([[0], [1], [1], [0]])

# Seed for reproducibility
np.random.seed(42)

# Initialize weights and biases
input_layer_neurons = X.shape[1]
hidden_layer_neurons = 3
output_layer_neurons = 1

# Weights
W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
W2 = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))

# Biases
b1 = np.random.uniform(size=(1, hidden_layer_neurons))
b2 = np.random.uniform(size=(1, output_layer_neurons))

# Training parameters
learning_rate = 0.1
epochs = 10000

# Training loop
for epoch in range(epochs):
    # Forward Propagation
    hidden_layer_input = np.dot(X, W1) + b1
    hidden_layer_output = sigmoid(hidden_layer_input)
    
    output_layer_input = np.dot(hidden_layer_output, W2) + b2
    predicted_output = sigmoid(output_layer_input)
    
    # Calculate error
    error = y - predicted_output
    
    # Backward Propagation
    d_predicted_output = error * sigmoid_derivative(predicted_output)
    error_hidden_layer = d_predicted_output.dot(W2.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)
    
    # Update weights and biases
    W2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    W1 += X.T.dot(d_hidden_layer) * learning_rate
    b1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

# Output after training
print(&quot;Predicted Output: \n&quot;, predicted_output)</pre></div><div class='content'></div><h2>Explanation</h2>
<div class='content'><ol>
<li><strong>Initialization</strong>: We initialize the weights and biases randomly.</li>
<li><strong>Forward Propagation</strong>: We calculate the output of each layer using the sigmoid activation function.</li>
<li><strong>Error Calculation</strong>: We compute the error between the predicted output and the actual output.</li>
<li><strong>Backward Propagation</strong>: We calculate the gradients and update the weights and biases to minimize the error.</li>
<li><strong>Training Loop</strong>: We repeat the forward and backward propagation steps for a specified number of epochs.</li>
</ol>
</div><h1>Practical Exercise</h1>
<div class='content'></div><h2>Exercise: Implement a Neural Network from Scratch</h2>
<div class='content'><p><strong>Task</strong>: Implement a neural network with the following structure:</p>
<ul>
<li>Input Layer: 3 neurons</li>
<li>Hidden Layer: 4 neurons</li>
<li>Output Layer: 2 neurons (multi-class classification)</li>
</ul>
<p><strong>Steps</strong>:</p>
<ol>
<li>Initialize the weights and biases.</li>
<li>Implement the forward propagation.</li>
<li>Calculate the loss using cross-entropy loss.</li>
<li>Implement the backward propagation.</li>
<li>Train the network for 5000 epochs.</li>
</ol>
<p><strong>Solution</strong>:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIEFjdGl2YXRpb24gZnVuY3Rpb246IFNvZnRtYXgKZGVmIHNvZnRtYXgoeCk6CiAgICBleHBfeCA9IG5wLmV4cCh4IC0gbnAubWF4KHgsIGF4aXM9MSwga2VlcGRpbXM9VHJ1ZSkpCiAgICByZXR1cm4gZXhwX3ggLyBucC5zdW0oZXhwX3gsIGF4aXM9MSwga2VlcGRpbXM9VHJ1ZSkKCiMgRGVyaXZhdGl2ZSBvZiBTb2Z0bWF4CmRlZiBzb2Z0bWF4X2Rlcml2YXRpdmUoeCk6CiAgICByZXR1cm4geCAqICgxIC0geCkKCiMgSW5wdXQgZGF0YSAoNCBzYW1wbGVzLCAzIGZlYXR1cmVzKQpYID0gbnAuYXJyYXkoW1swLCAwLCAxXSwgWzAsIDEsIDBdLCBbMSwgMCwgMF0sIFsxLCAxLCAxXV0pCgojIE91dHB1dCBkYXRhICg0IHNhbXBsZXMsIDIgb3V0cHV0cykKeSA9IG5wLmFycmF5KFtbMSwgMF0sIFswLCAxXSwgWzAsIDFdLCBbMSwgMF1dKQoKIyBTZWVkIGZvciByZXByb2R1Y2liaWxpdHkKbnAucmFuZG9tLnNlZWQoNDIpCgojIEluaXRpYWxpemUgd2VpZ2h0cyBhbmQgYmlhc2VzCmlucHV0X2xheWVyX25ldXJvbnMgPSBYLnNoYXBlWzFdCmhpZGRlbl9sYXllcl9uZXVyb25zID0gNApvdXRwdXRfbGF5ZXJfbmV1cm9ucyA9IDIKCiMgV2VpZ2h0cwpXMSA9IG5wLnJhbmRvbS51bmlmb3JtKHNpemU9KGlucHV0X2xheWVyX25ldXJvbnMsIGhpZGRlbl9sYXllcl9uZXVyb25zKSkKVzIgPSBucC5yYW5kb20udW5pZm9ybShzaXplPShoaWRkZW5fbGF5ZXJfbmV1cm9ucywgb3V0cHV0X2xheWVyX25ldXJvbnMpKQoKIyBCaWFzZXMKYjEgPSBucC5yYW5kb20udW5pZm9ybShzaXplPSgxLCBoaWRkZW5fbGF5ZXJfbmV1cm9ucykpCmIyID0gbnAucmFuZG9tLnVuaWZvcm0oc2l6ZT0oMSwgb3V0cHV0X2xheWVyX25ldXJvbnMpKQoKIyBUcmFpbmluZyBwYXJhbWV0ZXJzCmxlYXJuaW5nX3JhdGUgPSAwLjEKZXBvY2hzID0gNTAwMAoKIyBUcmFpbmluZyBsb29wCmZvciBlcG9jaCBpbiByYW5nZShlcG9jaHMpOgogICAgIyBGb3J3YXJkIFByb3BhZ2F0aW9uCiAgICBoaWRkZW5fbGF5ZXJfaW5wdXQgPSBucC5kb3QoWCwgVzEpICsgYjEKICAgIGhpZGRlbl9sYXllcl9vdXRwdXQgPSBzb2Z0bWF4KGhpZGRlbl9sYXllcl9pbnB1dCkKICAgIAogICAgb3V0cHV0X2xheWVyX2lucHV0ID0gbnAuZG90KGhpZGRlbl9sYXllcl9vdXRwdXQsIFcyKSArIGIyCiAgICBwcmVkaWN0ZWRfb3V0cHV0ID0gc29mdG1heChvdXRwdXRfbGF5ZXJfaW5wdXQpCiAgICAKICAgICMgQ2FsY3VsYXRlIGVycm9yCiAgICBlcnJvciA9IHkgLSBwcmVkaWN0ZWRfb3V0cHV0CiAgICAKICAgICMgQmFja3dhcmQgUHJvcGFnYXRpb24KICAgIGRfcHJlZGljdGVkX291dHB1dCA9IGVycm9yICogc29mdG1heF9kZXJpdmF0aXZlKHByZWRpY3RlZF9vdXRwdXQpCiAgICBlcnJvcl9oaWRkZW5fbGF5ZXIgPSBkX3ByZWRpY3RlZF9vdXRwdXQuZG90KFcyLlQpCiAgICBkX2hpZGRlbl9sYXllciA9IGVycm9yX2hpZGRlbl9sYXllciAqIHNvZnRtYXhfZGVyaXZhdGl2ZShoaWRkZW5fbGF5ZXJfb3V0cHV0KQogICAgCiAgICAjIFVwZGF0ZSB3ZWlnaHRzIGFuZCBiaWFzZXMKICAgIFcyICs9IGhpZGRlbl9sYXllcl9vdXRwdXQuVC5kb3QoZF9wcmVkaWN0ZWRfb3V0cHV0KSAqIGxlYXJuaW5nX3JhdGUKICAgIGIyICs9IG5wLnN1bShkX3ByZWRpY3RlZF9vdXRwdXQsIGF4aXM9MCwga2VlcGRpbXM9VHJ1ZSkgKiBsZWFybmluZ19yYXRlCiAgICBXMSArPSBYLlQuZG90KGRfaGlkZGVuX2xheWVyKSAqIGxlYXJuaW5nX3JhdGUKICAgIGIxICs9IG5wLnN1bShkX2hpZGRlbl9sYXllciwgYXhpcz0wLCBrZWVwZGltcz1UcnVlKSAqIGxlYXJuaW5nX3JhdGUKCiMgT3V0cHV0IGFmdGVyIHRyYWluaW5nCnByaW50KCJQcmVkaWN0ZWQgT3V0cHV0OiBcbiIsIHByZWRpY3RlZF9vdXRwdXQp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Activation function: Softmax
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

# Derivative of Softmax
def softmax_derivative(x):
    return x * (1 - x)

# Input data (4 samples, 3 features)
X = np.array([[0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 1]])

# Output data (4 samples, 2 outputs)
y = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])

# Seed for reproducibility
np.random.seed(42)

# Initialize weights and biases
input_layer_neurons = X.shape[1]
hidden_layer_neurons = 4
output_layer_neurons = 2

# Weights
W1 = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))
W2 = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))

# Biases
b1 = np.random.uniform(size=(1, hidden_layer_neurons))
b2 = np.random.uniform(size=(1, output_layer_neurons))

# Training parameters
learning_rate = 0.1
epochs = 5000

# Training loop
for epoch in range(epochs):
    # Forward Propagation
    hidden_layer_input = np.dot(X, W1) + b1
    hidden_layer_output = softmax(hidden_layer_input)
    
    output_layer_input = np.dot(hidden_layer_output, W2) + b2
    predicted_output = softmax(output_layer_input)
    
    # Calculate error
    error = y - predicted_output
    
    # Backward Propagation
    d_predicted_output = error * softmax_derivative(predicted_output)
    error_hidden_layer = d_predicted_output.dot(W2.T)
    d_hidden_layer = error_hidden_layer * softmax_derivative(hidden_layer_output)
    
    # Update weights and biases
    W2 += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    b2 += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    W1 += X.T.dot(d_hidden_layer) * learning_rate
    b1 += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

# Output after training
print(&quot;Predicted Output: \n&quot;, predicted_output)</pre></div><div class='content'></div><h2>Common Mistakes and Tips</h2>
<div class='content'><ul>
<li><strong>Initialization</strong>: Ensure weights and biases are initialized properly to avoid vanishing or exploding gradients.</li>
<li><strong>Learning Rate</strong>: Choose an appropriate learning rate. Too high can cause divergence, too low can slow down the training.</li>
<li><strong>Activation Functions</strong>: Use appropriate activation functions for different layers. For example, ReLU for hidden layers and softmax for output layers in classification problems.</li>
</ul>
</div><h1>Conclusion</h1>
<div class='content'><p>In this section, we covered the basic concepts of neural networks, including neurons, layers, weights, biases, activation functions, forward and backward propagation, and loss functions. We also implemented a simple neural network from scratch to solidify these concepts. Understanding these basics is crucial as we delve deeper into more complex neural network architectures in the upcoming modules.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='01-03-applications-deep-learning' title="Applications of Deep Learning" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='01-03-applications-deep-learning' title="Applications of Deep Learning" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary"
				data-read-mod="deep_learning" data-read-unit="1-4">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='02-01-perceptron-multilayer-perceptron' title="Perceptron and Multilayer Perceptron" class="py-2 px-3 btn btn-primary" 
				data-read-mod="deep_learning" data-read-unit="1-4">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>Deep Learning Course</h1>
<h2>Module 1: Introduction to Deep Learning</h2>
<ul>
<li><a href="01-01-what-is-deep-learning">What is Deep Learning?</a></li>
<li><a href="01-02-history-evolution-deep-learning">History and Evolution of Deep Learning</a></li>
<li><a href="01-03-applications-deep-learning">Applications of Deep Learning</a></li>
<li><a href="01-04-basic-concepts-neural-networks">Basic Concepts of Neural Networks</a></li>
</ul>
<h2>Module 2: Fundamentals of Neural Networks</h2>
<ul>
<li><a href="02-01-perceptron-multilayer-perceptron">Perceptron and Multilayer Perceptron</a></li>
<li><a href="02-02-activation-function">Activation Function</a></li>
<li><a href="02-03-forward-backward-propagation">Forward and Backward Propagation</a></li>
<li><a href="02-04-optimization-loss-function">Optimization and Loss Function</a></li>
</ul>
<h2>Module 3: Convolutional Neural Networks (CNN)</h2>
<ul>
<li><a href="03-01-introduction-cnn">Introduction to CNN</a></li>
<li><a href="03-02-convolutional-pooling-layers">Convolutional and Pooling Layers</a></li>
<li><a href="03-03-popular-cnn-architectures">Popular CNN Architectures</a></li>
<li><a href="03-04-cnn-applications-image-recognition">CNN Applications in Image Recognition</a></li>
</ul>
<h2>Module 4: Recurrent Neural Networks (RNN)</h2>
<ul>
<li><a href="04-01-introduction-rnn">Introduction to RNN</a></li>
<li><a href="04-02-lstm-gru">LSTM and GRU</a></li>
<li><a href="04-03-rnn-applications-nlp">RNN Applications in Natural Language Processing</a></li>
<li><a href="04-04-sequences-time-series">Sequences and Time Series</a></li>
</ul>
<h2>Module 5: Advanced Techniques in Deep Learning</h2>
<ul>
<li><a href="05-01-generative-adversarial-networks">Generative Adversarial Networks (GAN)</a></li>
<li><a href="05-02-autoencoders">Autoencoders</a></li>
<li><a href="05-03-transfer-learning">Transfer Learning</a></li>
<li><a href="05-04-regularization-improvement-techniques">Regularization and Improvement Techniques</a></li>
</ul>
<h2>Module 6: Tools and Frameworks</h2>
<ul>
<li><a href="06-01-introduction-tensorflow">Introduction to TensorFlow</a></li>
<li><a href="06-02-introduction-pytorch">Introduction to PyTorch</a></li>
<li><a href="06-03-framework-comparison">Framework Comparison</a></li>
<li><a href="06-04-development-environments-resources">Development Environments and Additional Resources</a></li>
</ul>
<h2>Module 7: Practical Projects</h2>
<ul>
<li><a href="07-01-image-classification-cnn">Image Classification with CNN</a></li>
<li><a href="07-02-text-generation-rnn">Text Generation with RNN</a></li>
<li><a href="07-03-anomaly-detection-autoencoders">Anomaly Detection with Autoencoders</a></li>
<li><a href="07-04-creating-gan-image-generation">Creating a GAN for Image Generation</a></li>
</ul>
<h2>Module 8: Ethical Considerations and the Future of Deep Learning</h2>
<ul>
<li><a href="08-01-ethics-deep-learning">Ethics in Deep Learning</a></li>
<li><a href="08-02-social-economic-impact">Social and Economic Impact</a></li>
<li><a href="08-03-future-trends-deep-learning">Future Trends in Deep Learning</a></li>
<li><a href="08-04-challenges-opportunities">Challenges and Opportunities</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">User not authenticated</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
