<!DOCTYPE html>
<html lang="en">
<head>
    <title> Reinforcement Learning with PyTorch </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, nofollow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/cursos/pytorch/06-02-reinforcement-learning" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/cursos/pytorch/06-02-reinforcement-learning" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/courses/pytorch/06-02-reinforcement-learning" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.ea63f62b9e.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "en";
  		var CATEGORY = "frameworks";
  		var MOD_NAME = "pytorch";
  		var TEMA_NAME = "6-2";
  		var TYPE = "mod";
  		var PATH = "mod/pytorch/06-02-reinforcement-learning";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.902a5a267d.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo-header_enterprise.png"></a>
			</h1>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/cursos/pytorch/06-02-reinforcement-learning" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/cursos/pytorch/06-02-reinforcement-learning" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/my-courses" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>My courses</b></i>
</a>
<a href="/completed-courses" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Completed             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="pytorch">
		<a  href="#" class="text-secondary d-none" data-read-mod="pytorch" data-read-unit="6-2" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Mark as read
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="pytorch" data-unread-unit="6-2" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Mark as unread
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Course Content
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='06-01-gans' title="Generative Adversarial Networks (GANs)" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='06-01-gans' title="Generative Adversarial Networks (GANs)" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h2 style="text-decoration:underline">Reinforcement Learning with PyTorch</h2>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='06-03-deploying-models' title="Deploying PyTorch Models" class="py-2 px-3 btn btn-primary"
				data-read-mod="pytorch" data-read-unit="6-2">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='06-03-deploying-models' title="Deploying PyTorch Models" class="py-2 px-3 btn btn-primary" 
				data-read-mod="pytorch" data-read-unit="6-2">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'><p>Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize some notion of cumulative reward. In this section, we will explore the basics of RL and how to implement RL algorithms using PyTorch.</p>
</div><h1>Key Concepts in Reinforcement Learning</h1>
<div class='content'><ol>
<li><strong>Agent</strong>: The learner or decision-maker.</li>
<li><strong>Environment</strong>: The external system with which the agent interacts.</li>
<li><strong>State (s)</strong>: A representation of the current situation of the agent.</li>
<li><strong>Action (a)</strong>: The set of all possible moves the agent can make.</li>
<li><strong>Reward (r)</strong>: The feedback from the environment based on the action taken.</li>
<li><strong>Policy (Ï€)</strong>: The strategy that the agent employs to determine the next action based on the current state.</li>
<li><strong>Value Function (V)</strong>: The expected cumulative reward from a given state.</li>
<li><strong>Q-Function (Q)</strong>: The expected cumulative reward from a given state-action pair.</li>
</ol>
</div><h1>Setting Up the Environment</h1>
<div class='content'><p>Before diving into the implementation, ensure you have the necessary libraries installed:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("cGlwIGluc3RhbGwgdG9yY2ggZ3lt"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>pip install torch gym</pre></div><div class='content'></div><h1>Implementing a Simple RL Algorithm: Q-Learning</h1>
<div class='content'></div><h2>Step 1: Import Libraries</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IGd5bQppbXBvcnQgdG9yY2gKaW1wb3J0IHRvcmNoLm5uIGFzIG5uCmltcG9ydCB0b3JjaC5vcHRpbSBhcyBvcHRpbQppbXBvcnQgbnVtcHkgYXMgbnA="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np</pre></div><div class='content'></div><h2>Step 2: Define the Q-Network</h2>
<div class='content'><p>A Q-Network approximates the Q-Function using a neural network.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUU5ldHdvcmsobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSk6CiAgICAgICAgc3VwZXIoUU5ldHdvcmssIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcihzdGF0ZV9zaXplLCA2NCkKICAgICAgICBzZWxmLmZjMiA9IG5uLkxpbmVhcig2NCwgNjQpCiAgICAgICAgc2VsZi5mYzMgPSBubi5MaW5lYXIoNjQsIGFjdGlvbl9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMih4KSkKICAgICAgICB4ID0gc2VsZi5mYzMoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x</pre></div><div class='content'></div><h2>Step 3: Initialize the Environment and Network</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("ZW52ID0gZ3ltLm1ha2UoJ0NhcnRQb2xlLXYxJykKc3RhdGVfc2l6ZSA9IGVudi5vYnNlcnZhdGlvbl9zcGFjZS5zaGFwZVswXQphY3Rpb25fc2l6ZSA9IGVudi5hY3Rpb25fc3BhY2UubgoKcV9uZXR3b3JrID0gUU5ldHdvcmsoc3RhdGVfc2l6ZSwgYWN0aW9uX3NpemUpCm9wdGltaXplciA9IG9wdGltLkFkYW0ocV9uZXR3b3JrLnBhcmFtZXRlcnMoKSwgbHI9MC4wMDEpCmNyaXRlcmlvbiA9IG5uLk1TRUxvc3MoKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>env = gym.make('CartPole-v1')
state_size = env.observation_space.shape[0]
action_size = env.action_space.n

q_network = QNetwork(state_size, action_size)
optimizer = optim.Adam(q_network.parameters(), lr=0.001)
criterion = nn.MSELoss()</pre></div><div class='content'></div><h2>Step 4: Define the Training Loop</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("bnVtX2VwaXNvZGVzID0gMTAwMApnYW1tYSA9IDAuOTkgICMgRGlzY291bnQgZmFjdG9yCmVwc2lsb24gPSAxLjAgICMgRXhwbG9yYXRpb24gcmF0ZQplcHNpbG9uX2RlY2F5ID0gMC45OTUKZXBzaWxvbl9taW4gPSAwLjAxCgpmb3IgZXBpc29kZSBpbiByYW5nZShudW1fZXBpc29kZXMpOgogICAgc3RhdGUgPSBlbnYucmVzZXQoKQogICAgc3RhdGUgPSB0b3JjaC5GbG9hdFRlbnNvcihzdGF0ZSkudW5zcXVlZXplKDApCiAgICB0b3RhbF9yZXdhcmQgPSAwCiAgICAKICAgIGZvciB0IGluIHJhbmdlKDIwMCk6CiAgICAgICAgIyBFcHNpbG9uLWdyZWVkeSBhY3Rpb24gc2VsZWN0aW9uCiAgICAgICAgaWYgbnAucmFuZG9tLnJhbmQoKSA8PSBlcHNpbG9uOgogICAgICAgICAgICBhY3Rpb24gPSBucC5yYW5kb20uY2hvaWNlKGFjdGlvbl9zaXplKQogICAgICAgIGVsc2U6CiAgICAgICAgICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgICAgICAgICAgcV92YWx1ZXMgPSBxX25ldHdvcmsoc3RhdGUpCiAgICAgICAgICAgICAgICBhY3Rpb24gPSB0b3JjaC5hcmdtYXgocV92YWx1ZXMpLml0ZW0oKQogICAgICAgIAogICAgICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgICAgICBuZXh0X3N0YXRlID0gdG9yY2guRmxvYXRUZW5zb3IobmV4dF9zdGF0ZSkudW5zcXVlZXplKDApCiAgICAgICAgdG90YWxfcmV3YXJkICs9IHJld2FyZAogICAgICAgIAogICAgICAgICMgQ29tcHV0ZSB0aGUgdGFyZ2V0IFEtdmFsdWUKICAgICAgICB3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgICAgICAgICAgdGFyZ2V0X3FfdmFsdWUgPSByZXdhcmQgKyBnYW1tYSAqIHRvcmNoLm1heChxX25ldHdvcmsobmV4dF9zdGF0ZSkpCiAgICAgICAgCiAgICAgICAgIyBDb21wdXRlIHRoZSBjdXJyZW50IFEtdmFsdWUKICAgICAgICBjdXJyZW50X3FfdmFsdWUgPSBxX25ldHdvcmsoc3RhdGUpWzAsIGFjdGlvbl0KICAgICAgICAKICAgICAgICAjIENvbXB1dGUgdGhlIGxvc3MKICAgICAgICBsb3NzID0gY3JpdGVyaW9uKGN1cnJlbnRfcV92YWx1ZSwgdGFyZ2V0X3FfdmFsdWUpCiAgICAgICAgCiAgICAgICAgIyBPcHRpbWl6ZSB0aGUgUS1uZXR3b3JrCiAgICAgICAgb3B0aW1pemVyLnplcm9fZ3JhZCgpCiAgICAgICAgbG9zcy5iYWNrd2FyZCgpCiAgICAgICAgb3B0aW1pemVyLnN0ZXAoKQogICAgICAgIAogICAgICAgIHN0YXRlID0gbmV4dF9zdGF0ZQogICAgICAgIAogICAgICAgIGlmIGRvbmU6CiAgICAgICAgICAgIGJyZWFrCiAgICAKICAgICMgRGVjYXkgZXBzaWxvbgogICAgaWYgZXBzaWxvbiA+IGVwc2lsb25fbWluOgogICAgICAgIGVwc2lsb24gKj0gZXBzaWxvbl9kZWNheQogICAgCiAgICBwcmludChmIkVwaXNvZGUge2VwaXNvZGUrMX0ve251bV9lcGlzb2Rlc30sIFRvdGFsIFJld2FyZDoge3RvdGFsX3Jld2FyZH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>num_episodes = 1000
gamma = 0.99  # Discount factor
epsilon = 1.0  # Exploration rate
epsilon_decay = 0.995
epsilon_min = 0.01

for episode in range(num_episodes):
    state = env.reset()
    state = torch.FloatTensor(state).unsqueeze(0)
    total_reward = 0
    
    for t in range(200):
        # Epsilon-greedy action selection
        if np.random.rand() &lt;= epsilon:
            action = np.random.choice(action_size)
        else:
            with torch.no_grad():
                q_values = q_network(state)
                action = torch.argmax(q_values).item()
        
        next_state, reward, done, _ = env.step(action)
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        total_reward += reward
        
        # Compute the target Q-value
        with torch.no_grad():
            target_q_value = reward + gamma * torch.max(q_network(next_state))
        
        # Compute the current Q-value
        current_q_value = q_network(state)[0, action]
        
        # Compute the loss
        loss = criterion(current_q_value, target_q_value)
        
        # Optimize the Q-network
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        state = next_state
        
        if done:
            break
    
    # Decay epsilon
    if epsilon &gt; epsilon_min:
        epsilon *= epsilon_decay
    
    print(f&quot;Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h2>Step 5: Evaluate the Trained Agent</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("c3RhdGUgPSBlbnYucmVzZXQoKQpzdGF0ZSA9IHRvcmNoLkZsb2F0VGVuc29yKHN0YXRlKS51bnNxdWVlemUoMCkKdG90YWxfcmV3YXJkID0gMAoKZm9yIHQgaW4gcmFuZ2UoMjAwKToKICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgIHFfdmFsdWVzID0gcV9uZXR3b3JrKHN0YXRlKQogICAgICAgIGFjdGlvbiA9IHRvcmNoLmFyZ21heChxX3ZhbHVlcykuaXRlbSgpCiAgICAKICAgIG5leHRfc3RhdGUsIHJld2FyZCwgZG9uZSwgXyA9IGVudi5zdGVwKGFjdGlvbikKICAgIG5leHRfc3RhdGUgPSB0b3JjaC5GbG9hdFRlbnNvcihuZXh0X3N0YXRlKS51bnNxdWVlemUoMCkKICAgIHRvdGFsX3Jld2FyZCArPSByZXdhcmQKICAgIHN0YXRlID0gbmV4dF9zdGF0ZQogICAgCiAgICBpZiBkb25lOgogICAgICAgIGJyZWFrCgpwcmludChmIlRvdGFsIFJld2FyZDoge3RvdGFsX3Jld2FyZH0iKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>state = env.reset()
state = torch.FloatTensor(state).unsqueeze(0)
total_reward = 0

for t in range(200):
    with torch.no_grad():
        q_values = q_network(state)
        action = torch.argmax(q_values).item()
    
    next_state, reward, done, _ = env.step(action)
    next_state = torch.FloatTensor(next_state).unsqueeze(0)
    total_reward += reward
    state = next_state
    
    if done:
        break

print(f&quot;Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h1>Practical Exercises</h1>
<div class='content'></div><h2>Exercise 1: Modify the Q-Network Architecture</h2>
<div class='content'><p><strong>Task</strong>: Modify the Q-Network to include an additional hidden layer with 128 neurons. Train the network and observe the performance.</p>
</div><h2>Solution:</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgUU5ldHdvcmsobm4uTW9kdWxlKToKICAgIGRlZiBfX2luaXRfXyhzZWxmLCBzdGF0ZV9zaXplLCBhY3Rpb25fc2l6ZSk6CiAgICAgICAgc3VwZXIoUU5ldHdvcmssIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmZjMSA9IG5uLkxpbmVhcihzdGF0ZV9zaXplLCA2NCkKICAgICAgICBzZWxmLmZjMiA9IG5uLkxpbmVhcig2NCwgMTI4KQogICAgICAgIHNlbGYuZmMzID0gbm4uTGluZWFyKDEyOCwgNjQpCiAgICAgICAgc2VsZi5mYzQgPSBubi5MaW5lYXIoNjQsIGFjdGlvbl9zaXplKQogICAgCiAgICBkZWYgZm9yd2FyZChzZWxmLCB4KToKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMSh4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMih4KSkKICAgICAgICB4ID0gdG9yY2gucmVsdShzZWxmLmZjMyh4KSkKICAgICAgICB4ID0gc2VsZi5mYzQoeCkKICAgICAgICByZXR1cm4geA=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class QNetwork(nn.Module):
    def __init__(self, state_size, action_size):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, 64)
        self.fc2 = nn.Linear(64, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, action_size)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        x = self.fc4(x)
        return x</pre></div><div class='content'></div><h2>Exercise 2: Implement Experience Replay</h2>
<div class='content'><p><strong>Task</strong>: Implement experience replay to store and sample past experiences to break the correlation between consecutive experiences.</p>
</div><h2>Solution:</h2>
<div class='content'></div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("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"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>from collections import deque
import random

class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

# Initialize replay buffer
replay_buffer = ReplayBuffer(10000)

# Modify the training loop to use experience replay
batch_size = 64

for episode in range(num_episodes):
    state = env.reset()
    state = torch.FloatTensor(state).unsqueeze(0)
    total_reward = 0
    
    for t in range(200):
        if np.random.rand() &lt;= epsilon:
            action = np.random.choice(action_size)
        else:
            with torch.no_grad():
                q_values = q_network(state)
                action = torch.argmax(q_values).item()
        
        next_state, reward, done, _ = env.step(action)
        next_state = torch.FloatTensor(next_state).unsqueeze(0)
        total_reward += reward
        
        replay_buffer.push(state, action, reward, next_state, done)
        
        state = next_state
        
        if len(replay_buffer) &gt; batch_size:
            states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)
            states = torch.cat(states)
            next_states = torch.cat(next_states)
            actions = torch.tensor(actions).unsqueeze(1)
            rewards = torch.tensor(rewards).unsqueeze(1)
            dones = torch.tensor(dones).unsqueeze(1)
            
            with torch.no_grad():
                target_q_values = rewards + gamma * torch.max(q_network(next_states), dim=1, keepdim=True)[0] * (1 - dones)
            
            current_q_values = q_network(states).gather(1, actions)
            
            loss = criterion(current_q_values, target_q_values)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        if done:
            break
    
    if epsilon &gt; epsilon_min:
        epsilon *= epsilon_decay
    
    print(f&quot;Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}&quot;)</pre></div><div class='content'></div><h1>Summary</h1>
<div class='content'><p>In this section, we covered the basics of reinforcement learning and implemented a simple Q-Learning algorithm using PyTorch. We also explored practical exercises to modify the Q-Network architecture and implement experience replay. These exercises help reinforce the concepts and provide hands-on experience with RL in PyTorch.</p>
<p>In the next module, we will delve into more advanced topics and explore other RL algorithms and techniques.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='06-01-gans' title="Generative Adversarial Networks (GANs)" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='06-01-gans' title="Generative Adversarial Networks (GANs)" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='06-03-deploying-models' title="Deploying PyTorch Models" class="py-2 px-3 btn btn-primary"
				data-read-mod="pytorch" data-read-unit="6-2">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='06-03-deploying-models' title="Deploying PyTorch Models" class="py-2 px-3 btn btn-primary" 
				data-read-mod="pytorch" data-read-unit="6-2">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>PyTorch: From Beginner to Advanced</h1>
<h2>Module 1: Introduction to PyTorch</h2>
<ul>
<li><a href="01-01-what-is-pytorch">What is PyTorch?</a></li>
<li><a href="01-02-setting-up-environment">Setting Up the Environment</a></li>
<li><a href="01-03-basic-tensor-operations">Basic Tensor Operations</a></li>
<li><a href="01-04-autograd">Autograd: Automatic Differentiation</a></li>
</ul>
<h2>Module 2: Building Neural Networks</h2>
<ul>
<li><a href="02-01-introduction-to-neural-networks">Introduction to Neural Networks</a></li>
<li><a href="02-02-creating-simple-neural-network">Creating a Simple Neural Network</a></li>
<li><a href="02-03-activation-functions">Activation Functions</a></li>
<li><a href="02-04-loss-functions-optimization">Loss Functions and Optimization</a></li>
</ul>
<h2>Module 3: Training Neural Networks</h2>
<ul>
<li><a href="03-01-data-loading-preprocessing">Data Loading and Preprocessing</a></li>
<li><a href="03-02-training-loop">Training Loop</a></li>
<li><a href="03-03-validation-testing">Validation and Testing</a></li>
<li><a href="03-04-saving-loading-models">Saving and Loading Models</a></li>
</ul>
<h2>Module 4: Convolutional Neural Networks (CNNs)</h2>
<ul>
<li><a href="04-01-introduction-to-cnns">Introduction to CNNs</a></li>
<li><a href="04-02-building-cnn-from-scratch">Building a CNN from Scratch</a></li>
<li><a href="04-03-transfer-learning">Transfer Learning with Pre-trained Models</a></li>
<li><a href="04-04-fine-tuning-cnns">Fine-Tuning CNNs</a></li>
</ul>
<h2>Module 5: Recurrent Neural Networks (RNNs)</h2>
<ul>
<li><a href="05-01-introduction-to-rnns">Introduction to RNNs</a></li>
<li><a href="05-02-building-rnn-from-scratch">Building an RNN from Scratch</a></li>
<li><a href="05-03-lstm-networks">Long Short-Term Memory (LSTM) Networks</a></li>
<li><a href="05-04-gru-networks">Gated Recurrent Units (GRUs)</a></li>
</ul>
<h2>Module 6: Advanced Topics</h2>
<ul>
<li><a href="06-01-gans">Generative Adversarial Networks (GANs)</a></li>
<li><a href="06-02-reinforcement-learning">Reinforcement Learning with PyTorch</a></li>
<li><a href="06-03-deploying-models">Deploying PyTorch Models</a></li>
<li><a href="06-04-optimizing-performance">Optimizing Performance</a></li>
</ul>
<h2>Module 7: Case Studies and Projects</h2>
<ul>
<li><a href="07-01-image-classification-project">Image Classification Project</a></li>
<li><a href="07-02-nlp-project">Natural Language Processing Project</a></li>
<li><a href="07-03-time-series-forecasting-project">Time Series Forecasting Project</a></li>
<li><a href="07-04-custom-project">Custom Project</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">User not authenticated</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
