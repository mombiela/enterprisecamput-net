<!DOCTYPE html>
<html lang="en">
<head>
    <title> Activation Functions </title>
        
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="index, follow, noarchive">
    
    <link rel="alternate" href="https://campusempresa.com/cursos/tensorflow/04-03-activation-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/cursos/tensorflow/04-03-activation-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/courses/tensorflow/04-03-activation-functions" hreflang="en" />
    
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.1ab297bfa4.css" rel="stylesheet">
	 
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="/js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script>
  		var LANG = "en";
  		var CATEGORY = "frameworks";
  		var MOD_NAME = "tensorflow";
  		var TEMA_NAME = "4-3";
  		var TYPE = "mod";
  		var PATH = "mod/tensorflow/04-03-activation-functions";
  		var IS_INDEX = false;
  	</script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="module" src="/js/app.902a5a267d.js"></script>
	<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-0611338592562725" crossorigin="anonymous"></script>
	  	
	<!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-VVPMPJSR3P"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());	
	  gtag('config', 'G-VVPMPJSR3P');
	</script>
</head>

<body class="d-none">
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-12 col-md-6 p-0">
			<a href="/"><img src="/img/logo-header_enterprise.png" alt="Logo Enterprise Campus"></a>
		</div>
		<div class="col-12 col-md-6 p-0 text-end">
			<p class="mb-0 p-0">	<b id="lit_lang_es" class="px-2">EN</b>
	|
	<a href="https://campusempresa.com/cursos/tensorflow/04-03-activation-functions" class="px-2">ES</a></b>
	|
	<a href="https://campusempresa.cat/cursos/tensorflow/04-03-activation-functions" class="px-2">CA</a>
</p>
			<p class="mb-4 mt-0 mx-2  d-none d-md-block"><cite>All the knowledge within your reach</cite></p>
		</div>
	</div>
</div>
<div class="subheader container-xxl d-none d-md-block">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>
		<div class="top-bar container-fluid p-0">
	<div class="container-xxl p-0">
		<div class="row">
			<div class="col">
				<div class="d-flex justify-content-between">
					<div class="left">
						<a href="/" class="nav-link px-3" id="btnHome">
	<i class="bi bi-house-fill"></i>
	HOME
</a>

<a href="/my-courses" class="nav-link px-3 d-none" id="btnMyCourses">
	<i class="bi bi-rocket-takeoff-fill"></i>
	<i><b>My courses</b></i>
</a>
<a href="/completed-courses" class="nav-link px-3 d-none" id="trophy_button">
	<i class="bi bi-trophy-fill"></i>
	Completed             
</a>

					</div>
                    <div class="ms-auto right">
                        <a id="user_button" href="#" class="nav-link px-3" data-bs-toggle="modal" data-bs-target="#loginModal">
                            <i id="user_icon" class="bi"></i>                            
                        </a>
                    </div>					
				</div>
			</div>
		</div>
	</div>
</div>

		<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
										<div class="row py-1 m-0" id="buttonsModSection">
	<div class="col-6 p-0" data-mod="tensorflow">
		<a  href="#" class="text-secondary d-none" data-read-mod="tensorflow" data-read-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-check-circle-fill"></i> 
			Mark as read
		</a>
		<a href="#" class="text-secondary d-none" data-unread-mod="tensorflow" data-unread-unit="4-3" style="text-decoration:none;">
			<i class="bi bi-x-circle-fill"></i>
			Mark as unread
		</a>
	</div>
	<div class="col-6 text-end p-0">
					<a href="./"  class="nav-link">
				<i class="bi bi-journal-text"></i>
				Course Content
			</a>
			</div>
</div>						<div id="inner_content">
				<div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-creating-a-simple-neural-network' title="Creating a Simple Neural Network" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-creating-a-simple-neural-network' title="Creating a Simple Neural Network" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
					<h1 style="text-decoration:underline">Activation Functions</h1>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-loss-functions-and-optimizers' title="Loss Functions and Optimizers" class="py-2 px-3 btn btn-primary"
				data-read-mod="tensorflow" data-read-unit="4-3">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-loss-functions-and-optimizers' title="Loss Functions and Optimizers" class="py-2 px-3 btn btn-primary" 
				data-read-mod="tensorflow" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>
<div class='content'><p>Activation functions play a crucial role in neural networks by introducing non-linearity into the model, allowing it to learn complex patterns. In this section, we will explore various activation functions, their properties, and their applications.</p>
</div><h2>Key Concepts</h2>
<div class='content'><ol>
<li>
<p><strong>What is an Activation Function?</strong></p>
<ul>
<li>An activation function determines whether a neuron should be activated or not by calculating the weighted sum and adding bias to it.</li>
<li>It introduces non-linearity into the output of a neuron, enabling the network to learn complex patterns.</li>
</ul>
</li>
<li>
<p><strong>Types of Activation Functions</strong></p>
<ul>
<li><strong>Linear Activation Function</strong></li>
<li><strong>Non-Linear Activation Functions</strong>
<ul>
<li>Sigmoid</li>
<li>Tanh</li>
<li>ReLU (Rectified Linear Unit)</li>
<li>Leaky ReLU</li>
<li>Softmax</li>
</ul>
</li>
</ul>
</li>
</ol>
</div><h2>Linear Activation Function</h2>
<div class='content'></div><h3>Definition</h3>
<div class='content'><p>A linear activation function is simply the identity function, where the output is directly proportional to the input.</p>
</div><h3>Formula</h3>
<div class='content'><p>\[ f(x) = x \]</p>
</div><h3>Characteristics</h3>
<div class='content'><ul>
<li><strong>Pros:</strong> Simple and easy to implement.</li>
<li><strong>Cons:</strong> Cannot handle complex patterns due to its linear nature.</li>
</ul>
</div><h3>Code Example</h3>
<div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgTGluZWFyIGFjdGl2YXRpb24gZnVuY3Rpb24KZGVmIGxpbmVhcl9hY3RpdmF0aW9uKHgpOgogICAgcmV0dXJuIHgKCiMgRXhhbXBsZSB1c2FnZQp4ID0gdGYuY29uc3RhbnQoWzEuMCwgMi4wLCAzLjBdKQpvdXRwdXQgPSBsaW5lYXJfYWN0aXZhdGlvbih4KQpwcmludChvdXRwdXQubnVtcHkoKSkgICMgT3V0cHV0OiBbMS4gMi4gMy5d"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Linear activation function
def linear_activation(x):
    return x

# Example usage
x = tf.constant([1.0, 2.0, 3.0])
output = linear_activation(x)
print(output.numpy())  # Output: [1. 2. 3.]</pre></div><div class='content'></div><h2>Non-Linear Activation Functions</h2>
<div class='content'></div><h3>Sigmoid</h3>
<div class='content'><h4>Definition</h4>
<p>The sigmoid function maps any input to a value between 0 and 1.</p>
<h4>Formula</h4>
<p>\[ f(x) = \frac{1}{1 + e^{-x}} \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Smooth gradient, output range (0, 1), good for binary classification.</li>
<li><strong>Cons:</strong> Vanishing gradient problem, not zero-centered.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgU2lnbW9pZCBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0pCm91dHB1dCA9IHRmLm5uLnNpZ21vaWQoeCkKcHJpbnQob3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuNzMxMDU4NiAwLjg4MDc5NyAgMC45NTI1NzQxM10="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Sigmoid activation function
x = tf.constant([1.0, 2.0, 3.0])
output = tf.nn.sigmoid(x)
print(output.numpy())  # Output: [0.7310586 0.880797  0.95257413]</pre></div><div class='content'></div><h3>Tanh</h3>
<div class='content'><h4>Definition</h4>
<p>The tanh function maps any input to a value between -1 and 1.</p>
<h4>Formula</h4>
<p>\[ f(x) = \tanh(x) = \frac{2}{1 + e^{-2x}} - 1 \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Zero-centered, smooth gradient.</li>
<li><strong>Cons:</strong> Vanishing gradient problem.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgVGFuaCBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0pCm91dHB1dCA9IHRmLm5uLnRhbmgoeCkKcHJpbnQob3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuNzYxNTk0MiAwLjk2NDAyNzYgMC45OTUwNTQ3XQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Tanh activation function
x = tf.constant([1.0, 2.0, 3.0])
output = tf.nn.tanh(x)
print(output.numpy())  # Output: [0.7615942 0.9640276 0.9950547]</pre></div><div class='content'></div><h3>ReLU (Rectified Linear Unit)</h3>
<div class='content'><h4>Definition</h4>
<p>The ReLU function outputs the input directly if it is positive; otherwise, it outputs zero.</p>
<h4>Formula</h4>
<p>\[ f(x) = \max(0, x) \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Computationally efficient, mitigates the vanishing gradient problem.</li>
<li><strong>Cons:</strong> Can cause &quot;dead neurons&quot; if many neurons output zero.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgUmVMVSBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbLTEuMCwgMi4wLCAzLjBdKQpvdXRwdXQgPSB0Zi5ubi5yZWx1KHgpCnByaW50KG91dHB1dC5udW1weSgpKSAgIyBPdXRwdXQ6IFswLiAyLiAzLl0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# ReLU activation function
x = tf.constant([-1.0, 2.0, 3.0])
output = tf.nn.relu(x)
print(output.numpy())  # Output: [0. 2. 3.]</pre></div><div class='content'></div><h3>Leaky ReLU</h3>
<div class='content'><h4>Definition</h4>
<p>Leaky ReLU allows a small, non-zero gradient when the input is negative.</p>
<h4>Formula</h4>
<p>\[ f(x) = \max(0.01x, x) \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Prevents &quot;dead neurons&quot; by allowing a small gradient when the input is negative.</li>
<li><strong>Cons:</strong> The slope of the negative part is a hyperparameter that needs tuning.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgTGVha3kgUmVMVSBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbLTEuMCwgMi4wLCAzLjBdKQpvdXRwdXQgPSB0Zi5ubi5sZWFreV9yZWx1KHgsIGFscGhhPTAuMDEpCnByaW50KG91dHB1dC5udW1weSgpKSAgIyBPdXRwdXQ6IFstMC4wMSAgMi4gICAgMy4gIF0="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Leaky ReLU activation function
x = tf.constant([-1.0, 2.0, 3.0])
output = tf.nn.leaky_relu(x, alpha=0.01)
print(output.numpy())  # Output: [-0.01  2.    3.  ]</pre></div><div class='content'></div><h3>Softmax</h3>
<div class='content'><h4>Definition</h4>
<p>The softmax function converts a vector of values into a probability distribution.</p>
<h4>Formula</h4>
<p>\[ f(x_i) = \frac{e^{x_i}}{\sum_{j} e^{x_j}} \]</p>
<h4>Characteristics</h4>
<ul>
<li><strong>Pros:</strong> Useful for multi-class classification problems.</li>
<li><strong>Cons:</strong> Computationally expensive for large number of classes.</li>
</ul>
<h4>Code Example</h4>
</div><div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKCiMgU29mdG1heCBhY3RpdmF0aW9uIGZ1bmN0aW9uCnggPSB0Zi5jb25zdGFudChbMS4wLCAyLjAsIDMuMF0pCm91dHB1dCA9IHRmLm5uLnNvZnRtYXgoeCkKcHJpbnQob3V0cHV0Lm51bXB5KCkpICAjIE91dHB1dDogWzAuMDkwMDMwNTcgMC4yNDQ3Mjg0OCAwLjY2NTI0MDk0XQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf

# Softmax activation function
x = tf.constant([1.0, 2.0, 3.0])
output = tf.nn.softmax(x)
print(output.numpy())  # Output: [0.09003057 0.24472848 0.66524094]</pre></div><div class='content'></div><h2>Practical Exercise</h2>
<div class='content'></div><h3>Task</h3>
<div class='content'><p>Implement a simple neural network using TensorFlow that uses different activation functions and compare their performance on the MNIST dataset.</p>
</div><h3>Solution</h3>
<div style='position:relative'><a class='copy_button d-none d-md-inline' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRlbnNvcmZsb3cgYXMgdGYKZnJvbSB0ZW5zb3JmbG93LmtlcmFzLmRhdGFzZXRzIGltcG9ydCBtbmlzdApmcm9tIHRlbnNvcmZsb3cua2VyYXMubW9kZWxzIGltcG9ydCBTZXF1ZW50aWFsCmZyb20gdGVuc29yZmxvdy5rZXJhcy5sYXllcnMgaW1wb3J0IERlbnNlLCBGbGF0dGVuCgojIExvYWQgTU5JU1QgZGF0YXNldAooeF90cmFpbiwgeV90cmFpbiksICh4X3Rlc3QsIHlfdGVzdCkgPSBtbmlzdC5sb2FkX2RhdGEoKQp4X3RyYWluLCB4X3Rlc3QgPSB4X3RyYWluIC8gMjU1LjAsIHhfdGVzdCAvIDI1NS4wCgojIERlZmluZSBhIHNpbXBsZSBuZXVyYWwgbmV0d29yayBtb2RlbApkZWYgY3JlYXRlX21vZGVsKGFjdGl2YXRpb25fZnVuY3Rpb24pOgogICAgbW9kZWwgPSBTZXF1ZW50aWFsKFsKICAgICAgICBGbGF0dGVuKGlucHV0X3NoYXBlPSgyOCwgMjgpKSwKICAgICAgICBEZW5zZSgxMjgsIGFjdGl2YXRpb249YWN0aXZhdGlvbl9mdW5jdGlvbiksCiAgICAgICAgRGVuc2UoMTAsIGFjdGl2YXRpb249J3NvZnRtYXgnKQogICAgXSkKICAgIG1vZGVsLmNvbXBpbGUob3B0aW1pemVyPSdhZGFtJywKICAgICAgICAgICAgICAgICAgbG9zcz0nc3BhcnNlX2NhdGVnb3JpY2FsX2Nyb3NzZW50cm9weScsCiAgICAgICAgICAgICAgICAgIG1ldHJpY3M9WydhY2N1cmFjeSddKQogICAgcmV0dXJuIG1vZGVsCgojIFRyYWluIGFuZCBldmFsdWF0ZSB0aGUgbW9kZWwgd2l0aCBkaWZmZXJlbnQgYWN0aXZhdGlvbiBmdW5jdGlvbnMKYWN0aXZhdGlvbl9mdW5jdGlvbnMgPSBbJ3NpZ21vaWQnLCAndGFuaCcsICdyZWx1JywgJ2xlYWt5X3JlbHUnXQpmb3IgYWN0aXZhdGlvbiBpbiBhY3RpdmF0aW9uX2Z1bmN0aW9uczoKICAgIHByaW50KGYiVHJhaW5pbmcgd2l0aCB7YWN0aXZhdGlvbn0gYWN0aXZhdGlvbiBmdW5jdGlvbiIpCiAgICBtb2RlbCA9IGNyZWF0ZV9tb2RlbChhY3RpdmF0aW9uKQogICAgbW9kZWwuZml0KHhfdHJhaW4sIHlfdHJhaW4sIGVwb2Nocz01LCB2YWxpZGF0aW9uX2RhdGE9KHhfdGVzdCwgeV90ZXN0KSkKICAgIHRlc3RfbG9zcywgdGVzdF9hY2MgPSBtb2RlbC5ldmFsdWF0ZSh4X3Rlc3QsIHlfdGVzdCkKICAgIHByaW50KGYiVGVzdCBhY2N1cmFjeSB3aXRoIHthY3RpdmF0aW9ufToge3Rlc3RfYWNjfVxuIik="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Load MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Define a simple neural network model
def create_model(activation_function):
    model = Sequential([
        Flatten(input_shape=(28, 28)),
        Dense(128, activation=activation_function),
        Dense(10, activation='softmax')
    ])
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Train and evaluate the model with different activation functions
activation_functions = ['sigmoid', 'tanh', 'relu', 'leaky_relu']
for activation in activation_functions:
    print(f&quot;Training with {activation} activation function&quot;)
    model = create_model(activation)
    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))
    test_loss, test_acc = model.evaluate(x_test, y_test)
    print(f&quot;Test accuracy with {activation}: {test_acc}\n&quot;)</pre></div><div class='content'></div><h2>Summary</h2>
<div class='content'><p>In this section, we covered the importance of activation functions in neural networks and explored various types, including linear, sigmoid, tanh, ReLU, Leaky ReLU, and softmax. Each activation function has its own characteristics, advantages, and disadvantages. Understanding these will help you choose the right activation function for your specific neural network model.</p>
</div><div class='row navigation'>
	<div class='col-2 d-none d-md-block'>
					<a href='04-02-creating-a-simple-neural-network' title="Creating a Simple Neural Network" class="py-2 px-3 btn btn-primary">
				&#x25C4; Previous 
			</a>
			</div>
	<div class='col-2 d-md-none'>
					<a href='04-02-creating-a-simple-neural-network' title="Creating a Simple Neural Network" class="py-2 px-3 btn btn-primary">
				&#x25C4;
			</a>
			</div>
	<div class='col-8 text-center'>
			</div>
	<div class='col-2 text-end d-none d-md-block'>
					<a href='04-04-loss-functions-and-optimizers' title="Loss Functions and Optimizers" class="py-2 px-3 btn btn-primary"
				data-read-mod="tensorflow" data-read-unit="4-3">
				Next &#x25BA;
			</a>
			</div>
	<div class='col-2 text-end d-md-none '>
					<a href='04-04-loss-functions-and-optimizers' title="Loss Functions and Optimizers" class="py-2 px-3 btn btn-primary" 
				data-read-mod="tensorflow" data-read-unit="4-3">
				 &#x25BA;
			</a>
			</div>
</div>

			</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
						
	<div class="container mt-2 d-none d-md-block index">
		<h1>TensorFlow Course</h1>
<h2>Module 1: Introduction to TensorFlow</h2>
<ul>
<li><a href="01-01-what-is-tensorflow">What is TensorFlow?</a></li>
<li><a href="01-02-setting-up-tensorflow">Setting Up TensorFlow</a></li>
<li><a href="01-03-basic-tensorflow-concepts">Basic TensorFlow Concepts</a></li>
<li><a href="01-04-tensorflow-hello-world">TensorFlow Hello World</a></li>
</ul>
<h2>Module 2: TensorFlow Basics</h2>
<ul>
<li><a href="02-01-tensors-and-operations">Tensors and Operations</a></li>
<li><a href="02-02-variables-and-constants">Variables and Constants</a></li>
<li><a href="02-03-tensorflow-graphs">TensorFlow Graphs</a></li>
<li><a href="02-04-eager-execution">Eager Execution</a></li>
</ul>
<h2>Module 3: Data Handling in TensorFlow</h2>
<ul>
<li><a href="03-01-loading-data">Loading Data</a></li>
<li><a href="03-02-data-pipelines-with-tf-data">Data Pipelines with tf.data</a></li>
<li><a href="03-03-data-augmentation">Data Augmentation</a></li>
<li><a href="03-04-working-with-datasets">Working with Datasets</a></li>
</ul>
<h2>Module 4: Building Neural Networks</h2>
<ul>
<li><a href="04-01-introduction-to-neural-networks">Introduction to Neural Networks</a></li>
<li><a href="04-02-creating-a-simple-neural-network">Creating a Simple Neural Network</a></li>
<li><a href="04-03-activation-functions">Activation Functions</a></li>
<li><a href="04-04-loss-functions-and-optimizers">Loss Functions and Optimizers</a></li>
</ul>
<h2>Module 5: Convolutional Neural Networks (CNNs)</h2>
<ul>
<li><a href="05-01-introduction-to-cnns">Introduction to CNNs</a></li>
<li><a href="05-02-building-a-cnn">Building a CNN</a></li>
<li><a href="05-03-pooling-layers">Pooling Layers</a></li>
<li><a href="05-04-advanced-cnn-architectures">Advanced CNN Architectures</a></li>
</ul>
<h2>Module 6: Recurrent Neural Networks (RNNs)</h2>
<ul>
<li><a href="06-01-introduction-to-rnns">Introduction to RNNs</a></li>
<li><a href="06-02-building-an-rnn">Building an RNN</a></li>
<li><a href="06-03-long-short-term-memory">Long Short-Term Memory (LSTM)</a></li>
<li><a href="06-04-gated-recurrent-units">Gated Recurrent Units (GRUs)</a></li>
</ul>
<h2>Module 7: Advanced TensorFlow Techniques</h2>
<ul>
<li><a href="07-01-custom-layers-and-models">Custom Layers and Models</a></li>
<li><a href="07-02-tensorflow-hub">TensorFlow Hub</a></li>
<li><a href="07-03-transfer-learning">Transfer Learning</a></li>
<li><a href="07-04-hyperparameter-tuning">Hyperparameter Tuning</a></li>
</ul>
<h2>Module 8: TensorFlow for Production</h2>
<ul>
<li><a href="08-01-model-saving-and-loading">Model Saving and Loading</a></li>
<li><a href="08-02-tensorflow-serving">TensorFlow Serving</a></li>
<li><a href="08-03-deploying-models">Deploying Models</a></li>
<li><a href="08-04-monitoring-and-maintenance">Monitoring and Maintenance</a></li>
</ul>
<h2>Module 9: TensorFlow Extended (TFX)</h2>
<ul>
<li><a href="09-01-introduction-to-tfx">Introduction to TFX</a></li>
<li><a href="09-02-data-validation">Data Validation</a></li>
<li><a href="09-03-transforming-data">Transforming Data</a></li>
<li><a href="09-04-model-analysis">Model Analysis</a></li>
</ul>
<h2>Module 10: Special Topics</h2>
<ul>
<li><a href="10-01-tensorflow-lite">TensorFlow Lite</a></li>
<li><a href="10-02-tensorflow-js">TensorFlow.js</a></li>
<li><a href="10-03-tensorflow-federated">TensorFlow Federated</a></li>
<li><a href="10-04-tensorflow-quantum">TensorFlow Quantum</a></li>
</ul>

	</div>










		</div>
	</div>
</div>		
<div class="container-xxl d-block d-md-none">
	<div class="row">
		<div class="col-12 p-2 p-md-0 m-0 text-end">
			<a href="/objective" rel="nofollow">The Project</a> | 
<a href="/about" rel="nofollow">About Us</a> | 
<a href="/contribute" rel="nofollow">Contribute</a> | 
<a href="/donate" rel="nofollow">Donations</a> | 
<a href="/license" rel="nofollow">License</a>
		</div>
	</div>
</div>

<div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	We use cookies to improve your user experience and offer content tailored to your interests.
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

		<div class="modal fade" id="loginModal" tabindex="-1" aria-labelledby="loginModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h5 class="modal-title" id="loginModalLabel">User not authenticated</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
            </div>
            <div class="modal-body">
            	<div id="modal-body-main"></div>
            </div>
        </div>
    </div>
</div>	</div>    
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js" crossorigin="anonymous"></script>
</body>
</html>
