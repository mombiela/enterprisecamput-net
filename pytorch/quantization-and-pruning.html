<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quantization and Pruning in PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/pytorch/quantization-and-pruning" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/pytorch/quantization-and-pruning" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/pytorch/quantization-and-pruning" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/pytorch/quantization-and-pruning" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/pytorch/quantization-and-pruning" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='distributed-training'>&#x25C4;Distributed Training</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Quantization and Pruning in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='graph-neural-networks'>Graph Neural Networks &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>Quantization and pruning are two techniques used to optimize deep learning models, making them more efficient and faster without significantly compromising accuracy. This section will guide you through the basics of these techniques, their implementation in PyTorch, and advanced usage.</p>
</div><h1>Quantization</h1>
<div class='content'></div><h2>What is Quantization?</h2>
<div class='content'><p>Quantization is the process of reducing the number of bits that represent a number. In the context of deep learning, it involves converting the weights and activations of a neural network from floating-point precision (32-bit) to a lower precision (e.g., 8-bit integers).</p>
</div><h2>Benefits of Quantization</h2>
<div class='content'><ul>
<li><strong>Reduced Model Size</strong>: Lower precision weights take up less memory.</li>
<li><strong>Increased Inference Speed</strong>: Operations on lower precision data are faster.</li>
<li><strong>Lower Power Consumption</strong>: Less computational power is required.</li>
</ul>
</div><h2>Types of Quantization</h2>
<div class='content'><ul>
<li><strong>Dynamic Quantization</strong>: Weights are quantized post-training, but activations are quantized dynamically during inference.</li>
<li><strong>Static Quantization</strong>: Both weights and activations are quantized post-training.</li>
<li><strong>Quantization-Aware Training (QAT)</strong>: The model is trained with quantization in mind, leading to better accuracy.</li>
</ul>
</div><h2>Implementing Dynamic Quantization in PyTorch</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaHZpc2lvbi5tb2RlbHMgYXMgbW9kZWxzCgojIExvYWQgYSBwcmUtdHJhaW5lZCBtb2RlbAptb2RlbCA9IG1vZGVscy5yZXNuZXQxOChwcmV0cmFpbmVkPVRydWUpCgojIEFwcGx5IGR5bmFtaWMgcXVhbnRpemF0aW9uCnF1YW50aXplZF9tb2RlbCA9IHRvcmNoLnF1YW50aXphdGlvbi5xdWFudGl6ZV9keW5hbWljKAogICAgbW9kZWwsIHt0b3JjaC5ubi5MaW5lYXJ9LCBkdHlwZT10b3JjaC5xaW50OAopCgojIFByaW50IHRoZSBtb2RlbCB0byBzZWUgdGhlIGNoYW5nZXMKcHJpbnQocXVhbnRpemVkX21vZGVsKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torchvision.models as models

# Load a pre-trained model
model = models.resnet18(pretrained=True)

# Apply dynamic quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

# Print the model to see the changes
print(quantized_model)</pre></div><div class='content'></div><h2>Implementing Static Quantization in PyTorch</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaHZpc2lvbi5tb2RlbHMgYXMgbW9kZWxzCgojIExvYWQgYSBwcmUtdHJhaW5lZCBtb2RlbAptb2RlbCA9IG1vZGVscy5yZXNuZXQxOChwcmV0cmFpbmVkPVRydWUpCm1vZGVsLmV2YWwoKQoKIyBGdXNlIHRoZSBsYXllcnMgKG5lY2Vzc2FyeSBmb3Igc3RhdGljIHF1YW50aXphdGlvbikKbW9kZWwuZnVzZV9tb2RlbCgpCgojIFByZXBhcmUgdGhlIG1vZGVsIGZvciBzdGF0aWMgcXVhbnRpemF0aW9uCm1vZGVsLnFjb25maWcgPSB0b3JjaC5xdWFudGl6YXRpb24uZ2V0X2RlZmF1bHRfcWNvbmZpZygnZmJnZW1tJykKdG9yY2gucXVhbnRpemF0aW9uLnByZXBhcmUobW9kZWwsIGlucGxhY2U9VHJ1ZSkKCiMgQ2FsaWJyYXRlIHRoZSBtb2RlbCB3aXRoIGEgcmVwcmVzZW50YXRpdmUgZGF0YXNldAojIChIZXJlLCB3ZSB1c2UgYSBkdW1teSBpbnB1dCBmb3Igc2ltcGxpY2l0eSkKaW5wdXRfdGVuc29yID0gdG9yY2gucmFuZG4oMSwgMywgMjI0LCAyMjQpCm1vZGVsKGlucHV0X3RlbnNvcikKCiMgQ29udmVydCB0aGUgbW9kZWwgdG8gYSBxdWFudGl6ZWQgdmVyc2lvbgp0b3JjaC5xdWFudGl6YXRpb24uY29udmVydChtb2RlbCwgaW5wbGFjZT1UcnVlKQoKIyBQcmludCB0aGUgbW9kZWwgdG8gc2VlIHRoZSBjaGFuZ2VzCnByaW50KG1vZGVsKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torchvision.models as models

# Load a pre-trained model
model = models.resnet18(pretrained=True)
model.eval()

# Fuse the layers (necessary for static quantization)
model.fuse_model()

# Prepare the model for static quantization
model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
torch.quantization.prepare(model, inplace=True)

# Calibrate the model with a representative dataset
# (Here, we use a dummy input for simplicity)
input_tensor = torch.randn(1, 3, 224, 224)
model(input_tensor)

# Convert the model to a quantized version
torch.quantization.convert(model, inplace=True)

# Print the model to see the changes
print(model)</pre></div><div class='content'></div><h2>Quantization-Aware Training (QAT)</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaHZpc2lvbi5tb2RlbHMgYXMgbW9kZWxzCgojIExvYWQgYSBwcmUtdHJhaW5lZCBtb2RlbAptb2RlbCA9IG1vZGVscy5yZXNuZXQxOChwcmV0cmFpbmVkPVRydWUpCm1vZGVsLnRyYWluKCkKCiMgRnVzZSB0aGUgbGF5ZXJzCm1vZGVsLmZ1c2VfbW9kZWwoKQoKIyBQcmVwYXJlIHRoZSBtb2RlbCBmb3IgUUFUCm1vZGVsLnFjb25maWcgPSB0b3JjaC5xdWFudGl6YXRpb24uZ2V0X2RlZmF1bHRfcWF0X3Fjb25maWcoJ2ZiZ2VtbScpCnRvcmNoLnF1YW50aXphdGlvbi5wcmVwYXJlX3FhdChtb2RlbCwgaW5wbGFjZT1UcnVlKQoKIyBGaW5lLXR1bmUgdGhlIG1vZGVsIHdpdGggdHJhaW5pbmcgZGF0YQojIChIZXJlLCB3ZSB1c2UgYSBkdW1teSB0cmFpbmluZyBsb29wIGZvciBzaW1wbGljaXR5KQpvcHRpbWl6ZXIgPSB0b3JjaC5vcHRpbS5TR0QobW9kZWwucGFyYW1ldGVycygpLCBscj0wLjAxKQpmb3IgZXBvY2ggaW4gcmFuZ2UoNSk6CiAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgIG91dHB1dCA9IG1vZGVsKGlucHV0X3RlbnNvcikKICAgIGxvc3MgPSB0b3JjaC5ubi5mdW5jdGlvbmFsLmNyb3NzX2VudHJvcHkob3V0cHV0LCB0b3JjaC50ZW5zb3IoWzBdKSkKICAgIGxvc3MuYmFja3dhcmQoKQogICAgb3B0aW1pemVyLnN0ZXAoKQoKIyBDb252ZXJ0IHRoZSBtb2RlbCB0byBhIHF1YW50aXplZCB2ZXJzaW9uCnRvcmNoLnF1YW50aXphdGlvbi5jb252ZXJ0KG1vZGVsLCBpbnBsYWNlPVRydWUpCgojIFByaW50IHRoZSBtb2RlbCB0byBzZWUgdGhlIGNoYW5nZXMKcHJpbnQobW9kZWwp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torchvision.models as models

# Load a pre-trained model
model = models.resnet18(pretrained=True)
model.train()

# Fuse the layers
model.fuse_model()

# Prepare the model for QAT
model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
torch.quantization.prepare_qat(model, inplace=True)

# Fine-tune the model with training data
# (Here, we use a dummy training loop for simplicity)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
for epoch in range(5):
    optimizer.zero_grad()
    output = model(input_tensor)
    loss = torch.nn.functional.cross_entropy(output, torch.tensor([0]))
    loss.backward()
    optimizer.step()

# Convert the model to a quantized version
torch.quantization.convert(model, inplace=True)

# Print the model to see the changes
print(model)</pre></div><div class='content'></div><h1>Pruning</h1>
<div class='content'></div><h2>What is Pruning?</h2>
<div class='content'><p>Pruning involves removing less important weights from a neural network to reduce its size and complexity. This can lead to faster inference times and reduced memory usage.</p>
</div><h2>Benefits of Pruning</h2>
<div class='content'><ul>
<li><strong>Reduced Model Size</strong>: Fewer weights mean a smaller model.</li>
<li><strong>Increased Inference Speed</strong>: Less computation is required.</li>
<li><strong>Potential for Better Generalization</strong>: Removing redundant weights can sometimes improve model generalization.</li>
</ul>
</div><h2>Types of Pruning</h2>
<div class='content'><ul>
<li><strong>Unstructured Pruning</strong>: Individual weights are pruned based on their magnitude.</li>
<li><strong>Structured Pruning</strong>: Entire neurons, channels, or layers are pruned.</li>
</ul>
</div><h2>Implementing Unstructured Pruning in PyTorch</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi51dGlscy5wcnVuZSBhcyBwcnVuZQppbXBvcnQgdG9yY2h2aXNpb24ubW9kZWxzIGFzIG1vZGVscwoKIyBMb2FkIGEgcHJlLXRyYWluZWQgbW9kZWwKbW9kZWwgPSBtb2RlbHMucmVzbmV0MTgocHJldHJhaW5lZD1UcnVlKQoKIyBBcHBseSB1bnN0cnVjdHVyZWQgcHJ1bmluZyB0byB0aGUgZmlyc3QgbGF5ZXIKcHJ1bmUubDFfdW5zdHJ1Y3R1cmVkKG1vZGVsLmxheWVyMVswXS5jb252MSwgbmFtZT0nd2VpZ2h0JywgYW1vdW50PTAuMikKCiMgUHJpbnQgdGhlIG1vZGVsIHRvIHNlZSB0aGUgY2hhbmdlcwpwcmludChtb2RlbC5sYXllcjFbMF0uY29udjEud2VpZ2h0KQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn.utils.prune as prune
import torchvision.models as models

# Load a pre-trained model
model = models.resnet18(pretrained=True)

# Apply unstructured pruning to the first layer
prune.l1_unstructured(model.layer1[0].conv1, name='weight', amount=0.2)

# Print the model to see the changes
print(model.layer1[0].conv1.weight)</pre></div><div class='content'></div><h2>Implementing Structured Pruning in PyTorch</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubi51dGlscy5wcnVuZSBhcyBwcnVuZQppbXBvcnQgdG9yY2h2aXNpb24ubW9kZWxzIGFzIG1vZGVscwoKIyBMb2FkIGEgcHJlLXRyYWluZWQgbW9kZWwKbW9kZWwgPSBtb2RlbHMucmVzbmV0MTgocHJldHJhaW5lZD1UcnVlKQoKIyBBcHBseSBzdHJ1Y3R1cmVkIHBydW5pbmcgdG8gdGhlIGZpcnN0IGxheWVyCnBydW5lLmxuX3N0cnVjdHVyZWQobW9kZWwubGF5ZXIxWzBdLmNvbnYxLCBuYW1lPSd3ZWlnaHQnLCBhbW91bnQ9MC4yLCBuPTIsIGRpbT0wKQoKIyBQcmludCB0aGUgbW9kZWwgdG8gc2VlIHRoZSBjaGFuZ2VzCnByaW50KG1vZGVsLmxheWVyMVswXS5jb252MS53ZWlnaHQp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn.utils.prune as prune
import torchvision.models as models

# Load a pre-trained model
model = models.resnet18(pretrained=True)

# Apply structured pruning to the first layer
prune.ln_structured(model.layer1[0].conv1, name='weight', amount=0.2, n=2, dim=0)

# Print the model to see the changes
print(model.layer1[0].conv1.weight)</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Quantization and pruning are powerful techniques for optimizing deep learning models. Quantization reduces the precision of weights and activations, leading to smaller and faster models. Pruning removes less important weights, reducing model size and complexity. Both techniques can be implemented in PyTorch with relative ease, allowing you to deploy efficient models in resource-constrained environments.</p>
<p>By understanding and applying these techniques, you can significantly enhance the performance of your deep learning models, making them more suitable for real-world applications.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='distributed-training'>&#x25C4;Distributed Training</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Quantization and Pruning in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='graph-neural-networks'>Graph Neural Networks &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
