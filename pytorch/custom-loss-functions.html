<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Custom Loss Functions in PyTorch</title>

    <link rel="alternate" href="https://campusempresa.com/pytorch/custom-loss-functions" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/pytorch/custom-loss-functions" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/pytorch/custom-loss-functions" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/pytorch/custom-loss-functions" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/pytorch/custom-loss-functions" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='row navigation'>
	<div class='col-4'>
					<a href='custom-datasets-and-dataloaders'>&#x25C4;Custom Datasets and DataLoaders</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Custom Loss Functions in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='custom-layers-and-modules'>Custom Layers and Modules &#x25BA;</a>
			</div>
</div>
<div class='content'></div><h1>Introduction</h1>
<div class='content'><p>In this section, we will explore how to create custom loss functions in PyTorch. Loss functions are a crucial component of training neural networks as they measure how well the model is performing. While PyTorch provides many built-in loss functions, there are scenarios where you might need to define your own.</p>
</div><h1>Key Concepts</h1>
<div class='content'><ul>
<li><strong>Loss Function</strong>: A method to evaluate how well the model's predictions match the target values.</li>
<li><strong>Backward Propagation</strong>: The process of calculating the gradient of the loss function with respect to the model's parameters.</li>
<li><strong>Autograd</strong>: PyTorch's automatic differentiation library that supports all operations on Tensors.</li>
</ul>
</div><h1>Built-in Loss Functions</h1>
<div class='content'><p>Before diving into custom loss functions, let's briefly review some common built-in loss functions in PyTorch:</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKIyBNZWFuIFNxdWFyZWQgRXJyb3IgTG9zcwptc2VfbG9zcyA9IG5uLk1TRUxvc3MoKQoKIyBDcm9zcyBFbnRyb3B5IExvc3MKY3Jvc3NfZW50cm9weV9sb3NzID0gbm4uQ3Jvc3NFbnRyb3B5TG9zcygpCgojIEJpbmFyeSBDcm9zcyBFbnRyb3B5IExvc3MKYmNlX2xvc3MgPSBubi5CQ0VMb3NzKCk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

# Mean Squared Error Loss
mse_loss = nn.MSELoss()

# Cross Entropy Loss
cross_entropy_loss = nn.CrossEntropyLoss()

# Binary Cross Entropy Loss
bce_loss = nn.BCELoss()</pre></div><div class='content'></div><h1>Creating Custom Loss Functions</h1>
<div class='content'><p>Creating a custom loss function in PyTorch involves defining a new class that inherits from <code>nn.Module</code> and overriding the <code>forward</code> method.</p>
</div><h2>Example 1: Mean Absolute Error (MAE) Loss</h2>
<div class='content'><p>Let's start with a simple example of creating a Mean Absolute Error (MAE) loss function.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IHRvcmNoCmltcG9ydCB0b3JjaC5ubiBhcyBubgoKY2xhc3MgTUFFTG9zcyhubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYpOgogICAgICAgIHN1cGVyKE1BRUxvc3MsIHNlbGYpLl9faW5pdF9fKCkKCiAgICBkZWYgZm9yd2FyZChzZWxmLCBwcmVkaWN0aW9ucywgdGFyZ2V0cyk6CiAgICAgICAgcmV0dXJuIHRvcmNoLm1lYW4odG9yY2guYWJzKHByZWRpY3Rpb25zIC0gdGFyZ2V0cykpCgojIEV4YW1wbGUgdXNhZ2UKcHJlZGljdGlvbnMgPSB0b3JjaC50ZW5zb3IoWzAuNSwgMS41LCAyLjVdKQp0YXJnZXRzID0gdG9yY2gudGVuc29yKFsxLjAsIDIuMCwgMy4wXSkKbWFlX2xvc3MgPSBNQUVMb3NzKCkKbG9zcyA9IG1hZV9sb3NzKHByZWRpY3Rpb25zLCB0YXJnZXRzKQpwcmludChmJ01BRSBMb3NzOiB7bG9zcy5pdGVtKCl9Jyk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import torch
import torch.nn as nn

class MAELoss(nn.Module):
    def __init__(self):
        super(MAELoss, self).__init__()

    def forward(self, predictions, targets):
        return torch.mean(torch.abs(predictions - targets))

# Example usage
predictions = torch.tensor([0.5, 1.5, 2.5])
targets = torch.tensor([1.0, 2.0, 3.0])
mae_loss = MAELoss()
loss = mae_loss(predictions, targets)
print(f'MAE Loss: {loss.item()}')</pre></div><div class='content'></div><h2>Example 2: Huber Loss</h2>
<div class='content'><p>Huber Loss is less sensitive to outliers in data than the Mean Squared Error loss. It combines the best properties of both MAE and MSE.</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgSHViZXJMb3NzKG5uLk1vZHVsZSk6CiAgICBkZWYgX19pbml0X18oc2VsZiwgZGVsdGE9MS4wKToKICAgICAgICBzdXBlcihIdWJlckxvc3MsIHNlbGYpLl9faW5pdF9fKCkKICAgICAgICBzZWxmLmRlbHRhID0gZGVsdGEKCiAgICBkZWYgZm9yd2FyZChzZWxmLCBwcmVkaWN0aW9ucywgdGFyZ2V0cyk6CiAgICAgICAgZXJyb3IgPSBwcmVkaWN0aW9ucyAtIHRhcmdldHMKICAgICAgICBpc19zbWFsbF9lcnJvciA9IHRvcmNoLmFicyhlcnJvcikgPD0gc2VsZi5kZWx0YQogICAgICAgIHNtYWxsX2Vycm9yX2xvc3MgPSAwLjUgKiB0b3JjaC5wb3coZXJyb3IsIDIpCiAgICAgICAgbGFyZ2VfZXJyb3JfbG9zcyA9IHNlbGYuZGVsdGEgKiAodG9yY2guYWJzKGVycm9yKSAtIDAuNSAqIHNlbGYuZGVsdGEpCiAgICAgICAgcmV0dXJuIHRvcmNoLm1lYW4odG9yY2gud2hlcmUoaXNfc21hbGxfZXJyb3IsIHNtYWxsX2Vycm9yX2xvc3MsIGxhcmdlX2Vycm9yX2xvc3MpKQoKIyBFeGFtcGxlIHVzYWdlCnByZWRpY3Rpb25zID0gdG9yY2gudGVuc29yKFswLjUsIDEuNSwgMi41XSkKdGFyZ2V0cyA9IHRvcmNoLnRlbnNvcihbMS4wLCAyLjAsIDMuMF0pCmh1YmVyX2xvc3MgPSBIdWJlckxvc3MoZGVsdGE9MS4wKQpsb3NzID0gaHViZXJfbG9zcyhwcmVkaWN0aW9ucywgdGFyZ2V0cykKcHJpbnQoZidIdWJlciBMb3NzOiB7bG9zcy5pdGVtKCl9Jyk="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class HuberLoss(nn.Module):
    def __init__(self, delta=1.0):
        super(HuberLoss, self).__init__()
        self.delta = delta

    def forward(self, predictions, targets):
        error = predictions - targets
        is_small_error = torch.abs(error) &lt;= self.delta
        small_error_loss = 0.5 * torch.pow(error, 2)
        large_error_loss = self.delta * (torch.abs(error) - 0.5 * self.delta)
        return torch.mean(torch.where(is_small_error, small_error_loss, large_error_loss))

# Example usage
predictions = torch.tensor([0.5, 1.5, 2.5])
targets = torch.tensor([1.0, 2.0, 3.0])
huber_loss = HuberLoss(delta=1.0)
loss = huber_loss(predictions, targets)
print(f'Huber Loss: {loss.item()}')</pre></div><div class='content'></div><h1>Advanced Custom Loss Functions</h1>
<div class='content'><p>For more advanced custom loss functions, you might need to leverage PyTorch's autograd functionality directly.</p>
</div><h2>Example 3: Custom Gradient Penalty</h2>
<div class='content'><p>Suppose you want to create a custom loss function that includes a gradient penalty term, often used in Generative Adversarial Networks (GANs).</p>
</div><div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("Y2xhc3MgR3JhZGllbnRQZW5hbHR5TG9zcyhubi5Nb2R1bGUpOgogICAgZGVmIF9faW5pdF9fKHNlbGYsIGxhbWJkYV9ncD0xMCk6CiAgICAgICAgc3VwZXIoR3JhZGllbnRQZW5hbHR5TG9zcywgc2VsZikuX19pbml0X18oKQogICAgICAgIHNlbGYubGFtYmRhX2dwID0gbGFtYmRhX2dwCgogICAgZGVmIGZvcndhcmQoc2VsZiwgZGlzY3JpbWluYXRvciwgcmVhbF9kYXRhLCBmYWtlX2RhdGEpOgogICAgICAgIGJhdGNoX3NpemUgPSByZWFsX2RhdGEuc2l6ZSgwKQogICAgICAgIGVwc2lsb24gPSB0b3JjaC5yYW5kKGJhdGNoX3NpemUsIDEsIDEsIDEsIGRldmljZT1yZWFsX2RhdGEuZGV2aWNlKQogICAgICAgIGludGVycG9sYXRlZCA9IGVwc2lsb24gKiByZWFsX2RhdGEgKyAoMSAtIGVwc2lsb24pICogZmFrZV9kYXRhCiAgICAgICAgaW50ZXJwb2xhdGVkLnJlcXVpcmVzX2dyYWRfKFRydWUpCgogICAgICAgIGRfaW50ZXJwb2xhdGVkID0gZGlzY3JpbWluYXRvcihpbnRlcnBvbGF0ZWQpCiAgICAgICAgZ3JhZGllbnRzID0gdG9yY2guYXV0b2dyYWQuZ3JhZCgKICAgICAgICAgICAgb3V0cHV0cz1kX2ludGVycG9sYXRlZCwKICAgICAgICAgICAgaW5wdXRzPWludGVycG9sYXRlZCwKICAgICAgICAgICAgZ3JhZF9vdXRwdXRzPXRvcmNoLm9uZXNfbGlrZShkX2ludGVycG9sYXRlZCksCiAgICAgICAgICAgIGNyZWF0ZV9ncmFwaD1UcnVlLAogICAgICAgICAgICByZXRhaW5fZ3JhcGg9VHJ1ZSwKICAgICAgICApWzBdCgogICAgICAgIGdyYWRpZW50cyA9IGdyYWRpZW50cy52aWV3KGJhdGNoX3NpemUsIC0xKQogICAgICAgIGdyYWRpZW50X3BlbmFsdHkgPSAoKGdyYWRpZW50cy5ub3JtKDIsIGRpbT0xKSAtIDEpICoqIDIpLm1lYW4oKQogICAgICAgIHJldHVybiBzZWxmLmxhbWJkYV9ncCAqIGdyYWRpZW50X3BlbmFsdHkKCiMgRXhhbXBsZSB1c2FnZQojIEFzc3VtaW5nIGBkaXNjcmltaW5hdG9yYCwgYHJlYWxfZGF0YWAsIGFuZCBgZmFrZV9kYXRhYCBhcmUgZGVmaW5lZApncmFkaWVudF9wZW5hbHR5X2xvc3MgPSBHcmFkaWVudFBlbmFsdHlMb3NzKGxhbWJkYV9ncD0xMCkKbG9zcyA9IGdyYWRpZW50X3BlbmFsdHlfbG9zcyhkaXNjcmltaW5hdG9yLCByZWFsX2RhdGEsIGZha2VfZGF0YSkKcHJpbnQoZidHcmFkaWVudCBQZW5hbHR5IExvc3M6IHtsb3NzLml0ZW0oKX0nKQ=="))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>class GradientPenaltyLoss(nn.Module):
    def __init__(self, lambda_gp=10):
        super(GradientPenaltyLoss, self).__init__()
        self.lambda_gp = lambda_gp

    def forward(self, discriminator, real_data, fake_data):
        batch_size = real_data.size(0)
        epsilon = torch.rand(batch_size, 1, 1, 1, device=real_data.device)
        interpolated = epsilon * real_data + (1 - epsilon) * fake_data
        interpolated.requires_grad_(True)

        d_interpolated = discriminator(interpolated)
        gradients = torch.autograd.grad(
            outputs=d_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones_like(d_interpolated),
            create_graph=True,
            retain_graph=True,
        )[0]

        gradients = gradients.view(batch_size, -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        return self.lambda_gp * gradient_penalty

# Example usage
# Assuming `discriminator`, `real_data`, and `fake_data` are defined
gradient_penalty_loss = GradientPenaltyLoss(lambda_gp=10)
loss = gradient_penalty_loss(discriminator, real_data, fake_data)
print(f'Gradient Penalty Loss: {loss.item()}')</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Custom loss functions in PyTorch provide flexibility to tailor the training process to specific needs. By understanding how to create and implement these functions, you can enhance your models' performance and adapt to unique problem requirements. Whether it's a simple MAE loss or a more complex gradient penalty, PyTorch's modular design makes it straightforward to define and use custom loss functions in your training pipeline.</p>
</div><div class='row navigation'>
	<div class='col-4'>
					<a href='custom-datasets-and-dataloaders'>&#x25C4;Custom Datasets and DataLoaders</a>
			</div>
	<div class='col-4 text-center'>
		<a href="./" class="title">Custom Loss Functions in PyTorch</a>
	</div>
	<div class='col-4 text-end'>
					<a href='custom-layers-and-modules'>Custom Layers and Modules &#x25BA;</a>
			</div>
</div>
</div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Accept</a>
    <a href="/cookies">More Information</a>
</div>	

	</div>    
</body>
</html>
