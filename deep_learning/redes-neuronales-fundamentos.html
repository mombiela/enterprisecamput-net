<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Networks: Fundamentals</title>

    <link rel="alternate" href="https://campusempresa.com/deep_learning/redes-neuronales-fundamentos" hreflang="es" />
	<link rel="alternate" href="https://campusempresa.cat/deep_learning/redes-neuronales-fundamentos" hreflang="ca" />
	<link rel="alternate" href="https://enterprisecampus.net/deep_learning/redes-neuronales-fundamentos" hreflang="en" />
    
	<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css">
	<link href="/css/site.css" rel="stylesheet">
	
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"></script>
  	<script type="text/javascript" src="js/math_init.js"></script>
  	<script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js"></script>
  	<script type="text/javascript" src="/js/cookie.js"></script>
  	<script type="text/javascript" src="/js/main.js"></script>
</head>

<body>
    <div id="content">
		<div id="header" class="container-xxl">
	<div class="row">
		<div class="col-8 p-0">
			<h1 class="m-0 p-0">
				<a href="/"><img src="/img/logo_header.png" style="visibility:hiddenxx;"></a>
			</h1>
		</div>
		<div class="col-4 p-0 text-end">
			<h2 id="main_title"><cite>Building today's and tomorrow's society</cite></h2>
			<h3 id="main_subtitle"></h3>
		</div>
	</div>
</div>
<div class="container-xxl" style="margin-top: -1em;">
	<div class="row">
		<div class="col-12 p-0 m-0 text-end">
													<b id="lit_lang_es" class="px-2">EN</b>
				|
				<a href="https://campusempresa.com/deep_learning/redes-neuronales-fundamentos" class="px-2">ES</a></b>
				|
				<a href="https://campusempresa.cat/deep_learning/redes-neuronales-fundamentos" class="px-2">CA</a>
					</div>
	</div>
</div>
   <div class="top-bar container-fluid">
	<div class="container-xxl">
		<div class="row">
			<div class="col" id="left_menu">
				<a href="/objective">The Project</a>
				<a href="/about">About Us</a>
				<a href="/contribute">Contribute</a>
				<a href="/donate">Donations</a>
				<a href="/licence">License</a>
			</div>
		</div>
	</div>
   </div>

<div class="container-xxl" id="main_content">
	<div class="row">
		<div class="col-12 col-lg-8">
			<div id="nav1" class="navigation"></div>
			<div id="inner_content"><div class='content'></div><h1>Introduction to Neural Networks</h1>
<div class='content'><p>Neural networks are an essential component of Deep Learning, inspired by the structure and functioning of the human brain. These networks are composed of basic units called neurons, which are organized into layers and interconnected to process and learn from data.</p>
</div><h1>Structure of a Neural Network</h1>
<div class='content'><p>A typical neural network consists of the following layers:</p>
<ul>
<li><strong>Input Layer</strong>: Receives the input data.</li>
<li><strong>Hidden Layers</strong>: Process the information received from the input layer through interconnected neurons.</li>
<li><strong>Output Layer</strong>: Produces the final result of the network.</li>
</ul>
</div><h2>Artificial Neuron</h2>
<div class='content'><p>Each neuron in a neural network performs a simple mathematical operation. The basic structure of a neuron includes:</p>
<ul>
<li><strong>Weights</strong>: Coefficients that weigh the importance of each input.</li>
<li><strong>Activation Function</strong>: Introduces non-linearity in the neuron's output.</li>
<li><strong>Bias</strong>: An additional term that helps adjust the output along with the weights.</li>
</ul>
</div><h2>Example of a Neuron</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIERlZmluaXRpb24gb2YgdGhlIGFjdGl2YXRpb24gZnVuY3Rpb24gKHNpZ21vaWQpCmRlZiBzaWdtb2lkKHgpOgogICAgcmV0dXJuIDEgLyAoMSArIG5wLmV4cCgteCkpCgojIElucHV0cyBhbmQgd2VpZ2h0cwppbnB1dHMgPSBucC5hcnJheShbMC41LCAwLjNdKQp3ZWlnaHRzID0gbnAuYXJyYXkoWzAuNCwgMC43XSkKYmlhcyA9IDAuMQoKIyBDYWxjdWxhdGlvbiBvZiB0aGUgbmV1cm9uJ3Mgb3V0cHV0Cm91dHB1dCA9IHNpZ21vaWQobnAuZG90KGlucHV0cywgd2VpZ2h0cykgKyBiaWFzKQpwcmludChvdXRwdXQp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Definition of the activation function (sigmoid)
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Inputs and weights
inputs = np.array([0.5, 0.3])
weights = np.array([0.4, 0.7])
bias = 0.1

# Calculation of the neuron's output
output = sigmoid(np.dot(inputs, weights) + bias)
print(output)</pre></div><div class='content'></div><h1>Activation Functions</h1>
<div class='content'><p>Activation functions are crucial for introducing non-linearity into the network, allowing the network to learn and model complex data. Some common activation functions include:</p>
<ul>
<li><strong>Sigmoid</strong>: <code>σ(x) = 1 / (1 + e^(-x))</code></li>
<li><strong>ReLU (Rectified Linear Unit)</strong>: <code>f(x) = max(0, x)</code></li>
<li><strong>Tanh (Hyperbolic Tangent)</strong>: <code>tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))</code></li>
</ul>
</div><h2>Comparison of Activation Functions</h2>
<div class='content'><p>| Activation Function | Formula                  | Output Range     | Properties                              |
|---------------------|--------------------------|------------------|-----------------------------------------|
| Sigmoid             | \( \sigma(x) = \frac{1}{1 + e^{-x}} \) | (0, 1)           | Smooth, differentiable, but can saturate. |
| ReLU                | \( f(x) = \max(0, x) \)  | [0, ∞)           | Simple, efficient, but can cause &quot;dead neuron&quot;. |
| Tanh                | \( \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \) | (-1, 1)         | Smooth, differentiable, centered at zero. |</p>
</div><h1>Training Process</h1>
<div class='content'><p>Training a neural network involves adjusting the weights and biases to minimize the error in predictions. This process is carried out through:</p>
<ul>
<li><strong>Forward Propagation</strong>: Calculates the network's output.</li>
<li><strong>Cost Function</strong>: Measures the difference between the predicted output and the actual output.</li>
<li><strong>Backpropagation</strong>: Adjusts the weights and biases to reduce the error.</li>
</ul>
</div><h2>Example of Simple Training</h2>
<div style='position:relative'><a class='copy_button' href='#' onclick='navigator.clipboard.writeText(decodeURIComponent(escape(atob("aW1wb3J0IG51bXB5IGFzIG5wCgojIElucHV0IGRhdGEgYW5kIGV4cGVjdGVkIG91dHB1dAppbnB1dHMgPSBucC5hcnJheShbMC41LCAwLjNdKQpleHBlY3RlZF9vdXRwdXQgPSAwLjcKCiMgSW5pdGlhbGl6YXRpb24gb2Ygd2VpZ2h0cyBhbmQgYmlhcwp3ZWlnaHRzID0gbnAuYXJyYXkoWzAuNCwgMC43XSkKYmlhcyA9IDAuMQpsZWFybmluZ19yYXRlID0gMC4wMQoKIyBEZWZpbml0aW9uIG9mIHRoZSBhY3RpdmF0aW9uIGZ1bmN0aW9uIGFuZCBpdHMgZGVyaXZhdGl2ZQpkZWYgc2lnbW9pZCh4KToKICAgIHJldHVybiAxIC8gKDEgKyBucC5leHAoLXgpKQoKZGVmIHNpZ21vaWRfZGVyaXZhdGl2ZSh4KToKICAgIHJldHVybiB4ICogKDEgLSB4KQoKIyBUcmFpbmluZyBwcm9jZXNzCmZvciBlcG9jaCBpbiByYW5nZSgxMDAwMCk6CiAgICAjIEZvcndhcmQgcHJvcGFnYXRpb24KICAgIHdlaWdodGVkX3N1bSA9IG5wLmRvdChpbnB1dHMsIHdlaWdodHMpICsgYmlhcwogICAgb3V0cHV0ID0gc2lnbW9pZCh3ZWlnaHRlZF9zdW0pCiAgICAKICAgICMgQ2FsY3VsYXRpb24gb2YgdGhlIGVycm9yCiAgICBlcnJvciA9IGV4cGVjdGVkX291dHB1dCAtIG91dHB1dAogICAgCiAgICAjIEJhY2twcm9wYWdhdGlvbgogICAgYWRqdXN0bWVudHMgPSBlcnJvciAqIHNpZ21vaWRfZGVyaXZhdGl2ZShvdXRwdXQpCiAgICB3ZWlnaHRzICs9IGxlYXJuaW5nX3JhdGUgKiBhZGp1c3RtZW50cyAqIGlucHV0cwogICAgYmlhcyArPSBsZWFybmluZ19yYXRlICogYWRqdXN0bWVudHMKCnByaW50KCJBZGp1c3RlZCB3ZWlnaHRzOiIsIHdlaWdodHMpCnByaW50KCJBZGp1c3RlZCBiaWFzOiIsIGJpYXMp"))));alert("Copied!");return false;'><i class='bi bi-copy'></i></a><pre class='code'>import numpy as np

# Input data and expected output
inputs = np.array([0.5, 0.3])
expected_output = 0.7

# Initialization of weights and bias
weights = np.array([0.4, 0.7])
bias = 0.1
learning_rate = 0.01

# Definition of the activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Training process
for epoch in range(10000):
    # Forward propagation
    weighted_sum = np.dot(inputs, weights) + bias
    output = sigmoid(weighted_sum)
    
    # Calculation of the error
    error = expected_output - output
    
    # Backpropagation
    adjustments = error * sigmoid_derivative(output)
    weights += learning_rate * adjustments * inputs
    bias += learning_rate * adjustments

print(&quot;Adjusted weights:&quot;, weights)
print(&quot;Adjusted bias:&quot;, bias)</pre></div><div class='content'></div><h1>Conclusion</h1>
<div class='content'><p>Neural networks are a powerful tool in the field of Deep Learning, capable of learning and generalizing from complex data. Understanding the fundamentals of their structure, activation functions, and training process is crucial for developing effective models. In the following topics, we will delve into more advanced architectures and optimization techniques to improve the performance of neural networks.</p>
</div></div>
		</div>
		<div class="col-12 col-lg-4 publi" id="div_publi">
			<h1>Advertising</h1>
			<p>This space is reserved for advertising.</p>
			<p>If you want to be a sponsor, contact us to include links in this area: <a href='mailto:admin@campusempresa.cat'>admin@campusempresa.cat</a></p>
			<p>Thank you for collaborating!</p>
		</div>
	</div>
</div>

   <div class="container-xxl my-3">
	<div class="row">
		<div class="col">
			<footer>&copy; Copyright 2024. All rights reserved</footer>
		</div>
	</div>
</div>	

<div id="cookies_adv" style="display:none;">
	Fem servir galetes per millorar la teva experiència d'ús i oferir continguts adaptats als teus interessos
    <a href="#" id="btn_accept_cookies" class="button">Entès!</a>
    <a href="/cookies">Més informació</a>
</div>	

	</div>    
</body>
</html>
